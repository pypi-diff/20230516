# Comparing `tmp/azureml_metrics-0.0.6.post1-py3-none-any.whl.zip` & `tmp/azureml_metrics-0.0.7-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,48 +1,48 @@
-Zip file size: 151244 bytes, number of entries: 46
--rw-rw-rw-  2.0 fat      267 b- defN 23-Apr-19 08:56 azureml/__init__.py
--rw-rw-rw-  2.0 fat   260065 b- defN 23-Apr-19 08:57 azureml/metrics/NOTICE
--rw-rw-rw-  2.0 fat      805 b- defN 23-Apr-19 08:57 azureml/metrics/__init__.py
--rw-rw-rw-  2.0 fat    49711 b- defN 23-Apr-19 08:57 azureml/metrics/_classification.py
--rw-rw-rw-  2.0 fat     7829 b- defN 23-Apr-19 08:57 azureml/metrics/_dataset_binning.py
--rw-rw-rw-  2.0 fat    24070 b- defN 23-Apr-19 08:57 azureml/metrics/_forecasting.py
--rw-rw-rw-  2.0 fat     5034 b- defN 23-Apr-19 08:57 azureml/metrics/_metric_base.py
--rw-rw-rw-  2.0 fat    25843 b- defN 23-Apr-19 08:57 azureml/metrics/_regression.py
--rw-rw-rw-  2.0 fat    32574 b- defN 23-Apr-19 08:57 azureml/metrics/_score.py
--rw-rw-rw-  2.0 fat    32735 b- defN 23-Apr-19 08:57 azureml/metrics/_scoring.py
--rw-rw-rw-  2.0 fat    23940 b- defN 23-Apr-19 08:57 azureml/metrics/_scoring_confidence.py
--rw-rw-rw-  2.0 fat    34427 b- defN 23-Apr-19 08:57 azureml/metrics/_scoring_utilities.py
--rw-rw-rw-  2.0 fat     2213 b- defN 23-Apr-19 08:57 azureml/metrics/_seq2seq_fill_mask.py
--rw-rw-rw-  2.0 fat     5135 b- defN 23-Apr-19 08:57 azureml/metrics/_seq2seq_qa.py
--rw-rw-rw-  2.0 fat     2137 b- defN 23-Apr-19 08:57 azureml/metrics/_seq2seq_summarization.py
--rw-rw-rw-  2.0 fat     1968 b- defN 23-Apr-19 08:57 azureml/metrics/_seq2seq_translation.py
--rw-rw-rw-  2.0 fat     5023 b- defN 23-Apr-19 08:57 azureml/metrics/_token_classification.py
--rw-rw-rw-  2.0 fat    44604 b- defN 23-Apr-19 08:57 azureml/metrics/_validation.py
--rw-rw-rw-  2.0 fat       40 b- defN 23-Apr-19 09:08 azureml/metrics/_version.py
--rw-rw-rw-  2.0 fat    14894 b- defN 23-Apr-19 08:57 azureml/metrics/azureml_classification_metrics.py
--rw-rw-rw-  2.0 fat     3975 b- defN 23-Apr-19 08:57 azureml/metrics/azureml_fill_mask_metrics.py
--rw-rw-rw-  2.0 fat    15561 b- defN 23-Apr-19 08:57 azureml/metrics/azureml_forecasting_metrics.py
--rw-rw-rw-  2.0 fat     6621 b- defN 23-Apr-19 08:57 azureml/metrics/azureml_metrics.py
--rw-rw-rw-  2.0 fat    13109 b- defN 23-Apr-19 08:57 azureml/metrics/azureml_od_is_metrics.py
--rw-rw-rw-  2.0 fat     4523 b- defN 23-Apr-19 08:57 azureml/metrics/azureml_qa_metrics.py
--rw-rw-rw-  2.0 fat     9157 b- defN 23-Apr-19 08:57 azureml/metrics/azureml_regression_metrics.py
--rw-rw-rw-  2.0 fat     3858 b- defN 23-Apr-19 08:57 azureml/metrics/azureml_summarization_metrics.py
--rw-rw-rw-  2.0 fat     7329 b- defN 23-Apr-19 08:57 azureml/metrics/azureml_text_classification_metrics.py
--rw-rw-rw-  2.0 fat     3985 b- defN 23-Apr-19 08:57 azureml/metrics/azureml_text_generation_metrics.py
--rw-rw-rw-  2.0 fat     4200 b- defN 23-Apr-19 08:57 azureml/metrics/azureml_text_ner_metrics.py
--rw-rw-rw-  2.0 fat     3646 b- defN 23-Apr-19 08:57 azureml/metrics/azureml_translation_metrics.py
--rw-rw-rw-  2.0 fat     4601 b- defN 23-Apr-19 08:57 azureml/metrics/bleu.py
--rw-rw-rw-  2.0 fat    35559 b- defN 23-Apr-19 08:57 azureml/metrics/constants.py
--rw-rw-rw-  2.0 fat     4820 b- defN 23-Apr-19 08:57 azureml/metrics/contract.py
--rw-rw-rw-  2.0 fat     6624 b- defN 23-Apr-19 08:57 azureml/metrics/exceptions.py
--rw-rw-rw-  2.0 fat    47616 b- defN 23-Apr-19 08:57 azureml/metrics/reference_codes.py
--rw-rw-rw-  2.0 fat     6424 b- defN 23-Apr-19 08:57 azureml/metrics/scoring.py
--rw-rw-rw-  2.0 fat    13249 b- defN 23-Apr-19 08:57 azureml/metrics/utilities.py
--rw-rw-rw-  2.0 fat      239 b- defN 23-Apr-19 08:59 azureml/metrics/od_is_eval/__init__.py
--rw-rw-rw-  2.0 fat    21196 b- defN 23-Apr-19 08:59 azureml/metrics/od_is_eval/incremental_voc_evaluator.py
--rw-rw-rw-  2.0 fat    17250 b- defN 23-Apr-19 08:59 azureml/metrics/od_is_eval/metric_computation_utils.py
--rw-rw-rw-  2.0 fat      859 b- defN 23-Apr-19 09:08 azureml_metrics-0.0.6.post1.dist-info/LICENSE.txt
--rw-rw-rw-  2.0 fat     1806 b- defN 23-Apr-19 09:08 azureml_metrics-0.0.6.post1.dist-info/METADATA
--rw-rw-rw-  2.0 fat       97 b- defN 23-Apr-19 09:08 azureml_metrics-0.0.6.post1.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        8 b- defN 23-Apr-19 09:08 azureml_metrics-0.0.6.post1.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     4286 b- defN 23-Apr-19 09:08 azureml_metrics-0.0.6.post1.dist-info/RECORD
-46 files, 813912 bytes uncompressed, 144304 bytes compressed:  82.3%
+Zip file size: 152891 bytes, number of entries: 46
+-rw-rw-rw-  2.0 fat      267 b- defN 23-May-15 22:05 azureml/__init__.py
+-rw-rw-rw-  2.0 fat   260065 b- defN 23-May-15 22:05 azureml/metrics/NOTICE
+-rw-rw-rw-  2.0 fat      805 b- defN 23-May-15 22:05 azureml/metrics/__init__.py
+-rw-rw-rw-  2.0 fat    49711 b- defN 23-May-15 22:05 azureml/metrics/_classification.py
+-rw-rw-rw-  2.0 fat     7829 b- defN 23-May-15 22:05 azureml/metrics/_dataset_binning.py
+-rw-rw-rw-  2.0 fat    32140 b- defN 23-May-15 22:05 azureml/metrics/_forecasting.py
+-rw-rw-rw-  2.0 fat     5034 b- defN 23-May-15 22:05 azureml/metrics/_metric_base.py
+-rw-rw-rw-  2.0 fat    25843 b- defN 23-May-15 22:05 azureml/metrics/_regression.py
+-rw-rw-rw-  2.0 fat    33472 b- defN 23-May-15 22:05 azureml/metrics/_score.py
+-rw-rw-rw-  2.0 fat    32841 b- defN 23-May-15 22:05 azureml/metrics/_scoring.py
+-rw-rw-rw-  2.0 fat    23940 b- defN 23-May-15 22:05 azureml/metrics/_scoring_confidence.py
+-rw-rw-rw-  2.0 fat    34530 b- defN 23-May-15 22:05 azureml/metrics/_scoring_utilities.py
+-rw-rw-rw-  2.0 fat     2213 b- defN 23-May-15 22:05 azureml/metrics/_seq2seq_fill_mask.py
+-rw-rw-rw-  2.0 fat     5135 b- defN 23-May-15 22:05 azureml/metrics/_seq2seq_qa.py
+-rw-rw-rw-  2.0 fat     2137 b- defN 23-May-15 22:05 azureml/metrics/_seq2seq_summarization.py
+-rw-rw-rw-  2.0 fat     1968 b- defN 23-May-15 22:05 azureml/metrics/_seq2seq_translation.py
+-rw-rw-rw-  2.0 fat     5023 b- defN 23-May-15 22:05 azureml/metrics/_token_classification.py
+-rw-rw-rw-  2.0 fat    44604 b- defN 23-May-15 22:05 azureml/metrics/_validation.py
+-rw-rw-rw-  2.0 fat       34 b- defN 23-May-15 22:11 azureml/metrics/_version.py
+-rw-rw-rw-  2.0 fat    14894 b- defN 23-May-15 22:05 azureml/metrics/azureml_classification_metrics.py
+-rw-rw-rw-  2.0 fat     3975 b- defN 23-May-15 22:05 azureml/metrics/azureml_fill_mask_metrics.py
+-rw-rw-rw-  2.0 fat    16778 b- defN 23-May-15 22:05 azureml/metrics/azureml_forecasting_metrics.py
+-rw-rw-rw-  2.0 fat     6621 b- defN 23-May-15 22:05 azureml/metrics/azureml_metrics.py
+-rw-rw-rw-  2.0 fat    13109 b- defN 23-May-15 22:05 azureml/metrics/azureml_od_is_metrics.py
+-rw-rw-rw-  2.0 fat     4523 b- defN 23-May-15 22:05 azureml/metrics/azureml_qa_metrics.py
+-rw-rw-rw-  2.0 fat     9157 b- defN 23-May-15 22:05 azureml/metrics/azureml_regression_metrics.py
+-rw-rw-rw-  2.0 fat     3858 b- defN 23-May-15 22:05 azureml/metrics/azureml_summarization_metrics.py
+-rw-rw-rw-  2.0 fat     7329 b- defN 23-May-15 22:05 azureml/metrics/azureml_text_classification_metrics.py
+-rw-rw-rw-  2.0 fat     3985 b- defN 23-May-15 22:05 azureml/metrics/azureml_text_generation_metrics.py
+-rw-rw-rw-  2.0 fat     4200 b- defN 23-May-15 22:05 azureml/metrics/azureml_text_ner_metrics.py
+-rw-rw-rw-  2.0 fat     3646 b- defN 23-May-15 22:05 azureml/metrics/azureml_translation_metrics.py
+-rw-rw-rw-  2.0 fat     4601 b- defN 23-May-15 22:05 azureml/metrics/bleu.py
+-rw-rw-rw-  2.0 fat    36132 b- defN 23-May-15 22:05 azureml/metrics/constants.py
+-rw-rw-rw-  2.0 fat     4820 b- defN 23-May-15 22:05 azureml/metrics/contract.py
+-rw-rw-rw-  2.0 fat     6624 b- defN 23-May-15 22:05 azureml/metrics/exceptions.py
+-rw-rw-rw-  2.0 fat    47695 b- defN 23-May-15 22:05 azureml/metrics/reference_codes.py
+-rw-rw-rw-  2.0 fat     6424 b- defN 23-May-15 22:05 azureml/metrics/scoring.py
+-rw-rw-rw-  2.0 fat    13249 b- defN 23-May-15 22:05 azureml/metrics/utilities.py
+-rw-rw-rw-  2.0 fat      239 b- defN 23-May-15 22:05 azureml/metrics/od_is_eval/__init__.py
+-rw-rw-rw-  2.0 fat    21196 b- defN 23-May-15 22:05 azureml/metrics/od_is_eval/incremental_voc_evaluator.py
+-rw-rw-rw-  2.0 fat    17250 b- defN 23-May-15 22:05 azureml/metrics/od_is_eval/metric_computation_utils.py
+-rw-rw-rw-  2.0 fat      859 b- defN 23-May-15 22:11 azureml_metrics-0.0.7.dist-info/LICENSE.txt
+-rw-rw-rw-  2.0 fat     1800 b- defN 23-May-15 22:11 azureml_metrics-0.0.7.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       97 b- defN 23-May-15 22:11 azureml_metrics-0.0.7.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        8 b- defN 23-May-15 22:11 azureml_metrics-0.0.7.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     4256 b- defN 23-May-15 22:11 azureml_metrics-0.0.7.dist-info/RECORD
+46 files, 824916 bytes uncompressed, 146011 bytes compressed:  82.3%
```

## zipnote {}

```diff
@@ -117,23 +117,23 @@
 
 Filename: azureml/metrics/od_is_eval/incremental_voc_evaluator.py
 Comment: 
 
 Filename: azureml/metrics/od_is_eval/metric_computation_utils.py
 Comment: 
 
-Filename: azureml_metrics-0.0.6.post1.dist-info/LICENSE.txt
+Filename: azureml_metrics-0.0.7.dist-info/LICENSE.txt
 Comment: 
 
-Filename: azureml_metrics-0.0.6.post1.dist-info/METADATA
+Filename: azureml_metrics-0.0.7.dist-info/METADATA
 Comment: 
 
-Filename: azureml_metrics-0.0.6.post1.dist-info/WHEEL
+Filename: azureml_metrics-0.0.7.dist-info/WHEEL
 Comment: 
 
-Filename: azureml_metrics-0.0.6.post1.dist-info/top_level.txt
+Filename: azureml_metrics-0.0.7.dist-info/top_level.txt
 Comment: 
 
-Filename: azureml_metrics-0.0.6.post1.dist-info/RECORD
+Filename: azureml_metrics-0.0.7.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## azureml/metrics/_forecasting.py

```diff
@@ -34,14 +34,16 @@
     @staticmethod
     def convert_to_list_of_str(val: Union[Any, Tuple[Any], List[Any]]) -> List[str]:
         """
         Convert an input to a list of str.
 
         Useful for converting grain column names or values to a list of strings.
         """
+        if not val:
+            return []
         val_collection = val if isinstance(val, list) or isinstance(val, tuple) else [val]
         return list(map(str, val_collection))
 
     def __init__(
         self,
         y_test: np.ndarray,
         y_pred: np.ndarray,
@@ -51,15 +53,16 @@
         y_std: Optional[float] = None,
         sample_weight: Optional[np.ndarray] = None,
         X_test: Optional[pd.DataFrame] = None,
         X_train: Optional[pd.DataFrame] = None,
         y_train: Optional[np.ndarray] = None,
         time_series_id_column_names: Optional[List[str]] = None,
         time_column_name: Optional[str] = None,
-        origin_column_name: Optional[Any] = None
+        origin_column_name: Optional[Any] = None,
+        **kwargs: Any
     ) -> None:
         """
         Initialize the forecasting metric class.
 
         :param y_test: True labels for the test set.
         :param y_pred: Predictions for each sample.
         :param horizons: The integer horizon alligned to each y_test. These values should be computed
@@ -509,7 +512,167 @@
             y_min_gr = self._y_min_dict.get(ts_id, np.min(y_act_one))
             y_max_gr = self._y_max_dict.get(ts_id, np.max(y_act_one))
             metric_inst = self._metric_class(y_act_one, y_pred_one, y_min=y_min_gr, y_max=y_max_gr, y_std=self._y_std,
                                              sample_weight=self._sample_weight)
             metrics.append(metric_inst.compute())
         self._X_test.drop(['actuals', 'predictions'], axis=1, inplace=True)
         return float(self._aggregation_function(metrics))
+
+
+class ForecastTsIDDistributionTable(ForecastingMetric, NonScalarMetric):
+    """Metric distribution by grain and forecast horizon.
+
+    :param y_test: True labels for the test set.
+    :param y_pred: Predictions for each sample.
+    :param horizons: The integer horizon alligned to each y_test. These values should be computed
+        by the timeseries transformer. If the timeseries transformer does not compute a horizon,
+        ensure all values are the same (ie. every y_test should be horizon 1.)
+    :param y_min_dict: The dictionary with minimal target values by grain.
+    :param y_max_dict: The dictionary with maximal target values by grain.
+    :param y_std: Standard deviation of the targets.
+    :param sample_weight: Weighting of each sample in the calculation.
+    :param X_test: The inputs which were used to compute the predictions.
+    :param X_train: The inputs which were used to train the model.
+    :param y_train: The targets which were used to train the model.
+    :param time_series_id_column_names: The time series id column names also known as
+                                        grain column names.
+    :param time_column_name: The time column name.
+    :param origin_column_name: The origin time column name.
+    :param metric_class: The scalar metric to wrap.
+    :param aggregation_function: The function used to aggregate by grain metrics.
+    """
+
+    SCHEMA_TYPE = constants.SCHEMA_TYPE_DISTRIBUTION_TABLE
+    SCHEMA_VERSION = "1.0.0"
+
+    _ACTUALS = 'actuals'
+    _PREDICTIONS = 'predictions'
+
+    def __init__(
+            self,
+            y_test: np.ndarray,
+            y_pred: np.ndarray,
+            horizons: np.ndarray,
+            sample_weight: Optional[np.ndarray] = None,
+            X_test: Optional[pd.DataFrame] = None,
+            X_train: Optional[pd.DataFrame] = None,
+            y_train: Optional[np.ndarray] = None,
+            time_series_id_column_names: Optional[List[str]] = None,
+            time_column_name: Optional[str] = None,
+            origin_column_name: Optional[Any] = None,
+            y_min_dict: Optional[Dict[Union[str, Tuple[str]], float]] = None,
+            y_max_dict: Optional[Dict[Union[str, Tuple[str]], float]] = None,
+            **kwargs
+    ) -> None:
+        super().__init__(
+            y_test,
+            y_pred,
+            horizons,
+            sample_weight=sample_weight,
+            X_test=X_test,
+            X_train=X_train,
+            y_train=y_train,
+            time_series_id_column_names=time_series_id_column_names,
+            time_column_name=time_column_name,
+            origin_column_name=origin_column_name,
+            **kwargs)
+        self._y_min_dict = y_min_dict if y_min_dict else {}
+        self._y_max_dict = y_max_dict if y_max_dict else {}
+
+    def _compute_all_regression_metrics(
+            self,
+            data: pd.DataFrame,
+            y_min: float,
+            y_max: float,
+            time_series_id_dict: Dict[str, Any]) -> Dict[str, float]:
+        """
+        Calculate all the regression metrics for one time series.
+
+        :param data: The data frame, containing predictions and ground truth.
+        :param y_min: Minimal actual value to be used for normalization.
+        :param y_max: Maximal actual value to be used for normalization.
+        :param time_series_id_dict: The dictionary containing
+                                    ts_id -> value columns.
+        :return: The dictionary with metics.
+        """
+        actuals = data.pop(ForecastTsIDDistributionTable._ACTUALS).values
+        predictions = data.pop(ForecastTsIDDistributionTable._PREDICTIONS).values
+        metrics = {}
+        for metric_name in constants.REGRESSION_SCALAR_SET:
+            metric_class = _scoring_utilities.get_metric_class(metric_name)
+            metrics_inst = metric_class(
+                actuals,
+                predictions,
+                y_min=y_min,
+                y_max=y_max,
+                y_std=self._y_std,
+                sample_weight=self._sample_weight)
+            metrics[metric_name] = metrics_inst.compute()
+        metrics.update(time_series_id_dict)
+        return metrics
+
+    def _to_ts_id_dict(self, time_series_id: Union[Any, Tuple[Any]], ts_id_column_names: List[str]) -> Dict[str, Any]:
+        """
+        Return the time series ID-s in form of dictionary.
+
+        :param time_series_id: The time series id.
+        :param ts_id_column_names: Time series id column names.
+        :return: The dictionary column name -> column value
+        """
+        if not isinstance(time_series_id, tuple):
+            time_series_id = [time_series_id]
+        return {col: val for col, val in zip(ts_id_column_names, time_series_id)}
+
+    def compute(self) -> Dict[str, Any]:
+        """Calculate metrics by grain."""
+        if self._time_series_id_column_names is None:
+            raise ForecastMetricGrainAbsent(
+                exception_message=("Time series ID column name is required "
+                                   "to compute Forecast time series ID distribution table"),
+                target="_grain_column_name",
+                reference_code=ReferenceCodes._TS_METRIX_NO_GRAIN_DISTRIBUTION)
+        self._X_test[ForecastTsIDDistributionTable._ACTUALS] = self._y_test
+        self._X_test[ForecastTsIDDistributionTable._PREDICTIONS] = self._y_pred
+        index = self.convert_to_list_of_str(self._time_series_id_column_names)
+        metrics_list = []
+        for ts_id, df_one in self._X_test.groupby(index):
+            # If we do not have minimum and maximum from the training set, we will use min and max from the test set.
+            y_min_gr = self._y_min_dict.get(ts_id, np.min(df_one[ForecastTsIDDistributionTable._ACTUALS].values))
+            y_max_gr = self._y_max_dict.get(ts_id, np.max(df_one[ForecastTsIDDistributionTable._ACTUALS].values))
+            time_series_id_dict = self._to_ts_id_dict(ts_id, index)
+            if constants._TimeSeriesInternal.FORECAST_ORIGIN_COLUMN_NAME in self._X_test.columns:
+                for fc_origin, df_one_orig in df_one.groupby(
+                        constants._TimeSeriesInternal.FORECAST_ORIGIN_COLUMN_NAME):
+                    all_metrics = self._compute_all_regression_metrics(
+                        df_one_orig, y_min_gr, y_max_gr, time_series_id_dict)
+                    all_metrics[constants._TimeSeriesInternal.FORECAST_ORIGIN_COLUMN_NAME] = fc_origin.isoformat()
+                    metrics_list.append(all_metrics)
+            else:
+                all_metrics = self._compute_all_regression_metrics(df_one, y_min_gr, y_max_gr, time_series_id_dict)
+                metrics_list.append(all_metrics)
+
+        self._X_test.drop([ForecastTsIDDistributionTable._ACTUALS,
+                           ForecastTsIDDistributionTable._PREDICTIONS], axis=1, inplace=True)
+        ret = NonScalarMetric._data_to_dict(
+            ForecastTable.SCHEMA_TYPE,
+            ForecastTable.SCHEMA_VERSION,
+            metrics_list)
+        return cast(Dict[str, Any], _scoring_utilities.make_json_safe(ret))
+
+    @staticmethod
+    def aggregate(scores: List[Dict[str, Any]]) -> Dict[str, Any]:
+        """
+        Fold several scores from a computed metric together.
+
+        :param scores: List of computed scores.
+        :return: Aggregated score.
+        """
+        mt_table = []
+        for dt in scores:
+            mt_table.extend(dt['data'])
+
+        metrics_df = pd.DataFrame.from_records(mt_table)
+        index = list(set(metrics_df.columns) - constants.REGRESSION_SCALAR_SET)
+        data = metrics_df.groupby(index, as_index=False).agg(np.nanmean)
+        data_dt = data.to_dict(orient='records')
+        ret = NonScalarMetric._data_to_dict(ForecastResiduals.SCHEMA_TYPE, ForecastResiduals.SCHEMA_VERSION, data_dt)
+        return cast(Dict[str, Any], _scoring_utilities.make_json_safe(ret))
```

## azureml/metrics/_score.py

```diff
@@ -34,15 +34,15 @@
     """Given task type, y_test, y_pred or y_pred_proba compute metrics for the respective task.
 
         :param task_type: Accepts an argument of type constants.Tasks for which metrics have to be computed.
             Can accept from any of the values constants.Tasks.CLASSIFICATION, constants.Tasks.REGRESSION,
             constants.Tasks.TEXT_CLASSIFICATION, constants.Tasks.TEXT_CLASSIFICATION_MULTILABEL,
             constants.Tasks.TEXT_NER, constants.Tasks.SUMMARIZATION, constants.Tasks.TRANSLATION,
             constants.Tasks.QUESTION_ANSWERING, constants.Tasks.IMAGE_OBJECT_DETECTION,
-            constants.Tasks.IMAGE_INSTANCE_SEGMENTATION.
+            constants.Tasks.IMAGE_INSTANCE_SEGMENTATION, constants.Tasks.FORECASTING.
         :param y_test: Ground truths or reference values.
             optional for computing few of language_modeling metrics.
         :param y_pred: Prediction values.
         :param y_pred_proba: Predicted probability values.
 
         Example for multiclass classification:
         --------------------------------------
@@ -168,14 +168,36 @@
                 "boxes": np.array([[160, 120, 320, 240]], dtype=np.float32),
                 "masks": [_rle_mask_from_bbox([1, 0, 2, 100], 640, 640)],
                 "classes": np.array([1]),
                 "scores": np.array([0.75]),
             }]
         result = compute_metrics(task_type=constants.Tasks.IMAGE_INSTANCE_SEGMENTATION, y_test=y_test,
                                         y_pred=y_pred,image_meta_info=image_meta_info)
+
+        Example for forecasting:
+        >>>from azureml.metrics import compute_metrics, constants
+        >>>X = pd.DataFrame({
+           'date': pd.date_range('2001-01-01', freq='D', periods=42),
+           'ts_id': 'a',
+           'target': np.random.rand(42)
+           })
+        >>>X_train = X.iloc[:-10]
+        >>>y_train = X_train.pop('target').values
+        >>>X_test = X.iloc[-10:]
+        >>>y_test = X_test.pop('target').values
+        >>>y_pred = np.random.rand(10)
+        >>>result = compute_metrics(
+            task_type=constants.Tasks.FORECASTING,
+            y_test=y_test,
+            y_pred=y_pred,
+            X_train=X_train,
+            y_train=y_train,
+            time_column_name='date',
+            time_series_id_column_names=['ts_id'],
+            X_test=X_test)
     """
     # Step 1: Check either y_pred or y_pred_proba exist
     # Step 2: Instantiate Metrics Class object on the basis of task type
     #   and pass in necessary parameters while creating object
     # Step 3: Call compute method of class object to compute and fetch metrics
     if y_test is None:
         if task_type in [constants.Tasks.FILL_MASK]:
@@ -465,34 +487,34 @@
           y_test: Union[np.ndarray, pd.DataFrame, List],
           **kwargs) -> Dict[str, Dict[str, Any]]:
     """Given task type, model, y_test, y_pred or y_pred_proba compute predictions and the respective metrics.
 
         :param task_type: Accepts an argument of type constants.Tasks for which metrics have to be computed.
             Can accept from any of the values constants.Tasks.CLASSIFICATION, constants.Tasks.REGRESSION,
             constants.Tasks.TEXT_CLASSIFICATION, constants.Tasks.TEXT_CLASSIFICATION_MULTILABEL,
-            constants.Tasks.TEXT_NER.
+            constants.Tasks.TEXT_NER, constants.Tasks.FORECASTING.
         :param model: Any model which has a callable predict method that generates predictions.
         :param X_test: Test data which is sent to the model to compute predictions.
         :param y_test: Ground truths or references.
     """
     # Step 1: Generate predictions using model
     # Step 2: Extract whether predict proba is required, compute and add to kwargs if yes
     # Step 3: Call compute metrics method and pass appropriate kwargs to compute and fetch metrics
 
     if not (hasattr(model, "predict") and callable(getattr(model, 'predict'))):
         raise Exception("Model should have callable predict method.")
 
     try:
         if hasattr(model, "forecast") and callable(getattr(model, 'forecast')):
-            # In the forecast data we are not gyaranteered to have the same
+            # In the forecast data we are not guaranteed to have the same
             # dimension of output data as the input so we have to preaggregate
             # the data here.
             if 'X_train' in kwargs and 'y_train' in kwargs:
                 kwargs['X_train'], kwargs['y_train'] = model.preaggregate_data_set(
-                    kwargs['X_train'], kwargs['y_train'])
+                    kwargs['X_train'], kwargs['y_train'], is_training_set=True)
             X_test_agg, y_test = model.preaggregate_data_set(X_test, y_test)
             y_pred, _ = model.forecast(X_test)
             X_test = X_test_agg
             # Take forecasting-specific parameters from the model.
             kwargs["time_series_id_column_names"] = model.grain_column_names
             kwargs["time_column_name"] = model.time_column_name
         else:
```

## azureml/metrics/_scoring.py

```diff
@@ -298,15 +298,18 @@
                     y_std=y_std,
                     sample_weight=sample_weight,
                     X_test=X_test,
                     X_train=X_train,
                     y_train=y_train,
                     time_series_id_column_names=time_series_id_column_names,
                     time_column_name=time_column_name,
-                    origin_column_name=origin_column_name)
+                    origin_column_name=origin_column_name,
+                    y_min_dict=y_min_dict,
+                    y_max_dict=y_max_dict
+                )
             elif name in constants.REGRESSION_NORMALIZED_SET:
                 # Calculate the metrics by grain/time_series_id.
                 metric = _NormalizedRegressorWrapper(
                     y_test=y_test,
                     y_pred=y_pred,
                     horizons=horizons,
                     y_min_dict=y_min_dict,
```

## azureml/metrics/_scoring_utilities.py

```diff
@@ -9,14 +9,15 @@
 import numpy as np
 import sklearn.metrics
 import sklearn.preprocessing
 from sklearn.base import TransformerMixin
 
 from azureml.metrics import _classification, _forecasting, _regression, _token_classification, constants
 from azureml.metrics import _seq2seq_translation, _seq2seq_summarization, _seq2seq_qa
+
 from azureml.metrics import _seq2seq_fill_mask
 from azureml.metrics._metric_base import Metric
 from azureml.metrics.constants import MetricExtrasConstants, Metric as metric_constants
 from azureml.metrics.contract import Contract
 from azureml.metrics.exceptions import DataErrorException, ResourceException
 from azureml.metrics.reference_codes import ReferenceCodes
 
@@ -339,15 +340,16 @@
         constants.MAPE: _regression.MAPE,
         constants.RESIDUALS: _regression.Residuals,
         constants.PREDICTED_TRUE: _regression.PredictedTrue
     }  # type: Dict[str, Type[Metric]]
     forecasting_classes = {
         constants.FORECASTING_MAPE: _forecasting.ForecastMAPE,
         constants.FORECASTING_RESIDUALS: _forecasting.ForecastResiduals,
-        constants.FORECASTING_TABLE: _forecasting.ForecastTable
+        constants.FORECASTING_TABLE: _forecasting.ForecastTable,
+        constants.FORECASTING_TS_ID_DISTRIBUTION_TABLE: _forecasting.ForecastTsIDDistributionTable
     }
     translation_classes = {
         constants.Metric.TranslationBleu_1: _seq2seq_translation.Bleu,
         constants.Metric.TranslationBleu_2: _seq2seq_translation.Bleu,
         constants.Metric.TranslationBleu_3: _seq2seq_translation.Bleu,
         constants.Metric.TranslationBleu_4: _seq2seq_translation.Bleu
     }
```

## azureml/metrics/_version.py

```diff
@@ -1,2 +1,2 @@
-ver = "0.0.6"
-selfver = "0.0.6.post1"
+ver = "0.0.7"
+selfver = "0.0.7"
```

## azureml/metrics/azureml_forecasting_metrics.py

```diff
@@ -76,15 +76,16 @@
                         the one attached to the exception is used.
         """
 
         if X_train is not None and y_train is not None:
             all_metrics = AzureMLForecastingMetrics.list_metrics()
         else:
             # Do not generate artifacts if training set was not provided.
-            all_metrics = constants.Metric.SCALAR_REGRESSION_SET
+            all_metrics = (constants.Metric.SCALAR_REGRESSION_SET
+                           | constants.Metric.FORECASTING_NONSCALAR_SET_NO_TRAINING)
         if metrics:
             all_metrics = list(set(metrics).intersection(set(all_metrics)))
             if len(all_metrics) < len(metrics):
                 logger.warn("Some metrics were not evaluated because "
                             "they are not supported or data context was not provided.")
                 fc_metrics = AzureMLForecastingMetrics.list_metrics()
                 if not all_metrics and any(m not in fc_metrics for m in metrics):
@@ -144,16 +145,16 @@
                 self._X_train = X_train.drop(constants._TimeSeriesInternal.DUMMY_TARGET_COLUMN, axis=1, inplace=False)
                 self._X_train = self._reindex_dataframe_maybe(self._X_train, index)
             else:
                 # Copy the data frame not to break the index on the original one
                 self._X_train = self._reindex_dataframe_maybe(self._X_train.copy(), index)
 
             for ts_id, df_one in X_train.groupby(self._time_series_id_column_names):
-                self._y_min_dict[ts_id] = df_one[constants._TimeSeriesInternal.DUMMY_TARGET_COLUMN].min()
-                self._y_max_dict[ts_id] = df_one[constants._TimeSeriesInternal.DUMMY_TARGET_COLUMN].max()
+                self._y_min_dict[ts_id] = df_one[constants._TimeSeriesInternal.DUMMY_TARGET_COLUMN].min(skipna=True)
+                self._y_max_dict[ts_id] = df_one[constants._TimeSeriesInternal.DUMMY_TARGET_COLUMN].max(skipna=True)
 
             X_train.drop(drop_columns, axis=1, inplace=True)
 
         Contract.assert_true(aggregation_method in (np.mean, np.median), target='aggregation_method',
                              message='only numpy.mean and numpy.median are allowed as an aggregation functions.',
                              reference_code=ReferenceCodes._METRIC_INVALID_AGGREGATION_FUNCTION)
         self._aggregation_method = aggregation_method
@@ -186,14 +187,33 @@
         if set(X.index.names) != set(index):
             if X.index.names[0]:
                 X.reset_index(drop=False, inplace=True)
             if index[0]:
                 X.set_index(index, inplace=True)
         return X
 
+    def _remove_nan_from_target(
+            self, X: pd.DataFrame,
+            y_test: np.ndarray,
+            y_pred: np.ndarray) -> Tuple[pd.DataFrame, np.ndarray, np.ndarray]:
+        """
+        Remove The NaN-s pesent in target.
+
+        :param y_test: True labels for the test set.
+        :param y_pred: Predictions for each sample.
+        :param X_test: The regressors for the test data set. It must align with y_train and
+                       to contain time and time series ID.
+        """
+        X['pred'] = y_pred
+        X['actual'] = y_test
+        X.dropna(subset=['pred', 'actual'], axis=0, inplace=True)
+        y_pred = X.pop('pred').values
+        y_test = X.pop('actual').values
+        return X, y_test, y_pred
+
     def compute(self,
                 y_test: Union[np.ndarray, pd.DataFrame, List],
                 y_pred: Union[np.ndarray, pd.DataFrame, List],
                 X_test: pd.DataFrame) -> Dict[str, Dict[str, Any]]:
         """
         Given the scored data, generate metrics for forecasting task.
 
@@ -218,14 +238,19 @@
         if (self._time_series_id_column_names == [constants._TimeSeriesInternal.DUMMY_GRAIN_COLUMN]):
             if(constants._TimeSeriesInternal.DUMMY_GRAIN_COLUMN not in X_test.columns
                and constants._TimeSeriesInternal.DUMMY_GRAIN_COLUMN not in X_test.index.names):
                 X_test[constants._TimeSeriesInternal.DUMMY_GRAIN_COLUMN] = \
                     constants._TimeSeriesInternal.DUMMY_GRAIN_COLUMN
                 drop_columns.append(constants._TimeSeriesInternal.DUMMY_GRAIN_COLUMN)
         X_test = self._reindex_dataframe_maybe(X_test, self._get_tsds_index(X_test))
+        # Remove NaN values from X_test.
+        # If predictions or actuals are None, we will raise the error later.
+        if (y_test is not None and y_pred is not None
+                and len(y_test) == len(y_pred) and len(y_pred) == X_test.shape[0]):
+            X_test, y_test, y_pred = self._remove_nan_from_target(X_test, y_test, y_pred)
         # Calculate the horizons by grains
         ORDER = 'order'
         if constants._TimeSeriesInternal.HORIZON_NAME in X_test.columns:
             horizons = X_test[constants._TimeSeriesInternal.HORIZON_NAME].values
         else:
             horizons = np.zeros(X_test.shape[0])
             X_test[ORDER] = np.arange(X_test.shape[0])
```

## azureml/metrics/constants.py

```diff
@@ -102,14 +102,15 @@
     PredictedTrue = "predicted_true"
 
     # Forecast
     ForecastMAPE = 'forecast_mean_absolute_percentage_error'
     ForecastSMAPE = 'forecast_symmetric_mean_absolute_percentage_error'
     ForecastResiduals = 'forecast_residuals'
     ForecastTable = 'forecast_table'
+    ForecastTsIDDistributionTable = "forecast_time_series_id_distribution_table"
 
     # Sequence to Sequence Metrics
     # Seq2Seq Translation
     TranslationBleu_1 = "bleu_1"
     TranslationBleu_2 = "bleu_2"
     TranslationBleu_3 = "bleu_3"
     TranslationBleu_4 = "bleu_4"
@@ -298,19 +299,25 @@
 
     TEXT_CLASSIFICATION_MULTILABEL_PRIMARY_SET = {Accuracy}
 
     TEXT_NER_PRIMARY_SET = {Accuracy}
 
     NONSCALAR_FORECAST_SET = {
         ForecastMAPE, ForecastResiduals,
-        ForecastTable
+        ForecastTable, ForecastTsIDDistributionTable
     }
 
     FORECAST_SET = (NONSCALAR_FORECAST_SET)
 
+    # The set of non scalar metrics allowed even if the
+    # training set was not provided.
+    FORECASTING_NONSCALAR_SET_NO_TRAINING = {
+        ForecastTsIDDistributionTable
+    }
+
     # Metrics set for Sequence to Sequence Tasks
     SCALAR_TRANSLATION_SET = {
         TranslationBleu_1,
         TranslationBleu_2,
         TranslationBleu_3,
         TranslationBleu_4,
     }
@@ -681,21 +688,24 @@
 REGRESSION_PRIMARY_SET = {R2_SCORE, SPEARMAN, NORM_RMSE, NORM_MEAN_ABS_ERROR}
 
 # Forecasting metrics
 
 FORECASTING_MAPE = 'forecast_mean_absolute_percentage_error'
 FORECASTING_RESIDUALS = 'forecast_residuals'
 FORECASTING_TABLE = 'forecast_table'
+FORECASTING_TS_ID_DISTRIBUTION_TABLE = "forecast_time_series_id_distribution_table"
 
 FORECASTING_NONSCALAR_SET = {
     FORECASTING_MAPE,
     FORECASTING_RESIDUALS,
-    FORECASTING_TABLE
+    FORECASTING_TABLE,
+    FORECASTING_TS_ID_DISTRIBUTION_TABLE
 }
 
+
 FORECASTING_SET = FORECASTING_NONSCALAR_SET
 
 # Image Classification Metrics
 
 IMAGE_CLASSIFICATION_SET = {ACCURACY}
 
 IMAGE_CLASSIFICATION_MULTILABEL_CLASSIFICATION_SET = {IOU}
@@ -843,14 +853,15 @@
 SCHEMA_TYPE_ACCURACY_TABLE = "accuracy_table"
 SCHEMA_TYPE_FORECAST_HORIZON_TABLE = "forecast_horizon_table"
 SCHEMA_TYPE_CONFUSION_MATRIX = "confusion_matrix"
 SCHEMA_TYPE_CLASSIFICATION_REPORT = "classification_report"
 SCHEMA_TYPE_RESIDUALS = "residuals"
 SCHEMA_TYPE_PREDICTIONS = "predictions"
 SCHEMA_TYPE_MAPE = "mape_table"
+SCHEMA_TYPE_DISTRIBUTION_TABLE = "forecast_time_series_id_distribution_table"
 
 # Ranges
 
 SCORE_UPPER_BOUND = sys.float_info.max
 
 MULTILABEL_PREDICTION_THRESHOLD = 0.5
 
@@ -1191,7 +1202,8 @@
 
 class _TimeSeriesInternal:
     """Define the time series constants"""
 
     DUMMY_GRAIN_COLUMN = '_automl_dummy_grain_col'
     DUMMY_TARGET_COLUMN = '_automl_target_col'
     HORIZON_NAME = 'horizon_origin'
+    FORECAST_ORIGIN_COLUMN_NAME = '_automl_forecast_origin'
```

## azureml/metrics/reference_codes.py

```diff
@@ -533,14 +533,15 @@
     _TS_INPUT_IS_NOT_TSDF_ROLL_WIN = '650fb71b-3751-4058-9f6b-92c8761dd69b'
     _TS_INPUT_IS_NOT_TSDF_SHORT_GRAIN = '650fb71b-3751-4058-9f6b-82c8761dd69b'
     _TS_INPUT_IS_NOT_TSDF_SHORT_GRAIN_TRANS = '650fb71b-3751-4058-9f6b-72c8761dd69b'
     _TS_INPUT_IS_NOT_TSDF_TM_IDX_FEA_CHK_INPUT = 'a50fb71b-3751-4058-9f6b-62c8761dd69b'
     _TS_INPUT_IS_NOT_TSDF_TM_IDX_FEA_NOISY = '350fb71b-3751-4058-9f6b-52c8761dd69b'
     _TS_INPUT_IS_NOT_TSDF_TM_TS_UTIL = 'e50fb71b-3751-4058-9f6b-36c8761dd69d'
     _TS_METRIX_NO_GRAIN = '627b234c-90f4-11ec-b270-8c1645fec84b'
+    _TS_METRIX_NO_GRAIN_DISTRIBUTION = 'e83fcb4a-e47b-11ed-8533-8c1645fec84b'
     _TS_METRIX_NO_TRAIN = '6db6d564-90f4-11ec-95d2-8c1645fec84b'
     _TS_METRIX_NO_VALID = '7edd9f94-90f4-11ec-9f99-8c1645fec84b'
     _TS_METRIX_WRAPPER_NO_GRAIN = 'eda0ca38-7b1b-11ed-b856-3c52825a9bd1'
     _TS_METRIX_WRAPPER_NO_VALID = '05c1aee5-7b1c-11ed-bcac-3c52825a9bd1'
     _TS_NO_USABLE_GRAINS = '210abce8-c7ef-11eb-9855-04d3b0c6010a'
     _TS_ONE_VALUE_PER_GRAIN_RSCRIPT = '81bfc3ee-d90a-11ea-9819-04d3b0c6010a'
     _TS_ONE_VALUE_PER_GRAIN_TSUTIL = '710e2118-d90a-11ea-ad5b-04d3b0c6010a'
```

## Comparing `azureml_metrics-0.0.6.post1.dist-info/LICENSE.txt` & `azureml_metrics-0.0.7.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `azureml_metrics-0.0.6.post1.dist-info/METADATA` & `azureml_metrics-0.0.7.dist-info/METADATA`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: azureml-metrics
-Version: 0.0.6.post1
+Version: 0.0.7
 Summary: Contains the ML and non-Azure specific common code associated     with AzureML metrics.
 Home-page: https://docs.microsoft.com/python/api/overview/azure/ml/?view=azure-ml-py
 Author: Microsoft Corp
 License: https://aka.ms/azureml-sdk-license
 Platform: UNKNOWN
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Intended Audience :: Developers
@@ -27,15 +27,15 @@
 Requires-Dist: scipy (<=1.5.3,>=1.0.0)
 Requires-Dist: scikit-learn (<=0.24.2,>=0.19.0)
 Requires-Dist: sklearn-pandas (<=1.7.0,>=1.4.0)
 Requires-Dist: seqeval
 Requires-Dist: nltk
 Requires-Dist: evaluate (>=0.3.0)
 Requires-Dist: rouge-score (>=0.1.2)
-Requires-Dist: torch (>=1.13.1)
+Requires-Dist: torch (>=1.13.0)
 Requires-Dist: transformers (>=4.16.0)
 Requires-Dist: numpy (<=1.21.6,>=1.16.0) ; python_version < "3.8"
 Requires-Dist: numpy (<=1.22.3,>=1.16.0) ; python_version >= "3.8"
 
 The azureml-metrics package is a package for calculating classification and regression metrics
 used by azureml-train-automl package. Customer could also use it for computing the metrics.
```

## Comparing `azureml_metrics-0.0.6.post1.dist-info/RECORD` & `azureml_metrics-0.0.7.dist-info/RECORD`

 * *Files 18% similar despite different names*

```diff
@@ -1,46 +1,46 @@
 azureml/__init__.py,sha256=_a9uPeIDwvRCe07_84QytQ_Qv9bbO1lGzZnfArCb66o,267
 azureml/metrics/NOTICE,sha256=fIZkNZdLtnzMQ56XPLfz1N9FPTv-CO4BHBD1xl3_gYk,260065
 azureml/metrics/__init__.py,sha256=3mDFscFE4vSB2MuBR96MTYQohN11sSFguJIL9prpowc,805
 azureml/metrics/_classification.py,sha256=AJozqv3Z4NouGAB3_dwl6l0nebivjoAOej8YM2I_RHk,49711
 azureml/metrics/_dataset_binning.py,sha256=wD5ea9LMolPEMYqpWxDAgCYOHu2-a1OKCnFMFnevTCQ,7829
-azureml/metrics/_forecasting.py,sha256=zqmdiLKN5f6XCmmKfkqQ3cFTmOsvK0SMbeXgPj3yyz8,24070
+azureml/metrics/_forecasting.py,sha256=q48eOvUb4I8qrVsZIK1EorgY-nGXzvPaV_uNlLYuuPg,32140
 azureml/metrics/_metric_base.py,sha256=jE3ZZIoNbHoLJJWyhcOloAO2aIB7ST3fhd3eBUtKjpA,5034
 azureml/metrics/_regression.py,sha256=wxIL2eh82d7ABsPG04SkBxYsIRiZhEhzBCRIZJw6P5I,25843
-azureml/metrics/_score.py,sha256=8Hv3NrJ6C7s9bp1MELXjADBAnmjAhGrFawU47H6s3ec,32574
-azureml/metrics/_scoring.py,sha256=e-s9PTYxMEnQakwgxBg8HaN3nJGpreidQOyCrta0kyM,32735
+azureml/metrics/_score.py,sha256=GX8ixQjGASEvHMlRtDOyCOpq0tJuRZbtnRXkNSx-Kuk,33472
+azureml/metrics/_scoring.py,sha256=ZBjRi43PUVv6hQSt9xTNtEQZR1FVgijYQXvyZ-IHaY8,32841
 azureml/metrics/_scoring_confidence.py,sha256=eTaG0ud_4U9O16zrw213-_J_iik4BqJuvIqTDi6eNMs,23940
-azureml/metrics/_scoring_utilities.py,sha256=JGGhsNxiVocZBhvXlPr9yCiQMHWv-BejZUSvkmd-sEM,34427
+azureml/metrics/_scoring_utilities.py,sha256=hsozHmQP64ts2vXcw_LWDZyisWVWU5rc73HUQUdI2HE,34530
 azureml/metrics/_seq2seq_fill_mask.py,sha256=0nckgKC4h-TtCUBWWgsJGoOxIDEyYkReWETQFm3Du6s,2213
 azureml/metrics/_seq2seq_qa.py,sha256=BtsT8wqJhrgtaf1x91Opm2jnhXsBt2YVEcDzJNHEk48,5135
 azureml/metrics/_seq2seq_summarization.py,sha256=ZTXtotmF35KwyI3WDZ14A14I3oLQpItKa49pXjMAg38,2137
 azureml/metrics/_seq2seq_translation.py,sha256=UrkMnPgpQv8q_T6qzYWUpcespno4E-MTXvDjIacA5Ho,1968
 azureml/metrics/_token_classification.py,sha256=xZ9JJ4R8GfUNxKqQhVHDttj4AuA7U6H4jCiqCuH9nAc,5023
 azureml/metrics/_validation.py,sha256=HXWL0j1XDuI5mlhEhIVvnLWcyh04rTf-7dfRhUPuHY4,44604
-azureml/metrics/_version.py,sha256=uh0VWzHdEQl7gw9VpWtMeegnzWKSHzOpEVYk20d7uEs,40
+azureml/metrics/_version.py,sha256=zawyv7zEI6MXSytrylagjBd73hxk9yGZGoxcRkYe78g,34
 azureml/metrics/azureml_classification_metrics.py,sha256=YTNL92lBAIR5qvaUlI3nY_MQArfrlw4M3Poq_EKRt9c,14894
 azureml/metrics/azureml_fill_mask_metrics.py,sha256=vbuHl_j57p1vYZXUeyJ8gCrB3APrCk5fH0qTxrb4DJ4,3975
-azureml/metrics/azureml_forecasting_metrics.py,sha256=BhvVu6HJ8HVz2cXkf-39T5RjQJUqSlDxuAq8Vo8tmos,15561
+azureml/metrics/azureml_forecasting_metrics.py,sha256=H14BVIw8Uj7Wiw-kYasPEqcOPk7dLQwhrTiVh4aSenA,16778
 azureml/metrics/azureml_metrics.py,sha256=fqQREyFxQ_YlKXFoRTILm1XD3FaB6C66nFQJ6pyJArg,6621
 azureml/metrics/azureml_od_is_metrics.py,sha256=ch2gucXD5Z4dOZmWKkRQvgrT8vNkvewODNEu7qfKQaI,13109
 azureml/metrics/azureml_qa_metrics.py,sha256=8f3FI8CNmf8HSt_5ekZ5MjnFJLoSRdPANez7M7XhGtM,4523
 azureml/metrics/azureml_regression_metrics.py,sha256=nswYZ3QgzonObxedGvsVnBQ8-oD8Spyel38DnzrqeMQ,9157
 azureml/metrics/azureml_summarization_metrics.py,sha256=TQ9EDtkbuyDn7BKuEJULze_H5pzbfCIAnVDlo723MNA,3858
 azureml/metrics/azureml_text_classification_metrics.py,sha256=MMWj1IpiG2xUsreX43fWUjR1-NMDNBwYdqRkEQKuLRA,7329
 azureml/metrics/azureml_text_generation_metrics.py,sha256=bPhxT863PUYpy8jhF4dFOrZcqwt3aU7scwhUo8RnoN8,3985
 azureml/metrics/azureml_text_ner_metrics.py,sha256=GzWtzGMEOHqft4RloaIGqiKR86eqjcM3EM-P9F67t6M,4200
 azureml/metrics/azureml_translation_metrics.py,sha256=ausHWlkwuszJfi-KuqAnGZCRGZDSMntMVs-uRwtFwco,3646
 azureml/metrics/bleu.py,sha256=CCa6LCAm2bm1LtBk9dNXTo-0C8Ptws44WPksQp1PNRQ,4601
-azureml/metrics/constants.py,sha256=1AnBgyB-iwOFxioqoAlEE1Os7zvdQWc0wVNEeRBxpck,35559
+azureml/metrics/constants.py,sha256=q8I5JlMQLPRKWpfrh6ueAL3HB3Th-hFPFa0r19prDPQ,36132
 azureml/metrics/contract.py,sha256=Ny1KJb35i4V4tCOE_qgqfIkEWKfX_PrjoFCyoo_WUoE,4820
 azureml/metrics/exceptions.py,sha256=cv4hLN-5skqXgUu_9tE-Wnv5azBH7A6CAAcDBMdywSw,6624
-azureml/metrics/reference_codes.py,sha256=r58ZAM2mWtzW9IJrO8RJd8vQx_KU3a2tUF4XZjp3ooI,47616
+azureml/metrics/reference_codes.py,sha256=iRPFo6au4uyqr2qqTDtP0YyHjhfC7JVCzGA9GYA1ofE,47695
 azureml/metrics/scoring.py,sha256=BF5jicht3V4ZbeQQ_2Si2yVfu_1QQds1iHFOIdTNaVM,6424
 azureml/metrics/utilities.py,sha256=4oU6p74Eb1p3uu0CeTnofMWrtIosAgCEXsJMrV1t8S4,13249
 azureml/metrics/od_is_eval/__init__.py,sha256=VjnWZi7NjkYq6T61O4RMa6Pz5Nzqe8qoSpU7TlyFHQI,239
 azureml/metrics/od_is_eval/incremental_voc_evaluator.py,sha256=kXkNgBNN7t7aMG0MWA5fwQfVKZAacgTvyQJ2X5682TM,21196
 azureml/metrics/od_is_eval/metric_computation_utils.py,sha256=f7fpAiwlqI-MMM5q5eDDZN5kYwY4fy05zvApPJE5_Sk,17250
-azureml_metrics-0.0.6.post1.dist-info/LICENSE.txt,sha256=GBoIyZ-6vJ4xjRc8U3wTw4EfkuaEdVTm_gbr1Nm8uDI,859
-azureml_metrics-0.0.6.post1.dist-info/METADATA,sha256=rHlRC9yZ-7VEXAoitNw7rqmPKoZE-EQ_k7mD-Atn4Zw,1806
-azureml_metrics-0.0.6.post1.dist-info/WHEEL,sha256=YUYzQ6UQdoqxXjimOitTqynltBCkwY6qlTfTh2IzqQU,97
-azureml_metrics-0.0.6.post1.dist-info/top_level.txt,sha256=ZOeEa0TAXo6i5wOjwBoqfIGEuxOcKuscGgNSpizqREY,8
-azureml_metrics-0.0.6.post1.dist-info/RECORD,,
+azureml_metrics-0.0.7.dist-info/LICENSE.txt,sha256=GBoIyZ-6vJ4xjRc8U3wTw4EfkuaEdVTm_gbr1Nm8uDI,859
+azureml_metrics-0.0.7.dist-info/METADATA,sha256=3Bd-B1UE-2Ce4aGthDOhwY7UQ6zhMJgmS39zecDArnk,1800
+azureml_metrics-0.0.7.dist-info/WHEEL,sha256=YUYzQ6UQdoqxXjimOitTqynltBCkwY6qlTfTh2IzqQU,97
+azureml_metrics-0.0.7.dist-info/top_level.txt,sha256=ZOeEa0TAXo6i5wOjwBoqfIGEuxOcKuscGgNSpizqREY,8
+azureml_metrics-0.0.7.dist-info/RECORD,,
```

