# Comparing `tmp/azureml_evaluate_mlflow-0.0.6.post1-py3-none-any.whl.zip` & `tmp/azureml_evaluate_mlflow-0.0.7-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,56 +1,63 @@
-Zip file size: 162410 bytes, number of entries: 54
--rw-rw-rw-  2.0 fat      251 b- defN 23-Apr-19 08:56 azureml/__init__.py
--rw-rw-rw-  2.0 fat      183 b- defN 23-Apr-19 08:57 azureml/evaluate/__init__.py
--rw-rw-rw-  2.0 fat   477828 b- defN 23-Apr-19 08:58 azureml/evaluate/mlflow/NOTICE
--rw-rw-rw-  2.0 fat     7071 b- defN 23-Apr-19 08:58 azureml/evaluate/mlflow/__init__.py
--rw-rw-rw-  2.0 fat       25 b- defN 23-Apr-19 09:08 azureml/evaluate/mlflow/_version.py
--rw-rw-rw-  2.0 fat      373 b- defN 23-Apr-19 08:58 azureml/evaluate/mlflow/exceptions.py
--rw-rw-rw-  2.0 fat      183 b- defN 23-Apr-19 09:00 azureml/evaluate/mlflow/_loader_modules/__init__.py
--rw-rw-rw-  2.0 fat     2769 b- defN 23-Apr-19 09:00 azureml/evaluate/mlflow/_loader_modules/hftransformers.py
--rw-rw-rw-  2.0 fat      457 b- defN 23-Apr-19 09:00 azureml/evaluate/mlflow/_loader_modules/sklearn.py
--rw-rw-rw-  2.0 fat    32732 b- defN 23-Apr-19 09:00 azureml/evaluate/mlflow/aml/__init__.py
--rw-rw-rw-  2.0 fat      930 b- defN 23-Apr-19 09:00 azureml/evaluate/mlflow/aml/constants.py
--rw-rw-rw-  2.0 fat    14964 b- defN 23-Apr-19 09:00 azureml/evaluate/mlflow/aml/model.py
--rw-rw-rw-  2.0 fat    12432 b- defN 23-Apr-19 09:00 azureml/evaluate/mlflow/aml/utils.py
--rw-rw-rw-  2.0 fat    38122 b- defN 23-Apr-19 09:00 azureml/evaluate/mlflow/hftransformers/__init__.py
--rw-rw-rw-  2.0 fat     3895 b- defN 23-Apr-19 09:00 azureml/evaluate/mlflow/hftransformers/_task_based_pipeline_predictors.py
--rw-rw-rw-  2.0 fat    32935 b- defN 23-Apr-19 09:00 azureml/evaluate/mlflow/hftransformers/_task_based_predictors.py
--rw-rw-rw-  2.0 fat     1050 b- defN 23-Apr-19 09:00 azureml/evaluate/mlflow/hftransformers/constants.py
--rw-rw-rw-  2.0 fat     4875 b- defN 23-Apr-19 09:00 azureml/evaluate/mlflow/hftransformers/dataset_wrappers.py
--rw-rw-rw-  2.0 fat     2675 b- defN 23-Apr-19 09:00 azureml/evaluate/mlflow/hftransformers/utils.py
--rw-rw-rw-  2.0 fat     1832 b- defN 23-Apr-19 09:00 azureml/evaluate/mlflow/models/__init__.py
--rw-rw-rw-  2.0 fat      540 b- defN 23-Apr-19 09:01 azureml/evaluate/mlflow/models/evaluation/__init__.py
--rw-rw-rw-  2.0 fat    42463 b- defN 23-Apr-19 09:01 azureml/evaluate/mlflow/models/evaluation/base.py
--rw-rw-rw-  2.0 fat    42935 b- defN 23-Apr-19 09:01 azureml/evaluate/mlflow/models/evaluation/default_evaluator.py
--rw-rw-rw-  2.0 fat     2473 b- defN 23-Apr-19 09:01 azureml/evaluate/mlflow/models/evaluation/evaluator_registry.py
--rw-rw-rw-  2.0 fat      183 b- defN 23-Apr-19 09:01 azureml/evaluate/mlflow/models/evaluation/azureml/__init__.py
--rw-rw-rw-  2.0 fat     1286 b- defN 23-Apr-19 09:01 azureml/evaluate/mlflow/models/evaluation/azureml/_classifier_evaluator.py
--rw-rw-rw-  2.0 fat     1830 b- defN 23-Apr-19 09:01 azureml/evaluate/mlflow/models/evaluation/azureml/_classifier_multilabel_evaluator.py
--rw-rw-rw-  2.0 fat     1204 b- defN 23-Apr-19 09:01 azureml/evaluate/mlflow/models/evaluation/azureml/_ner_evaluator.py
--rw-rw-rw-  2.0 fat     1163 b- defN 23-Apr-19 09:01 azureml/evaluate/mlflow/models/evaluation/azureml/_qna_evaluator.py
--rw-rw-rw-  2.0 fat     1058 b- defN 23-Apr-19 09:01 azureml/evaluate/mlflow/models/evaluation/azureml/_regressor_evaluator.py
--rw-rw-rw-  2.0 fat     1270 b- defN 23-Apr-19 09:01 azureml/evaluate/mlflow/models/evaluation/azureml/_summarization_evaluator.py
--rw-rw-rw-  2.0 fat      802 b- defN 23-Apr-19 09:01 azureml/evaluate/mlflow/models/evaluation/azureml/_task_evaluator.py
--rw-rw-rw-  2.0 fat     1751 b- defN 23-Apr-19 09:01 azureml/evaluate/mlflow/models/evaluation/azureml/_task_evaluator_factory.py
--rw-rw-rw-  2.0 fat     1266 b- defN 23-Apr-19 09:01 azureml/evaluate/mlflow/models/evaluation/azureml/_translation_evaluator.py
--rw-rw-rw-  2.0 fat    11116 b- defN 23-Apr-19 09:01 azureml/evaluate/mlflow/models/evaluation/azureml/azureml_evaluator.py
--rw-rw-rw-  2.0 fat      183 b- defN 23-Apr-19 08:57 tests/aml/__init__.py
--rw-rw-rw-  2.0 fat    51203 b- defN 23-Apr-19 08:57 tests/aml/test_model_export_with_class_and_artifacts.py
--rw-rw-rw-  2.0 fat    30719 b- defN 23-Apr-19 08:57 tests/aml/test_model_export_with_loader_module_and_data_path.py
--rw-rw-rw-  2.0 fat      183 b- defN 23-Apr-19 08:57 tests/hftransformers/__init__.py
--rw-rw-rw-  2.0 fat      317 b- defN 23-Apr-19 08:57 tests/hftransformers/hf_test_predict.py
--rw-rw-rw-  2.0 fat      154 b- defN 23-Apr-19 08:57 tests/hftransformers/preprocess.py
--rw-rw-rw-  2.0 fat     4019 b- defN 23-Apr-19 08:57 tests/hftransformers/test_hf_load_pipeline.py
--rw-rw-rw-  2.0 fat    32623 b- defN 23-Apr-19 08:57 tests/hftransformers/test_hf_model_export.py
--rw-rw-rw-  2.0 fat      183 b- defN 23-Apr-19 08:57 tests/models/__init__.py
--rw-rw-rw-  2.0 fat     4537 b- defN 23-Apr-19 08:57 tests/models/dummy_evaluator.py
--rw-rw-rw-  2.0 fat    17911 b- defN 23-Apr-19 08:57 tests/models/test_azureml_evaluator.py
--rw-rw-rw-  2.0 fat    43050 b- defN 23-Apr-19 08:57 tests/models/test_default_evaluator.py
--rw-rw-rw-  2.0 fat    37145 b- defN 23-Apr-19 08:57 tests/models/test_evaluation.py
--rw-rw-rw-  2.0 fat    38423 b- defN 23-Apr-19 08:57 tests/models/utils.py
--rw-rw-rw-  2.0 fat     1021 b- defN 23-Apr-19 09:08 azureml_evaluate_mlflow-0.0.6.post1.dist-info/LICENSE.txt
--rw-rw-rw-  2.0 fat     1692 b- defN 23-Apr-19 09:08 azureml_evaluate_mlflow-0.0.6.post1.dist-info/METADATA
--rw-rw-rw-  2.0 fat       97 b- defN 23-Apr-19 09:08 azureml_evaluate_mlflow-0.0.6.post1.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       14 b- defN 23-Apr-19 09:08 azureml_evaluate_mlflow-0.0.6.post1.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     5708 b- defN 23-Apr-19 09:08 azureml_evaluate_mlflow-0.0.6.post1.dist-info/RECORD
-54 files, 1016104 bytes uncompressed, 152910 bytes compressed:  85.0%
+Zip file size: 173718 bytes, number of entries: 61
+-rw-rw-rw-  2.0 fat      251 b- defN 23-May-15 22:05 azureml/__init__.py
+-rw-rw-rw-  2.0 fat      183 b- defN 23-May-15 22:05 azureml/evaluate/__init__.py
+-rw-rw-rw-  2.0 fat   477828 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/NOTICE
+-rw-rw-rw-  2.0 fat     7071 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/__init__.py
+-rw-rw-rw-  2.0 fat       19 b- defN 23-May-15 22:10 azureml/evaluate/mlflow/_version.py
+-rw-rw-rw-  2.0 fat      695 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/constants.py
+-rw-rw-rw-  2.0 fat      373 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/exceptions.py
+-rw-rw-rw-  2.0 fat      183 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/_loader_modules/__init__.py
+-rw-rw-rw-  2.0 fat     2769 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/_loader_modules/hftransformers.py
+-rw-rw-rw-  2.0 fat      457 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/_loader_modules/sklearn.py
+-rw-rw-rw-  2.0 fat    33109 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/aml/__init__.py
+-rw-rw-rw-  2.0 fat      930 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/aml/constants.py
+-rw-rw-rw-  2.0 fat    14964 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/aml/model.py
+-rw-rw-rw-  2.0 fat    12432 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/aml/utils.py
+-rw-rw-rw-  2.0 fat    38122 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/hftransformers/__init__.py
+-rw-rw-rw-  2.0 fat     3895 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/hftransformers/_task_based_pipeline_predictors.py
+-rw-rw-rw-  2.0 fat    32935 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/hftransformers/_task_based_predictors.py
+-rw-rw-rw-  2.0 fat     1050 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/hftransformers/constants.py
+-rw-rw-rw-  2.0 fat     4875 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/hftransformers/dataset_wrappers.py
+-rw-rw-rw-  2.0 fat     2675 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/hftransformers/utils.py
+-rw-rw-rw-  2.0 fat     1832 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/__init__.py
+-rw-rw-rw-  2.0 fat      540 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/__init__.py
+-rw-rw-rw-  2.0 fat    42463 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/base.py
+-rw-rw-rw-  2.0 fat      375 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/constants.py
+-rw-rw-rw-  2.0 fat    42935 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/default_evaluator.py
+-rw-rw-rw-  2.0 fat     2473 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/evaluator_registry.py
+-rw-rw-rw-  2.0 fat      183 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/__init__.py
+-rw-rw-rw-  2.0 fat     1286 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/_classifier_evaluator.py
+-rw-rw-rw-  2.0 fat     1830 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/_classifier_multilabel_evaluator.py
+-rw-rw-rw-  2.0 fat     1261 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/_fill_mask_evaluator.py
+-rw-rw-rw-  2.0 fat     2786 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/_forecaster_evaluator.py
+-rw-rw-rw-  2.0 fat     2113 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/_image_classifier_evaluator.py
+-rw-rw-rw-  2.0 fat     1204 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/_ner_evaluator.py
+-rw-rw-rw-  2.0 fat     1163 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/_qna_evaluator.py
+-rw-rw-rw-  2.0 fat     1058 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/_regressor_evaluator.py
+-rw-rw-rw-  2.0 fat     1270 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/_summarization_evaluator.py
+-rw-rw-rw-  2.0 fat      802 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/_task_evaluator.py
+-rw-rw-rw-  2.0 fat     2561 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/_task_evaluator_factory.py
+-rw-rw-rw-  2.0 fat     1273 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/_text_generation_evaluator.py
+-rw-rw-rw-  2.0 fat     1266 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/_translation_evaluator.py
+-rw-rw-rw-  2.0 fat    12303 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/azureml_evaluator.py
+-rw-rw-rw-  2.0 fat      183 b- defN 23-May-15 22:05 tests/aml/__init__.py
+-rw-rw-rw-  2.0 fat    51203 b- defN 23-May-15 22:05 tests/aml/test_model_export_with_class_and_artifacts.py
+-rw-rw-rw-  2.0 fat    30719 b- defN 23-May-15 22:05 tests/aml/test_model_export_with_loader_module_and_data_path.py
+-rw-rw-rw-  2.0 fat      183 b- defN 23-May-15 22:05 tests/hftransformers/__init__.py
+-rw-rw-rw-  2.0 fat      317 b- defN 23-May-15 22:05 tests/hftransformers/hf_test_predict.py
+-rw-rw-rw-  2.0 fat      154 b- defN 23-May-15 22:05 tests/hftransformers/preprocess.py
+-rw-rw-rw-  2.0 fat     4019 b- defN 23-May-15 22:05 tests/hftransformers/test_hf_load_pipeline.py
+-rw-rw-rw-  2.0 fat    32623 b- defN 23-May-15 22:05 tests/hftransformers/test_hf_model_export.py
+-rw-rw-rw-  2.0 fat      183 b- defN 23-May-15 22:05 tests/models/__init__.py
+-rw-rw-rw-  2.0 fat     4537 b- defN 23-May-15 22:05 tests/models/dummy_evaluator.py
+-rw-rw-rw-  2.0 fat    27305 b- defN 23-May-15 22:05 tests/models/test_azureml_evaluator.py
+-rw-rw-rw-  2.0 fat    43050 b- defN 23-May-15 22:05 tests/models/test_default_evaluator.py
+-rw-rw-rw-  2.0 fat    37145 b- defN 23-May-15 22:05 tests/models/test_evaluation.py
+-rw-rw-rw-  2.0 fat    13542 b- defN 23-May-15 22:05 tests/models/test_forecaster_evaluator.py
+-rw-rw-rw-  2.0 fat    50782 b- defN 23-May-15 22:05 tests/models/utils.py
+-rw-rw-rw-  2.0 fat     1021 b- defN 23-May-15 22:10 azureml_evaluate_mlflow-0.0.7.dist-info/LICENSE.txt
+-rw-rw-rw-  2.0 fat     1686 b- defN 23-May-15 22:10 azureml_evaluate_mlflow-0.0.7.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       97 b- defN 23-May-15 22:10 azureml_evaluate_mlflow-0.0.7.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       14 b- defN 23-May-15 22:10 azureml_evaluate_mlflow-0.0.7.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     6513 b- defN 23-May-15 22:10 azureml_evaluate_mlflow-0.0.7.dist-info/RECORD
+61 files, 1063069 bytes uncompressed, 162872 bytes compressed:  84.7%
```

## zipnote {}

```diff
@@ -9,14 +9,17 @@
 
 Filename: azureml/evaluate/mlflow/__init__.py
 Comment: 
 
 Filename: azureml/evaluate/mlflow/_version.py
 Comment: 
 
+Filename: azureml/evaluate/mlflow/constants.py
+Comment: 
+
 Filename: azureml/evaluate/mlflow/exceptions.py
 Comment: 
 
 Filename: azureml/evaluate/mlflow/_loader_modules/__init__.py
 Comment: 
 
 Filename: azureml/evaluate/mlflow/_loader_modules/hftransformers.py
@@ -60,14 +63,17 @@
 
 Filename: azureml/evaluate/mlflow/models/evaluation/__init__.py
 Comment: 
 
 Filename: azureml/evaluate/mlflow/models/evaluation/base.py
 Comment: 
 
+Filename: azureml/evaluate/mlflow/models/evaluation/constants.py
+Comment: 
+
 Filename: azureml/evaluate/mlflow/models/evaluation/default_evaluator.py
 Comment: 
 
 Filename: azureml/evaluate/mlflow/models/evaluation/evaluator_registry.py
 Comment: 
 
 Filename: azureml/evaluate/mlflow/models/evaluation/azureml/__init__.py
@@ -75,14 +81,23 @@
 
 Filename: azureml/evaluate/mlflow/models/evaluation/azureml/_classifier_evaluator.py
 Comment: 
 
 Filename: azureml/evaluate/mlflow/models/evaluation/azureml/_classifier_multilabel_evaluator.py
 Comment: 
 
+Filename: azureml/evaluate/mlflow/models/evaluation/azureml/_fill_mask_evaluator.py
+Comment: 
+
+Filename: azureml/evaluate/mlflow/models/evaluation/azureml/_forecaster_evaluator.py
+Comment: 
+
+Filename: azureml/evaluate/mlflow/models/evaluation/azureml/_image_classifier_evaluator.py
+Comment: 
+
 Filename: azureml/evaluate/mlflow/models/evaluation/azureml/_ner_evaluator.py
 Comment: 
 
 Filename: azureml/evaluate/mlflow/models/evaluation/azureml/_qna_evaluator.py
 Comment: 
 
 Filename: azureml/evaluate/mlflow/models/evaluation/azureml/_regressor_evaluator.py
@@ -93,14 +108,17 @@
 
 Filename: azureml/evaluate/mlflow/models/evaluation/azureml/_task_evaluator.py
 Comment: 
 
 Filename: azureml/evaluate/mlflow/models/evaluation/azureml/_task_evaluator_factory.py
 Comment: 
 
+Filename: azureml/evaluate/mlflow/models/evaluation/azureml/_text_generation_evaluator.py
+Comment: 
+
 Filename: azureml/evaluate/mlflow/models/evaluation/azureml/_translation_evaluator.py
 Comment: 
 
 Filename: azureml/evaluate/mlflow/models/evaluation/azureml/azureml_evaluator.py
 Comment: 
 
 Filename: tests/aml/__init__.py
@@ -138,26 +156,29 @@
 
 Filename: tests/models/test_default_evaluator.py
 Comment: 
 
 Filename: tests/models/test_evaluation.py
 Comment: 
 
+Filename: tests/models/test_forecaster_evaluator.py
+Comment: 
+
 Filename: tests/models/utils.py
 Comment: 
 
-Filename: azureml_evaluate_mlflow-0.0.6.post1.dist-info/LICENSE.txt
+Filename: azureml_evaluate_mlflow-0.0.7.dist-info/LICENSE.txt
 Comment: 
 
-Filename: azureml_evaluate_mlflow-0.0.6.post1.dist-info/METADATA
+Filename: azureml_evaluate_mlflow-0.0.7.dist-info/METADATA
 Comment: 
 
-Filename: azureml_evaluate_mlflow-0.0.6.post1.dist-info/WHEEL
+Filename: azureml_evaluate_mlflow-0.0.7.dist-info/WHEEL
 Comment: 
 
-Filename: azureml_evaluate_mlflow-0.0.6.post1.dist-info/top_level.txt
+Filename: azureml_evaluate_mlflow-0.0.7.dist-info/top_level.txt
 Comment: 
 
-Filename: azureml_evaluate_mlflow-0.0.6.post1.dist-info/RECORD
+Filename: azureml_evaluate_mlflow-0.0.7.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## azureml/evaluate/mlflow/_version.py

```diff
@@ -1 +1 @@
-VERSION = "0.0.6.post1"
+VERSION = "0.0.7"
```

## azureml/evaluate/mlflow/aml/__init__.py

```diff
@@ -120,16 +120,16 @@
 
     def __repr__(self):
         info = {}
         if self._model_meta is not None:
             if hasattr(self._model_meta, "run_id") and self._model_meta.run_id is not None:
                 info["run_id"] = self._model_meta.run_id
             if (
-                    hasattr(self._model_meta, "artifact_path")
-                    and self._model_meta.artifact_path is not None
+                    hasattr(self._model_meta, "artifact_path") and
+                    self._model_meta.artifact_path is not None
             ):
                 info["artifact_path"] = self._model_meta.artifact_path
             info["flavor"] = self._model_meta.flavors[FLAVOR_NAME]["loader_module"]
         return yaml.safe_dump({"mlflow.aml.loaded_model": info}, default_flow_style=False)
 
     # This is  a hack for enabling kwargs in aml. We need to get  better way of
     # passing the kwargs while runtime
@@ -165,47 +165,54 @@
         if input_schema is not None:
             data = _enforce_schema(data, input_schema)
         if self._support_kwargs_for_predict():
             return self._model_impl.predict_proba(data, **kwargs)
         return self._model_impl.predict_proba(data)
 
 
-# class AMLForecastModel(AMLGenericModel):
-#     def __init__(self, model_meta, model_impl):
-#         super().__init__(model_meta, model_impl)
-#         self._model_impl = model_impl
-#         if not hasattr(self._model_impl, "forecast"):
-#             raise MlflowException("Model implementation is missing required forecast method.")
-#
-#     def forecast(self, data: AzureMLInput):
-#         input_schema = self.metadata.get_input_schema()
-#         if input_schema is not None:
-#             data = _enforce_schema(data, input_schema)
-#         return self._model_impl.forecast(data)
-#
-#     def rolling_forecast(self, data: AzureMLInput, steps=1):
-#         if not hasattr(self._model_impl, "rolling_forecast"):
-#             return self.forecast(data)
-#         input_schema = self.metadata.get_input_schema()
-#         if input_schema is not None:
-#             data = _enforce_schema(data, input_schema)
-#         return self._model_impl.rolling_forecast(data, steps)
+class AMLForecastModel(AMLGenericModel):
+    def __init__(self, model_meta, model_impl):
+        super().__init__(model_meta, model_impl)
+        self._model_impl = model_impl
+        if not hasattr(self._model_impl, "forecast"):
+            raise MlflowException("Model implementation is missing required forecast method.")
+
+    def forecast(self, data: AzureMLInput):
+        input_schema = self.metadata.get_input_schema()
+        if input_schema is not None:
+            data = _enforce_schema(data, input_schema)
+        return self._model_impl.forecast(data, ignore_data_errors=True)
+
+    def rolling_forecast(self, data_x: AzureMLInput, data_y: AzureMLInput, step: int = 1):
+        if not hasattr(self._model_impl, "rolling_forecast"):
+            return self.forecast(data_x)
+        input_schema = self.metadata.get_input_schema()
+        if input_schema is not None:
+            data_x = _enforce_schema(data_x, input_schema)
+            data_y = _enforce_schema(data_y, input_schema)
+        return self._model_impl.rolling_forecast(data_x, data_y, step=step, ignore_data_errors=True)
 
 
 def _get_model(model_type, model_meta, model_impl):
     is_model_impl_classifier = hasattr(model_impl, "predict_proba") and hasattr(model_impl, "predict")
     is_aml_model_classifier = hasattr(model_impl, "aml_model") and all(hasattr(model_impl.aml_model, attr)
                                                                        for attr in ["predict", "predict_proba"])
+    is_forecaster = (
+        hasattr(model_impl, "forecast") and
+        hasattr(model_impl, "rolling_forecast") and
+        hasattr(model_impl, "forecast_quantiles"))
     # Todo: Should AMLCLassifierModel be returned if model_type=="classifier"
-    if model_type in ["classifier", "multiclass", "text-classifier"] and not (is_model_impl_classifier
-                                                                              or is_aml_model_classifier):
+    if model_type in ["classifier", "multiclass", "text-classifier"] and not (is_model_impl_classifier or
+                                                                              is_aml_model_classifier):
         _logger.warning("The model provided does not contains predict_proba method. This is required in order to "
                         "use evaluate functionality")
     if is_model_impl_classifier or is_aml_model_classifier:
         return AMLClassifierModel(model_meta, model_impl)
+    elif is_forecaster:
+        return AMLForecastModel(model_meta, model_impl)
     return AMLGenericModel(model_meta, model_impl)
 
 
 def load_model(
         model_uri: str, model_type: str = "", suppress_warnings: bool = False, dst_path: str = None
 ) -> AMLModel:
     """
```

## azureml/evaluate/mlflow/models/evaluation/azureml/_task_evaluator_factory.py

```diff
@@ -1,35 +1,47 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 from azureml.evaluate.mlflow.models.evaluation.azureml._classifier_evaluator import ClassifierEvaluator
 from azureml.evaluate.mlflow.models.evaluation.azureml._classifier_multilabel_evaluator import \
     ClassifierMultilabelEvaluator
+from azureml.evaluate.mlflow.models.evaluation.azureml._forecaster_evaluator import ForecasterEvaluator
 from azureml.evaluate.mlflow.models.evaluation.azureml._regressor_evaluator import RegressorEvaluator
 from azureml.evaluate.mlflow.models.evaluation.azureml._translation_evaluator import TranslationEvaluator
 from azureml.evaluate.mlflow.models.evaluation.azureml._qna_evaluator import QnAEvaluator
 from azureml.evaluate.mlflow.models.evaluation.azureml._summarization_evaluator import SummarizationEvaluator
 from azureml.evaluate.mlflow.models.evaluation.azureml._ner_evaluator import NerEvaluator
+from azureml.evaluate.mlflow.models.evaluation.azureml._text_generation_evaluator import TextGenerationEvaluator
+from azureml.evaluate.mlflow.models.evaluation.azureml._fill_mask_evaluator import FillMaskEvaluator
+from azureml.evaluate.mlflow.models.evaluation.azureml._image_classifier_evaluator import ImageClassifierEvaluator
 
 
 class EvaluatorFactory:
-
     def __init__(self):
         self._evaluators = {
             "classifier": ClassifierEvaluator,
             "classifier-multilabel": ClassifierMultilabelEvaluator,
             "multiclass": ClassifierEvaluator,
             "regressor": RegressorEvaluator,
             "ner": NerEvaluator,
             "text-ner": NerEvaluator,
             "text-classifier": ClassifierEvaluator,
             'text-classifier-multilabel': ClassifierMultilabelEvaluator,
             "translation": TranslationEvaluator,
             "summarization": SummarizationEvaluator,
-            "question-answering": QnAEvaluator
+            "question-answering": QnAEvaluator,
+            "forecaster": ForecasterEvaluator,
+            "text-generation": TextGenerationEvaluator,
+            "fill-mask": FillMaskEvaluator,
+            "image-classifier": ImageClassifierEvaluator,
+            "image-classifier-multilabel": ImageClassifierEvaluator,
         }
 
+    @property
+    def supported_tasks(self):
+        return list(self._evaluators.keys())
+
     def get_evaluator(self, model_type):
         return self._evaluators[model_type]()
 
     def register(self, name, obj):
         self._evaluators[name] = obj
```

## azureml/evaluate/mlflow/models/evaluation/azureml/azureml_evaluator.py

```diff
@@ -1,33 +1,35 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 from azureml.core.run import Run, _OfflineRun
 from azureml.core.workspace import Workspace
-import azureml.evaluate.mlflow as azureml_mlflow
 from azureml.metrics import constants, _scoring_utilities
 from mlflow.models.evaluation.artifacts import JsonEvaluationArtifact
+from azureml.evaluate.mlflow.constants import ForecastColumns
 from azureml.evaluate.mlflow.models.evaluation.azureml._task_evaluator_factory import EvaluatorFactory
+from azureml.evaluate.mlflow.models.evaluation.base import ModelEvaluator, EvaluationResult
+from azureml.evaluate.mlflow.exceptions import AzureMLMLFlowException
 
 import pandas as pd
 import os
-from azureml.evaluate.mlflow.models.evaluation.base import ModelEvaluator, EvaluationResult
 import logging
-from azureml.evaluate.mlflow.exceptions import AzureMLMLFlowException
+import azureml.evaluate.mlflow as azureml_mlflow
+import numpy as np
 
 _logger = logging.getLogger(__name__)
 DEFAULT_OUTPUT_FOLDER = "evaluation_result"
 PREDICTION_COLUMN_NAME = "predictions"
 
 
 class AzureMLEvaluator(ModelEvaluator):
     def __init__(self) -> None:
         super().__init__()
-        self.supported_model_types = ["classifier", "regressor", "ner", "text-classifier", "text-ner",
-                                      "classifier-multilabel", "translation", "summarization", "question-answering"]
+        self.supported_model_types = EvaluatorFactory().supported_tasks
+        self.list_metrics = [constants.Metric.FMPerplexity]
 
     def can_evaluate(self, *, model_type, evaluator_config, **kwargs) -> bool:
         """
         Helper Function to check if model type supported by AzureMLEvaluator
         @param model_type:
         @param evaluator_config:
         @param kwargs:
@@ -75,20 +77,28 @@
 
     def _log_metrics(self):
         """
         Log metrices in the run
         """
         table_scores = {}
         nonscalar_scores = {}
+        list_scores = {}
 
         for name, score in self.artifacts.items():
             if score is None:
                 continue
             elif _scoring_utilities.is_table_metric(name):
                 table_scores[name] = score
+            elif name in self.list_metrics:
+                try:
+                    list_scores[name] = list(score)
+                    if name == constants.Metric.FMPerplexity:
+                        self.metrics["mean_" + name] = np.mean(score)
+                except TypeError:
+                    _logger.warning(f"{name} is not of type list.")
             elif name in constants.Metric.NONSCALAR_FULL_SET:
                 nonscalar_scores[name] = score
             elif name in constants.TrainingResultsType.ALL_TIME:
                 # Filter out time metrics as we do not log these
                 pass
             else:
                 _logger.warning("Unknown metric {}. Will not log.".format(name))
@@ -106,14 +116,24 @@
             try:
                 self.run.log_table(name, score)
                 if self.parent_run is not None:
                     self.parent_run.log_table(name, score)
             except Exception:
                 raise AzureMLMLFlowException(f"Failed to log table metric {name} with value {score}")
 
+        for name, score in list_scores.items():
+            try:
+                if len(score) >= 250:
+                    continue
+                self.run.log_list(name, score)
+                if self.parent_run is not None:
+                    self.parent_run.log_list(name, score)
+            except Exception:
+                raise AzureMLMLFlowException(f"Failed to log list metric {name} with value {score}")
+
         # Log the non-scalar metrics. (Currently, these are all artifact-based.)
         for name, score in nonscalar_scores.items():
             try:
                 if name == constants.Metric.AccuracyTable:
                     self.run.log_accuracy_table(name, score)
                     if self.parent_run is not None:
                         self.parent_run.log_accuracy_table(name, score)
@@ -125,17 +145,14 @@
                     self.run.log_residuals(name, score)
                     if self.parent_run is not None:
                         self.parent_run.log_residuals(name, score)
                 elif name == constants.Metric.PredictedTrue:
                     self.run.log_predictions(name, score)
                     if self.parent_run is not None:
                         self.parent_run.log_predictions(name, score)
-                elif name in constants.Metric.NONSCALAR_FORECAST_SET:
-                    # Filter out non-scalar forecasting metrics as we do not log these yet
-                    pass
                 else:
                     _logger.warning("Unsupported non-scalar metric {}. Will not log.".format(name))
             except Exception:
                 raise AzureMLMLFlowException(f"Failed to log non-scalar metric {name} with value {score}")
 
     def _log_and_return_evaluation_result(self):
         """
@@ -177,15 +194,17 @@
         self.run = Run.get_context()
         self.parent_run = None
         if isinstance(self.run, _OfflineRun):
             workspace, resource_group, subscription = self._parse_aml_tracking_uri()
             if workspace:
                 ws = Workspace(subscription_id=subscription, workspace_name=workspace, resource_group=resource_group)
                 self.run = Run.get(ws, run_id)
-                self.parent_run = self.run.parent
+                # self.parent_run = self.run.parent
+                # Not Adding parent logging right now
+                self.parent_run = None
             else:
                 _logger.warning("Failed to parse AzureML credentials. Using OfflineRun")
         else:
             self.parent_run = self.run.parent
         _logger.info(f"Setting up AML Run Config with run_id {self.run.id}")
 
     def evaluate(self,
@@ -237,14 +256,21 @@
         #         f"According to the evaluation dataset label values, the model type looks like "
         #         f"{inferred_model_type}, but you specified model type {model_type}. Please "
         #         f"verify that you set the `model_type` and `dataset` arguments correctly."
         #     )
 
         self.evaluator = EvaluatorFactory().get_evaluator(model_type=model_type)
         metrics, predictions = self.evaluator.evaluate(model, self.X, self.y, **evaluator_config)
+        if model_type == "forecaster":
+            # If the model type is a forecaster, we cannot guarantee the same dimensions of input
+            # and output, so we are returning a data frame and replace self.X and self.y
+            # by the values taken from it.
+            self.y = predictions.pop(ForecastColumns._ACTUAL_COLUMN_NAME).values
+            self.X = predictions
+            predictions = self.X.pop(ForecastColumns._FORECAST_COLUMN_NAME).values
 
         self.metrics.update(metrics[constants.Metric.Metrics])
         artifacts = metrics[constants.Metric.Artifacts]
         self.artifacts.update(artifacts)
         self.predictions = predictions
 
         return self._log_and_return_evaluation_result()
```

## tests/models/test_azureml_evaluator.py

```diff
@@ -1,52 +1,26 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 # flake8: noqa
 import ast
 
 from azureml.metrics.azureml_regression_metrics import AzureMLRegressionMetrics
-from azureml.metrics.azureml_classification_metrics import AzureMLClassificationMetrics
-import matplotlib.pyplot as plt
-from unittest import mock
 import numpy as np
-import json
 import pandas as pd
-import pytest
-import sklearn
-from contextlib import nullcontext as does_not_raise
 from azureml.metrics import compute_metrics, constants
 import azureml.evaluate.mlflow as mlflow
 
-from mlflow.exceptions import MlflowException
 from azureml.evaluate.mlflow.models.evaluation import evaluate
-from azureml.evaluate.mlflow.models.evaluation.default_evaluator import (
-    _get_classifier_global_metrics,
-    _infer_model_type_by_labels,
-    _extract_raw_model,
-    _extract_predict_fn,
-    _get_regressor_metrics,
-    _get_binary_sum_up_label_pred_prob,
-    _get_classifier_per_class_metrics,
-    _gen_classifier_curve,
-    _evaluate_custom_metric,
-    _compute_df_mode_or_mean,
-    _CustomMetric,
-)
-from sklearn.linear_model import LogisticRegression, LinearRegression
-from sklearn.datasets import load_iris
-
-from tempfile import TemporaryDirectory
-from os.path import join as path_join
-from PIL import Image, ImageChops
-import io
 
 # noqa: F811
 from .utils import (  # noqa: F811
     get_run_data,
+    get_connll_dataset,
+    get_diabetes_dataset_timeseries,
     linear_regressor_model_uri,
     diabetes_dataset,
     multiclass_logistic_regressor_model_uri,
     iris_dataset,
     get_iris,
     newsgroup_dataset,
     newsgroup_dataset_text_pair,
@@ -57,18 +31,31 @@
     translation_llm_model_uri,
     billsum_dataset,
     squad_qna_dataset,
     opus_dataset,
     multilabel_llm_model_uri,
     y_transformer_arxiv,
     ner_dataset,
-    get_connll_dataset,
-    ner_llm_model_uri
+    ner_llm_model_uri,
+    fill_mask_llm_model_uri,
+    text_generation_llm_model_uri,
+    text_gen_data,
+    wiki_mask,
+    image_mc_classification_model_uri,
+    image_net,
+    image_ml_classification_model_uri,
+    fridge_multi_label,
+    forecaster_model_uri,
+    diabetes_dataset_timeseries
 )
-from mlflow.models.utils import plot_lines
+from azureml.metrics.azureml_forecasting_metrics import AzureMLForecastingMetrics
+from azureml.evaluate.mlflow.constants import ForecastFlavors
+from azureml.evaluate.mlflow.models.evaluation.base import EvaluationDataset
+from pandas.tseries.frequencies import to_offset
+
 
 
 def assert_dict_equal(d1, d2, rtol):
     for k in d1:
         assert k in d2
         assert np.isclose(d1[k], d2[k], rtol=rtol)
 
@@ -134,14 +121,118 @@
         if np.isscalar(expected_metrics[metric_key]):
             assert np.isclose(
                 expected_metrics[metric_key],
                 result.metrics[metric_key],
                 rtol=1e-3,
             )
 
+def _do_test_forecaster_evaluation(forecaster_model_uri,
+                                   forecast_flavour
+                                   ): # noqa: F811
+    """Test the forecasting evaluation using model uri."""
+    X_train, y_train, X_test, y_test = get_diabetes_dataset_timeseries()
+    cfg = {
+        ForecastFlavors.FLAVOUR: forecast_flavour,
+        'X_train': X_train,
+        'y_train': y_train,
+    }
+    if forecast_flavour == ForecastFlavors.ROLLING_FORECAST:
+        cfg['step'] = 2
+        freq = to_offset('D')
+        X_test2 = X_test.copy()
+        X_test2['date'] = pd.date_range(
+            X_test['date'].max() + freq, periods=X_test.shape[0], freq=freq)
+        X_test = pd.concat([X_test, X_test2], sort=False, ignore_index=True)
+        np.random.seed(42)
+        y_test = np.concatenate([y_test, y_test + np.random.rand(len(y_test))])
+    X_test['y'] = y_test
+
+    constructor_args = {"data": X_test, "targets": "y", "name": "diabetes_dataset_timeseries"}
+    diabetes_dataset_timeseries = EvaluationDataset(**constructor_args)
+    diabetes_dataset_timeseries._constructor_args = constructor_args
+    #diabetes_dataset_timeseries = 
+    with mlflow.start_run() as run:
+        result = evaluate(
+            forecaster_model_uri,
+            diabetes_dataset_timeseries._constructor_args["data"],
+            model_type="forecaster",
+            targets=diabetes_dataset_timeseries._constructor_args["targets"],
+            dataset_name=diabetes_dataset_timeseries.name,
+            evaluators="azureml",
+            evaluator_config={"azureml": cfg},
+        )
+
+    get_run_data(run.info.run_id)
+
+    model = mlflow.aml.load_model(forecaster_model_uri, model_type="regressor")
+
+    y = diabetes_dataset_timeseries.labels_data
+    if forecast_flavour == ForecastFlavors.ROLLING_FORECAST:
+        X_test = model.rolling_forecast(diabetes_dataset_timeseries.features_data, y, step=2)
+        y_pred = X_test.pop(model._model_impl.forecast_column_name).values
+        y = X_test.pop(model._model_impl.actual_column_name).values
+    else:
+        y_pred, _ = model.forecast(diabetes_dataset_timeseries.features_data)
+        
+    metric = AzureMLForecastingMetrics(
+        X_train=X_train,
+        y_train=y_train,
+        y_std=np.std(y_train),
+        time_column_name='date'
+        )
+    expected_metrics = metric.compute(
+        y_test=y, y_pred=y_pred,
+        X_test=X_test)
+    for metric_key in expected_metrics:
+        if np.isscalar(expected_metrics[metric_key]):
+            assert np.isclose(
+                expected_metrics[metric_key],
+                result.metrics[metric_key],
+                rtol=1e-3,
+            )
+
+def test_forecaster_evaluation(forecaster_model_uri): # noqa: F811):
+    """Test evaluation on regular forecast."""
+    _do_test_forecaster_evaluation(
+        forecaster_model_uri, ForecastFlavors.RECURSIVE_FORECAST)
+
+def test_forecaster_evaluation_rolling(forecaster_model_uri): # noqa: F811):
+    """Test evaluation on regular forecast."""
+    _do_test_forecaster_evaluation(
+        forecaster_model_uri, ForecastFlavors.ROLLING_FORECAST)
+
+def test_forecaster_evaluation_mlflow_model(forecaster_model_uri,
+                                            diabetes_dataset_timeseries):
+    """Test the forecasting evaluation using model instance."""
+    forecaster_model_mlflow = mlflow.aml.load_model(forecaster_model_uri)
+    with mlflow.start_run() as run:
+        result = evaluate(
+            forecaster_model_mlflow,
+            diabetes_dataset_timeseries._constructor_args["data"],
+            model_type="forecaster",
+            targets=diabetes_dataset_timeseries._constructor_args["targets"],
+            dataset_name=diabetes_dataset_timeseries.name,
+            evaluators="azureml",
+        )
+
+    get_run_data(run.info.run_id)
+
+    y = diabetes_dataset_timeseries.labels_data
+    y_pred, _ = forecaster_model_mlflow.forecast(diabetes_dataset_timeseries.features_data)
+    expected_metrics = compute_metrics(task_type=constants.Tasks.FORECASTING, y_test=y, y_pred=y_pred,
+                                       X_test=diabetes_dataset_timeseries._constructor_args["data"],
+                                       time_column_name='date')
+    for metric_key in expected_metrics:
+        if np.isscalar(expected_metrics[metric_key]):
+            assert np.isclose(
+                expected_metrics[metric_key],
+                result.metrics[metric_key],
+                rtol=1e-3,
+            )
+
 
 def test_multi_classifier_evaluation(multiclass_logistic_regressor_model_uri,
                                      iris_dataset):  # noqa: F811
     metrics_args = {
         "class_labels": np.unique(iris_dataset.labels_data),
         "train_labels": np.unique(iris_dataset.labels_data)
     }
@@ -431,14 +522,142 @@
         if np.isscalar(expected_metrics[metric_key]):
             assert np.isclose(
                 expected_metrics[metric_key],
                 result.metrics[metric_key],
                 rtol=1e-3,
             )
 
+def test_text_generation_llm_evaluation(text_generation_llm_model_uri, text_gen_data):  # noqa: F811
+    metrics_args = {
+
+    }
+    with mlflow.start_run() as run:
+        result = evaluate(
+            text_generation_llm_model_uri,
+            text_gen_data._constructor_args["data"],
+            model_type="text-generation",
+            targets=text_gen_data._constructor_args["targets"],
+            dataset_name=text_gen_data.name,
+            evaluators="azureml",
+            evaluator_config=metrics_args
+        )
+
+    _, metrics, tags, artifacts = get_run_data(run.info.run_id)
+
+    model = mlflow.aml.load_model(text_generation_llm_model_uri, model_type="text-generation")
+
+    y = text_gen_data.labels_data
+    y_test = np.reshape(y, (-1, 1))
+    y_pred = model.predict(text_gen_data.features_data)
+    y_pred = y_pred[y_pred.columns[0]].to_numpy().tolist()
+    expected_metrics = compute_metrics(task_type=constants.Tasks.TEXT_GENERATION, y_test=y_test.tolist(), y_pred=y_pred,
+                                       **metrics_args)
+    for metric_key in expected_metrics:
+        if np.isscalar(expected_metrics[metric_key]):
+            assert np.isclose(
+                expected_metrics[metric_key],
+                result.metrics[metric_key],
+                rtol=1e-3,
+            )
+
+def test_fill_mask_llm_evaluation(fill_mask_llm_model_uri, wiki_mask):  # noqa: F811
+    metrics_args = {
+
+    }
+    with mlflow.start_run() as run:
+        result = evaluate(
+            fill_mask_llm_model_uri,
+            wiki_mask._constructor_args["data"],
+            model_type="fill-mask",
+            targets=wiki_mask._constructor_args["targets"],
+            dataset_name=wiki_mask.name,
+            evaluators="azureml",
+            evaluator_config=metrics_args
+        )
+
+    _, metrics, tags, artifacts = get_run_data(run.info.run_id)
+
+    model = mlflow.aml.load_model(fill_mask_llm_model_uri, model_type="fill-mask")
+
+    y = wiki_mask.labels_data
+    y_test = np.reshape(y, (-1, 1))
+    y_pred = model.predict(wiki_mask.features_data)
+    y_pred = y_pred[y_pred.columns[0]].to_numpy().tolist()
+    expected_metrics = compute_metrics(task_type=constants.Tasks.FILL_MASK, y_test=y_test.tolist(), y_pred=y_pred,
+                                       **metrics_args)
+    for metric_key in expected_metrics:
+        if np.isscalar(expected_metrics[metric_key]):
+            assert np.isclose(
+                expected_metrics[metric_key],
+                result.metrics[metric_key],
+                rtol=1e-3,
+            )
+
+
+def test_image_classifier_evaluation(image_mc_classification_model_uri, image_net):  # noqa: F811
+    with mlflow.start_run() as run:
+        result = evaluate(
+            image_mc_classification_model_uri,
+            image_net._constructor_args["data"],
+            model_type="image-classifier",
+            targets=image_net._constructor_args["targets"],
+            dataset_name=image_net.name,
+            evaluators="azureml",
+        )
+
+    model = mlflow.aml.load_model(image_mc_classification_model_uri, "image-classifier")
+
+    y = image_net.labels_data
+    y_pred = model.predict(image_net.features_data)
+
+    expected_metrics = compute_metrics(task_type=constants.Tasks.CLASSIFICATION, y_test=y, y_pred=y_pred["labels"])
+    for metric_key, expected_metric_value in expected_metrics["metrics"].items():
+        if np.isscalar(expected_metric_value) and np.isnan(expected_metric_value) == False:
+            assert np.isclose(
+                expected_metric_value,
+                result.metrics[metric_key],
+                rtol=1e-3,
+            )
+
+
+def test_image_classifier_multilabel_evaluation(image_ml_classification_model_uri, fridge_multi_label):  # noqa: F811
+    with mlflow.start_run() as run:
+        metrics_args = {
+            "multilabel": True
+        }
+        result = evaluate(
+            image_ml_classification_model_uri,
+            fridge_multi_label._constructor_args["data"],
+            model_type="image-classifier-multilabel",
+            targets=fridge_multi_label._constructor_args["targets"],
+            dataset_name=fridge_multi_label.name,
+            evaluators="azureml",
+            evaluator_config = metrics_args
+        )
+
+    model = mlflow.aml.load_model(image_ml_classification_model_uri, "image-classifier-multilabel")
+
+    y = fridge_multi_label.labels_data
+    y_pred = model.predict(fridge_multi_label.features_data)["labels"]
+
+    y_pred = [str(x) for x in y_pred]
+    y_pred = np.array(list(map(lambda x: ast.literal_eval(x), y_pred)))
+    y = np.array(list(map(lambda x: ast.literal_eval(x), y)))
+
+    expected_metrics = compute_metrics(
+        task_type=constants.Tasks.CLASSIFICATION, y_test=y, y_pred=y_pred, **{"multilabel": True}
+    )
+    for metric_key, expected_metric_value in expected_metrics["metrics"].items():
+        if np.isscalar(expected_metric_value) and np.isnan(expected_metric_value) == False:
+            assert np.isclose(
+                expected_metric_value,
+                result.metrics[metric_key],
+                rtol=1e-3,
+            )
+
 
 def test_load_model():
     pass
 
 
 def test_log_predictions():
     pass
```

## tests/models/utils.py

```diff
@@ -1,28 +1,40 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 # pylint: disable-all
 # flake8: noqa
 import ast
+import base64
+import sys
 
 from datasets import load_dataset
 
+from azureml.automl.runtime import _ml_engine
+from azureml.automl.runtime.featurizer.transformer.timeseries.timeseries_transformer import TimeSeriesPipelineType,\
+    TimeSeriesTransformer
+from azureml.automl.runtime.shared.model_wrappers import ForecastingPipelineWrapper
+from azureml.automl.core.featurization.featurizationconfig import FeaturizationConfig
+from azureml.automl.core.shared.constants import TimeSeries, TimeSeriesInternal
 import azureml.evaluate.mlflow as azureml_mlflow
 from azureml.evaluate.mlflow.models.evaluation.base import EvaluationDataset
 from collections import namedtuple
 import pytest
 import numpy as np
-import sklearn
+import sklearn.datasets
+import sklearn.linear_model
 import pandas as pd
+from os.path import join, dirname
 from sklearn.preprocessing import MultiLabelBinarizer
-from transformers import AutoModelForSequenceClassification, AdamW, BertConfig, AutoModelForTokenClassification, \
-    AutoModelForSeq2SeqLM, AutoModelForQuestionAnswering
+from transformers import AutoModelForSequenceClassification, AutoModelForTokenClassification, \
+    AutoModelForSeq2SeqLM, AutoModelForQuestionAnswering, AutoModelForCausalLM, AutoModelForMaskedLM, \
+    AutoModelForImageClassification, AutoImageProcessor
 from transformers import AutoTokenizer
 from transformers import AutoConfig
+from sklearn.pipeline import Pipeline
 
 """Named entity recognition dataset wrapper class."""
 
 import logging
 from typing import List, Optional
 import torch
 from torch import nn
@@ -141,14 +153,28 @@
     return iris.data, iris.target
 
 
 def get_diabetes_dataset():
     data = sklearn.datasets.load_diabetes()
     return data.data, data.target
 
+def get_diabetes_dataset_timeseries():
+    # We will hack the diabetes data set so that it
+    # will mimic time series.
+    bn = sklearn.datasets.load_diabetes(return_X_y=False)
+    X = bn.data
+    df_diabetes = pd.DataFrame(X, columns=bn.feature_names)
+    df_diabetes['y'] = bn.target
+    df_diabetes['date'] = pd.date_range('2001-01-01', periods=df_diabetes.shape[0])
+    X_train = df_diabetes.iloc[:-7]
+    y_train = X_train.pop('y').values
+    X_test = df_diabetes.iloc[-7:]
+    y_test = X_test.pop('y').values
+    return X_train, y_train, X_test, y_test
+    
 
 def get_breast_cancer_dataset():
     data = sklearn.datasets.load_breast_cancer()
     return data.data, data.target
 
 
 def get_20newsgroup_dataset():
@@ -860,7 +886,304 @@
         }
     }
     with azureml_mlflow.start_run() as run:
         azureml_mlflow.hftransformers.log_model(model, "llm_qna_mdl", tokenizer=tokenizer, config=config,
                                                 hf_conf=misc_conf)
         qna_llm_model_uri = azureml_mlflow.tracking.artifact_utils.get_artifact_uri(run.info.run_id, "llm_qna_mdl")
     return qna_llm_model_uri
+
+@pytest.fixture
+def wiki_mask():
+    fm_data = load_dataset("rcds/wikipedia-for-mask-filling", "original_512", ignore_verifications=True)
+    fm_data = fm_data["train"].train_test_split(test_size=0.2)
+    fm_dataset = fm_data["test"]
+    data = fm_dataset.to_pandas().sample(n=5)
+    data["texts"] = data["texts"].apply(lambda x: x.replace("<mask>", "[MASK]"))
+    data = data[["texts", "masks"]]
+    data["masks"] = data["masks"].apply(lambda x: tuple(x))
+    constructor_args = {"data": data, "targets": 'masks', "name": "wikipedia_for_fill_mask"}
+    ds = EvaluationDataset(**constructor_args)
+    ds._constructor_args = constructor_args
+    return ds
+
+@pytest.fixture
+def text_gen_data():
+    text_gen_data = load_dataset("lmqg/qg_squad", ignore_verifications=True)
+    text_gen_data = text_gen_data["train"].train_test_split(test_size=0.2)
+    text_gen_dataset = text_gen_data["test"]
+    data = text_gen_dataset.to_pandas().sample(n=5)
+    data = data[["paragraph", "sentence"]]
+    constructor_args = {"data": data, "targets": 'paragraph', "name": "qg_squad"}
+    ds = EvaluationDataset(**constructor_args)
+    ds._constructor_args = constructor_args
+    return ds
+
+@pytest.fixture
+def text_generation_llm_model_uri():
+    model_name = 'distilgpt2'
+    config = AutoConfig.from_pretrained(model_name)
+    model = AutoModelForCausalLM.from_pretrained(model_name, config=config)
+    tokenizer = AutoTokenizer.from_pretrained(model_name)
+    misc_conf = {
+        "task_type": "text-generation"
+    }
+    with azureml_mlflow.start_run() as run:
+        azureml_mlflow.hftransformers.log_model(model, "llm_text_generation_mdl", tokenizer=tokenizer, config=config,
+                                                hf_conf=misc_conf)
+        text_generation_llm_model_uri = azureml_mlflow.tracking.artifact_utils.get_artifact_uri(run.info.run_id,
+                                                                                            "llm_text_generation_mdl")
+    return text_generation_llm_model_uri
+
+@pytest.fixture
+def fill_mask_llm_model_uri():
+    model_name = 'distilbert-base-uncased'
+    config = AutoConfig.from_pretrained(model_name)
+    model = AutoModelForMaskedLM.from_pretrained(model_name, config=config)
+    tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True)
+    misc_conf = {
+        "task_type": "fill-mask"
+    }
+    with azureml_mlflow.start_run() as run:
+        azureml_mlflow.hftransformers.log_model(model, "llm_fill_mask_mdl", tokenizer=tokenizer, config=config,
+                                                hf_conf=misc_conf)
+        fill_mask_llm_model_uri = azureml_mlflow.tracking.artifact_utils.get_artifact_uri(run.info.run_id,
+                                                                                            "llm_fill_mask_mdl")
+    return fill_mask_llm_model_uri
+
+
+@pytest.fixture
+def image_mc_classification_model_uri():
+    model_name = "google/vit-base-patch16-224"
+    config = AutoConfig.from_pretrained(model_name)
+    model = AutoModelForImageClassification.from_pretrained(model_name, config=config)
+    tokenizer = AutoImageProcessor.from_pretrained(model_name)
+    misc_conf = {
+        "task_type": "image-classification",
+        "train_label_list": list(model.config.id2label.values()),
+        "hf_predict_module": "hf_test_predict",
+    }
+
+    mlflow_dir = join(
+        dirname(dirname(dirname(dirname(__file__)))),
+        "azureml-acft-image-components",
+        "azureml",
+        "acft",
+        "image",
+        "components",
+        "finetune",
+        "common",
+        "mlflow",
+    )
+    sys.path.append(mlflow_dir)
+
+    files_to_include = ["common_constants.py", "common_utils.py", "hf_test_predict.py"]
+    code_paths = [join(mlflow_dir, x) for x in files_to_include]
+
+    with azureml_mlflow.start_run() as run:
+        azureml_mlflow.hftransformers.log_model(
+            model,
+            "image-classification",
+            tokenizer=tokenizer,
+            config=model.config,
+            hf_conf=misc_conf,
+            code_paths=code_paths,
+        )
+        classification_model_uri = azureml_mlflow.tracking.artifact_utils.get_artifact_uri(
+            run.info.run_id, "image-classification"
+        )
+    return classification_model_uri
+
+
+def read_image(image_path: str):
+    """Read image from path"""
+    with open(image_path, "rb") as f:
+        return base64.encodebytes(f.read()).decode("utf-8")
+
+
+@pytest.fixture
+def image_net():
+    dataset_path = join(dirname(__file__), "data", "image-classification")
+    data = load_dataset(dataset_path, split="train")
+
+    label_list = data.features["label"].names
+
+    data = data.to_pandas()
+    data = data[["image", "label"]]
+
+    data["image"] = data["image"].apply(lambda x: read_image(x["path"]))
+    data["label"] = data["label"].apply(lambda x: label_list[x])
+
+    constructor_args = {"data": data, "targets": "label", "name": "imagenet-1k-tiny"}
+    ds = EvaluationDataset(**constructor_args)
+    ds._constructor_args = constructor_args
+    return ds
+
+
+def read_image(image_path: str):
+    """Read image from path"""
+    with open(image_path, "rb") as f:
+        return base64.encodebytes(f.read()).decode("utf-8")
+
+
+@pytest.fixture
+def image_net():
+    dataset_path = join(dirname(__file__), "data", "image-classification")
+    data = load_dataset(dataset_path, split="train")
+
+    label_list = data.features["label"].names
+
+    data = data.to_pandas()
+    data = data[["image", "label"]]
+
+    data["image"] = data["image"].apply(lambda x: read_image(x["path"]))
+    data["label"] = data["label"].apply(lambda x: label_list[x])
+
+    constructor_args = {"data": data, "targets": "label", "name": "imagenet-1k-tiny"}
+    ds = EvaluationDataset(**constructor_args)
+    ds._constructor_args = constructor_args
+    return ds
+
+
+@pytest.fixture
+def image_ml_classification_model_uri():
+    config = AutoConfig.from_pretrained(
+        "https://automlcesdkdataresources.blob.core.windows.net/finetuning-image-models/HuggingFace/ML_Beit_Fridge/"
+        "config.json"
+    )
+    model = AutoModelForImageClassification.from_pretrained(
+        "https://automlcesdkdataresources.blob.core.windows.net/finetuning-image-models/HuggingFace/ML_Beit_Fridge/"
+        "pytorch_model.bin",
+        config=config,
+    )
+    tokenizer = AutoImageProcessor.from_pretrained(
+        "https://automlcesdkdataresources.blob.core.windows.net/finetuning-image-models/HuggingFace/ML_Beit_Fridge/"
+        "preprocessor_config.json"
+    )
+
+    misc_conf = {
+        "task_type": "image-classification-multilabel",
+        "train_label_list": list(model.config.id2label.values()),
+        "hf_predict_module": "hf_test_predict",
+    }
+
+    mlflow_dir = join(
+        dirname(dirname(dirname(dirname(__file__)))),
+        "azureml-acft-image-components",
+        "azureml",
+        "acft",
+        "image",
+        "components",
+        "finetune",
+        "common",
+        "mlflow",
+    )
+    sys.path.append(mlflow_dir)
+
+    files_to_include = ["common_constants.py", "common_utils.py", "hf_test_predict.py"]
+    code_paths = [join(mlflow_dir, x) for x in files_to_include]
+
+    with azureml_mlflow.start_run() as run:
+        azureml_mlflow.hftransformers.log_model(
+            model,
+            "image-classification-multilabel",
+            tokenizer=tokenizer,
+            config=model.config,
+            hf_conf=misc_conf,
+            code_paths=code_paths,
+        )
+        classification_model_uri = azureml_mlflow.tracking.artifact_utils.get_artifact_uri(
+            run.info.run_id, "image-classification-multilabel"
+        )
+    return classification_model_uri
+
+
+@pytest.fixture
+def fridge_multi_label():
+    dataset_path = join(dirname(__file__), "data", "image-classification-multilabel")
+
+    image_class_mapping = {
+        "path": [
+            join(dataset_path, "1.jpg"),
+            join(dataset_path, "10.jpg"),
+            join(dataset_path, "12.jpg"),
+            join(dataset_path, "100.jpg"),
+            join(dataset_path, "101.jpg"),
+            join(dataset_path, "111.jpg"),
+            join(dataset_path, "122.jpg"),
+        ],
+        "label": [
+            '["carton"]',
+            '["carton", "milk_bottle"]',
+            '["carton", "milk_bottle"]',
+            '["can"]',
+            '["water_bottle"]',
+            '["water_bottle", "carton"]',
+            '["water_bottle", "milk_bottle"]',
+        ],
+    }
+
+    data = pd.DataFrame.from_dict(image_class_mapping)
+
+    data["image"] = data["path"].apply(lambda x: read_image(x))
+
+    constructor_args = {"data": data, "targets": "label", "name": "fridge-tiny"}
+    ds = EvaluationDataset(**constructor_args)
+    ds._constructor_args = constructor_args
+    return ds
+@pytest.fixture(scope="module")
+def diabetes_dataset_timeseries():
+    _, _, X_test, y_test = get_diabetes_dataset_timeseries()
+    X_test["y"] = y_test
+    constructor_args = {"data": X_test, "targets": "y", "name": "diabetes_dataset_timeseries"}
+    ds = EvaluationDataset(**constructor_args)
+    ds._constructor_args = constructor_args
+    return ds
+
+@pytest.fixture
+def forecaster_model_uri():
+    X_train, y_train, _, _ = get_diabetes_dataset_timeseries()
+    ts_config = {
+        TimeSeries.TIME_COLUMN_NAME: 'date',
+        TimeSeries.GRAIN_COLUMN_NAMES: None,
+        TimeSeries.MAX_HORIZON: 7,
+        TimeSeriesInternal.DROP_NA: True,
+    }
+    featurization_config = FeaturizationConfig()
+    (
+        forecasting_pipeline,
+        ts_config,
+        lookback_removed,
+        time_index_non_holiday_features
+    ) = _ml_engine.suggest_featurizers_timeseries(
+        X_train,
+        y_train,
+        featurization_config,
+        ts_config,
+        TimeSeriesPipelineType.FULL,
+        y_transformer=None
+    )
+
+    ts_transformer = TimeSeriesTransformer(
+        forecasting_pipeline,
+        TimeSeriesPipelineType.FULL,
+        featurization_config,
+        time_index_non_holiday_features,
+        lookback_removed,
+        **ts_config
+    )
+    X_train = ts_transformer.fit_transform(X_train, y_train)
+    y_train = X_train.pop(TimeSeriesInternal.DUMMY_TARGET_COLUMN).values
+    estimator = sklearn.linear_model.LinearRegression()
+
+    estimator.fit(X_train, y_train)
+    model = Pipeline([('ts_transformer', ts_transformer),
+                      ('estimator', estimator)])
+
+    stdev = list(np.arange(1, ts_config[TimeSeries.MAX_HORIZON] + 1))
+    fw = ForecastingPipelineWrapper(
+        pipeline=model, stddev=stdev)
+
+    with azureml_mlflow.start_run() as run:
+        azureml_mlflow.sklearn.log_model(fw, "reg_model")
+        linear_regressor_model_uri = azureml_mlflow.tracking.artifact_utils.get_artifact_uri(run.info.run_id,
+                                                                                             "reg_model")
+    return linear_regressor_model_uri
```

## Comparing `azureml_evaluate_mlflow-0.0.6.post1.dist-info/LICENSE.txt` & `azureml_evaluate_mlflow-0.0.7.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `azureml_evaluate_mlflow-0.0.6.post1.dist-info/METADATA` & `azureml_evaluate_mlflow-0.0.7.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: azureml-evaluate-mlflow
-Version: 0.0.6.post1
+Version: 0.0.7
 Summary: Contains the integration code of AzureML Evaluate with Mlflow.
 Home-page: UNKNOWN
 Author: Microsoft Corp
 License: Proprietary https://aka.ms/azureml-preview-sdk-license 
 Platform: UNKNOWN
 Classifier: Development Status :: 4 - Beta
 Classifier: Intended Audience :: Developers
```

## Comparing `azureml_evaluate_mlflow-0.0.6.post1.dist-info/RECORD` & `azureml_evaluate_mlflow-0.0.7.dist-info/RECORD`

 * *Files 5% similar despite different names*

```diff
@@ -1,54 +1,61 @@
 azureml/__init__.py,sha256=n0xtZ3iWcoVg5Qognsb7InYAUVAK8s3iaVeHB5GOaNA,251
 azureml/evaluate/__init__.py,sha256=JlCCObuOLZyNhIZlsoL51KtFxiY4xKIIzIRDBcdeu6Y,183
 azureml/evaluate/mlflow/NOTICE,sha256=9yhw1yr8BF0uDAkvycPPtWKgGRZQ1qa-NjWjh3M9Ptw,477828
 azureml/evaluate/mlflow/__init__.py,sha256=XiZqhd8Bmmiq_2UNYiLBkzEKBKB2xEFF4Uq8nAsL7dg,7071
-azureml/evaluate/mlflow/_version.py,sha256=GQ36wSy_Pp7e6BY8gxS-d9VPHRYf_juthRhz0MaNBjw,25
+azureml/evaluate/mlflow/_version.py,sha256=8iO5GQcMv0IT7Iil5TDihy-fhGMWbAFFV8FZLU2C8A4,19
+azureml/evaluate/mlflow/constants.py,sha256=I4FeqGzLnBhlpkMc5IjqtZWGrphEwmNOQ5XHYuNpQ_s,695
 azureml/evaluate/mlflow/exceptions.py,sha256=IRI5it-nGfea4Mz2szTZe2JOPHbZQ_tJHoX4yFYz9-s,373
 azureml/evaluate/mlflow/_loader_modules/__init__.py,sha256=JlCCObuOLZyNhIZlsoL51KtFxiY4xKIIzIRDBcdeu6Y,183
 azureml/evaluate/mlflow/_loader_modules/hftransformers.py,sha256=ifpkefsthCK_T7rdOQcmVK4Hbw9zdvYHKevCCUZnpA8,2769
 azureml/evaluate/mlflow/_loader_modules/sklearn.py,sha256=3ucm1zLUfJYJ83SftdC28tGw4tHuLUGtxofFI8PAJJ8,457
-azureml/evaluate/mlflow/aml/__init__.py,sha256=jOjBcS5gLHhIJPaY6XkLJrS7MhiP_0zAzGlcpoWm_Dc,32732
+azureml/evaluate/mlflow/aml/__init__.py,sha256=xSVmSnGxtNRAuna8m8YS8zQj2hka73Emx3Nj29RTzxw,33109
 azureml/evaluate/mlflow/aml/constants.py,sha256=A6sNVbQ4w26UnwtOGKmHPrglaaCXsiJuYSxogeT0Lwc,930
 azureml/evaluate/mlflow/aml/model.py,sha256=rkH_gplcQiFyj5Za0e9IB7i9-ghggcUejOd3Ensp4Dg,14964
 azureml/evaluate/mlflow/aml/utils.py,sha256=T1xdnR0z1Kyi8R8q8pQDE2DQ_RX9mxr8ZJ41hV9bXLc,12432
 azureml/evaluate/mlflow/hftransformers/__init__.py,sha256=lptqB6nKjjpbEqqIgzG-srkZJLElsI1hQbK2d2Z9sjY,38122
 azureml/evaluate/mlflow/hftransformers/_task_based_pipeline_predictors.py,sha256=sIBsDjCKNgQaulAgvGTGxbQylGPwVu684hSOiBAvZs8,3895
 azureml/evaluate/mlflow/hftransformers/_task_based_predictors.py,sha256=YhXYE5jqIkuP9DdbsWe__09uZn3TGtpCzlwbAxT3p58,32935
 azureml/evaluate/mlflow/hftransformers/constants.py,sha256=mmUQrpNvJ45MA6ySgEVEUsLOPGgzgqF-kACi3Abj7Yo,1050
 azureml/evaluate/mlflow/hftransformers/dataset_wrappers.py,sha256=bf5RGChxfTGVgamSAZxntTiHVvCLAqWM-49WRABJ8dQ,4875
 azureml/evaluate/mlflow/hftransformers/utils.py,sha256=hm5YudTVzi5LESzY2Cor9AC32D8PzgJ3XAItzKnztQo,2675
 azureml/evaluate/mlflow/models/__init__.py,sha256=M8_B_jWlt2-Rb_8T8do5suYOA7d87fsRizwgqDGl0ag,1832
 azureml/evaluate/mlflow/models/evaluation/__init__.py,sha256=-OltsA-899LKiGGG7giMGOyjf_pH5kXoXNyiJT_n9mQ,540
 azureml/evaluate/mlflow/models/evaluation/base.py,sha256=2HAWAa5VKryaznaMAvstg45LWqDGlPv9OlGPM9nVupM,42463
+azureml/evaluate/mlflow/models/evaluation/constants.py,sha256=xvRcxjq0fXd4In7nh08TegQfn9r_WUVQk2g-9AfPKiY,375
 azureml/evaluate/mlflow/models/evaluation/default_evaluator.py,sha256=lBTj94HfDnOKhJt1ARlnGOZLqDjAug0kWo5RtGVsCnM,42935
 azureml/evaluate/mlflow/models/evaluation/evaluator_registry.py,sha256=gz328ecJ-VQzv7QtdbWe0DyenP0Rhv12x-GEevAB2Ko,2473
 azureml/evaluate/mlflow/models/evaluation/azureml/__init__.py,sha256=JlCCObuOLZyNhIZlsoL51KtFxiY4xKIIzIRDBcdeu6Y,183
 azureml/evaluate/mlflow/models/evaluation/azureml/_classifier_evaluator.py,sha256=rXWR5c7dlCeQ9cI3OfivIq5ucIEeHn5Z-nWzR9JgwbA,1286
 azureml/evaluate/mlflow/models/evaluation/azureml/_classifier_multilabel_evaluator.py,sha256=w0uskodiifkxd1ETmysx6ZoQvJWu6qiIH9k7AGLeGNs,1830
+azureml/evaluate/mlflow/models/evaluation/azureml/_fill_mask_evaluator.py,sha256=kqJ2obNUFxSvXCWq60himDClCztwZmPUeVaaUEGwuBk,1261
+azureml/evaluate/mlflow/models/evaluation/azureml/_forecaster_evaluator.py,sha256=2j_NiAU0wfe8HUpvjD8-oYDP-ECEbpzXTxtqU9TGdsU,2786
+azureml/evaluate/mlflow/models/evaluation/azureml/_image_classifier_evaluator.py,sha256=9jgXQAW653We56Vj2DXFLX-39CNC5mhhwCw7qTkvTRI,2113
 azureml/evaluate/mlflow/models/evaluation/azureml/_ner_evaluator.py,sha256=AC9ZIw9hVg1p2EiorIokNN4Yd1yj2fk1wy2X3hxrwcA,1204
 azureml/evaluate/mlflow/models/evaluation/azureml/_qna_evaluator.py,sha256=D0ydLC4mWjEybzOegvlOoPr8OFKpKs-h8PRciojLWV0,1163
 azureml/evaluate/mlflow/models/evaluation/azureml/_regressor_evaluator.py,sha256=SXDGyyY12YPuNvTrchM7h6KjIkOdO2lSV9hgni5wATY,1058
 azureml/evaluate/mlflow/models/evaluation/azureml/_summarization_evaluator.py,sha256=hTWA8Zca-OR5dNTuQ-Y8zE_iQr1-nTFluKlORFp7y0k,1270
 azureml/evaluate/mlflow/models/evaluation/azureml/_task_evaluator.py,sha256=xzpidupudXfOVCd_nqEZzx6nYWDyNj31MRTx4VGfgH4,802
-azureml/evaluate/mlflow/models/evaluation/azureml/_task_evaluator_factory.py,sha256=qvJBn-KddkQKBCXMDB1civhWKXu5F18CFeQqMYWeQT4,1751
+azureml/evaluate/mlflow/models/evaluation/azureml/_task_evaluator_factory.py,sha256=pKT8msOyAkb0A5AcZ7mZNFGwr98AKpULYimlqK7NSc4,2561
+azureml/evaluate/mlflow/models/evaluation/azureml/_text_generation_evaluator.py,sha256=iXX8xVjrPWQJiSi5l7y_waz9OiaktUITxGiMinpRt9M,1273
 azureml/evaluate/mlflow/models/evaluation/azureml/_translation_evaluator.py,sha256=YY7MqnscGwI2iMgaWm3FCgj9tFtwNFeuV_kDvYrseP0,1266
-azureml/evaluate/mlflow/models/evaluation/azureml/azureml_evaluator.py,sha256=WIXmp75XD0wKEFRfRj1jXzNahJeBAIpg4lNknF6X7sg,11116
+azureml/evaluate/mlflow/models/evaluation/azureml/azureml_evaluator.py,sha256=a6cn60NfR95_lp_XdcOitiEJKAAAnuh47RnuDo0NuNg,12303
 tests/aml/__init__.py,sha256=JlCCObuOLZyNhIZlsoL51KtFxiY4xKIIzIRDBcdeu6Y,183
 tests/aml/test_model_export_with_class_and_artifacts.py,sha256=cwkX8I96Lpe3uj7pZRGt1_vTWalVSjig2C51v_kRkXo,51203
 tests/aml/test_model_export_with_loader_module_and_data_path.py,sha256=vAwplwJ1D-ssopmYOEucWuBETM8b4hCi1ZXLqrtL9fA,30719
 tests/hftransformers/__init__.py,sha256=JlCCObuOLZyNhIZlsoL51KtFxiY4xKIIzIRDBcdeu6Y,183
 tests/hftransformers/hf_test_predict.py,sha256=XtvT129GmnU4bD5Ykd-Dh--x4PtP4j07aS0-ZBlxn6o,317
 tests/hftransformers/preprocess.py,sha256=a8IqSESoy8Gcpt2pQLHZKjwWLUZfbAp1bSCzEQ0Prvc,154
 tests/hftransformers/test_hf_load_pipeline.py,sha256=CV3sEQEAU2j10D8-jbdTXEiHl1wMgWrl8D9Xtb7VDlU,4019
 tests/hftransformers/test_hf_model_export.py,sha256=oqlcnS88e2dlqeOdYfeKfd4nSEcme2iySwcQrHxT-Og,32623
 tests/models/__init__.py,sha256=JlCCObuOLZyNhIZlsoL51KtFxiY4xKIIzIRDBcdeu6Y,183
 tests/models/dummy_evaluator.py,sha256=KpWJfNo9cTpDzhBeszCKSU032j2BU0l7OrkSYoT2H6Q,4537
-tests/models/test_azureml_evaluator.py,sha256=0QScwZ_rWr5j9WcuwPgNXtVj1e34UP2_oIlegxz1L74,17911
+tests/models/test_azureml_evaluator.py,sha256=lhJvkz1lkwtyt0LBy-UolV3fRs4NnrNYJ8oQgyQVYvw,27305
 tests/models/test_default_evaluator.py,sha256=J63xqa_1C2Tood7LB0d4VV7H1eJ2tKMBSY8_yzf8Gv8,43050
 tests/models/test_evaluation.py,sha256=qI4ieoSslO24d46tXFQyBlmk2WNQs3VdOcoPsQWwdRw,37145
-tests/models/utils.py,sha256=griLA8HWApVxdfSZwf1QEAM7jK878dwB70Jn6E4sE6o,38423
-azureml_evaluate_mlflow-0.0.6.post1.dist-info/LICENSE.txt,sha256=FOfkEEz4uS7g278F9Rq12rqKF3lLyBUZhVpLetuZrTg,1021
-azureml_evaluate_mlflow-0.0.6.post1.dist-info/METADATA,sha256=xoatRaxOHFKq80oPHIzk56UGQdCMgo6Ld47g3Q9tE2E,1692
-azureml_evaluate_mlflow-0.0.6.post1.dist-info/WHEEL,sha256=YUYzQ6UQdoqxXjimOitTqynltBCkwY6qlTfTh2IzqQU,97
-azureml_evaluate_mlflow-0.0.6.post1.dist-info/top_level.txt,sha256=YGbVVonfOvHBwpsDBTpLWbK7KEdGg0m7LlGFY7j0SCw,14
-azureml_evaluate_mlflow-0.0.6.post1.dist-info/RECORD,,
+tests/models/test_forecaster_evaluator.py,sha256=bInHaNFuHoHgnLH-_6s2uCOt3Q_bTonbYGXz8hBXDRM,13542
+tests/models/utils.py,sha256=P4775IKBgXY7qGtHYyuDdQsqooAWEIJ09Fm7B9GBMms,50782
+azureml_evaluate_mlflow-0.0.7.dist-info/LICENSE.txt,sha256=FOfkEEz4uS7g278F9Rq12rqKF3lLyBUZhVpLetuZrTg,1021
+azureml_evaluate_mlflow-0.0.7.dist-info/METADATA,sha256=YjmrtLbnuHGL4lX1lRJb_dRfiAYDUm_0HPG4Wjzc5ZE,1686
+azureml_evaluate_mlflow-0.0.7.dist-info/WHEEL,sha256=YUYzQ6UQdoqxXjimOitTqynltBCkwY6qlTfTh2IzqQU,97
+azureml_evaluate_mlflow-0.0.7.dist-info/top_level.txt,sha256=YGbVVonfOvHBwpsDBTpLWbK7KEdGg0m7LlGFY7j0SCw,14
+azureml_evaluate_mlflow-0.0.7.dist-info/RECORD,,
```

