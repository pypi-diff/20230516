# Comparing `tmp/azureml_rag-0.1.2-py3-none-any.whl.zip` & `tmp/azureml_rag-0.1.3-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,58 +1,59 @@
-Zip file size: 163816 bytes, number of entries: 56
--rw-rw-rw-  2.0 fat      246 b- defN 23-May-05 16:42 azureml/rag/__init__.py
--rw-rw-rw-  2.0 fat    39484 b- defN 23-May-05 16:42 azureml/rag/documents.py
--rw-rw-rw-  2.0 fat    27816 b- defN 23-May-05 16:42 azureml/rag/embeddings.py
--rw-rw-rw-  2.0 fat     7469 b- defN 23-May-05 16:42 azureml/rag/mlindex.py
--rw-rw-rw-  2.0 fat     3992 b- defN 23-May-05 16:42 azureml/rag/models.py
--rw-rw-rw-  2.0 fat     4646 b- defN 23-May-05 16:43 azureml/rag/_asset_client/client.py
--rw-rw-rw-  2.0 fat      893 b- defN 23-May-05 16:44 azureml/rag/_asset_client/_restclient/__init__.py
--rw-rw-rw-  2.0 fat     4381 b- defN 23-May-05 16:44 azureml/rag/_asset_client/_restclient/_azure_machine_learning_workspaces.py
--rw-rw-rw-  2.0 fat     3538 b- defN 23-May-05 16:44 azureml/rag/_asset_client/_restclient/_configuration.py
--rw-rw-rw-  2.0 fat     1561 b- defN 23-May-05 16:44 azureml/rag/_asset_client/_restclient/_patch.py
--rw-rw-rw-  2.0 fat      495 b- defN 23-May-05 16:44 azureml/rag/_asset_client/_restclient/_version.py
--rw-rw-rw-  2.0 fat      399 b- defN 23-May-05 16:44 azureml/rag/_asset_client/_restclient/models.py
--rw-rw-rw-  2.0 fat      957 b- defN 23-May-05 16:45 azureml/rag/_asset_client/_restclient/dataset/__init__.py
--rw-rw-rw-  2.0 fat     3802 b- defN 23-May-05 16:45 azureml/rag/_asset_client/_restclient/dataset/_azure_machine_learning_workspaces.py
--rw-rw-rw-  2.0 fat     3144 b- defN 23-May-05 16:45 azureml/rag/_asset_client/_restclient/dataset/_configuration.py
--rw-rw-rw-  2.0 fat      694 b- defN 23-May-05 16:45 azureml/rag/_asset_client/_restclient/dataset/_patch.py
--rw-rw-rw-  2.0 fat    81019 b- defN 23-May-05 16:45 azureml/rag/_asset_client/_restclient/dataset/_serialization.py
--rw-rw-rw-  2.0 fat     1833 b- defN 23-May-05 16:45 azureml/rag/_asset_client/_restclient/dataset/_vendor.py
--rw-rw-rw-  2.0 fat      495 b- defN 23-May-05 16:45 azureml/rag/_asset_client/_restclient/dataset/_version.py
--rw-rw-rw-  2.0 fat     6787 b- defN 23-May-05 16:45 azureml/rag/_asset_client/_restclient/dataset/models/__init__.py
--rw-rw-rw-  2.0 fat     4635 b- defN 23-May-05 16:45 azureml/rag/_asset_client/_restclient/dataset/models/_azure_machine_learning_workspaces_enums.py
--rw-rw-rw-  2.0 fat   131448 b- defN 23-May-05 16:45 azureml/rag/_asset_client/_restclient/dataset/models/_models_py3.py
--rw-rw-rw-  2.0 fat      694 b- defN 23-May-05 16:45 azureml/rag/_asset_client/_restclient/dataset/models/_patch.py
--rw-rw-rw-  2.0 fat      585 b- defN 23-May-05 16:45 azureml/rag/_asset_client/_restclient/dataset/operations/__init__.py
--rw-rw-rw-  2.0 fat   104636 b- defN 23-May-05 16:45 azureml/rag/_asset_client/_restclient/dataset/operations/_data_version_operations.py
--rw-rw-rw-  2.0 fat    80785 b- defN 23-May-05 16:45 azureml/rag/_asset_client/_restclient/dataset/operations/_mlindex_operations.py
--rw-rw-rw-  2.0 fat      893 b- defN 23-May-05 16:45 azureml/rag/_asset_client/_restclient/runhistory/__init__.py
--rw-rw-rw-  2.0 fat     3902 b- defN 23-May-05 16:45 azureml/rag/_asset_client/_restclient/runhistory/_azure_machine_learning_workspaces.py
--rw-rw-rw-  2.0 fat     3226 b- defN 23-May-05 16:45 azureml/rag/_asset_client/_restclient/runhistory/_configuration.py
--rw-rw-rw-  2.0 fat     1561 b- defN 23-May-05 16:45 azureml/rag/_asset_client/_restclient/runhistory/_patch.py
--rw-rw-rw-  2.0 fat     1255 b- defN 23-May-05 16:45 azureml/rag/_asset_client/_restclient/runhistory/_vendor.py
--rw-rw-rw-  2.0 fat      495 b- defN 23-May-05 16:45 azureml/rag/_asset_client/_restclient/runhistory/_version.py
--rw-rw-rw-  2.0 fat    11705 b- defN 23-May-05 16:45 azureml/rag/_asset_client/_restclient/runhistory/models/__init__.py
--rw-rw-rw-  2.0 fat     2566 b- defN 23-May-05 16:45 azureml/rag/_asset_client/_restclient/runhistory/models/_azure_machine_learning_workspaces_enums.py
--rw-rw-rw-  2.0 fat   175170 b- defN 23-May-05 16:45 azureml/rag/_asset_client/_restclient/runhistory/models/_models.py
--rw-rw-rw-  2.0 fat   188776 b- defN 23-May-05 16:45 azureml/rag/_asset_client/_restclient/runhistory/models/_models_py3.py
--rw-rw-rw-  2.0 fat      563 b- defN 23-May-05 16:45 azureml/rag/_asset_client/_restclient/runhistory/operations/__init__.py
--rw-rw-rw-  2.0 fat   162796 b- defN 23-May-05 16:45 azureml/rag/_asset_client/_restclient/runhistory/operations/_runs_operations.py
--rw-rw-rw-  2.0 fat     9201 b- defN 23-May-05 16:43 azureml/rag/langchain/acs.py
--rw-rw-rw-  2.0 fat      183 b- defN 23-May-05 16:43 azureml/rag/tasks/__init__.py
--rw-rw-rw-  2.0 fat     5480 b- defN 23-May-05 16:43 azureml/rag/tasks/build_faiss.py
--rw-rw-rw-  2.0 fat     7027 b- defN 23-May-05 16:43 azureml/rag/tasks/crack_and_chunk.py
--rw-rw-rw-  2.0 fat     7330 b- defN 23-May-05 16:43 azureml/rag/tasks/embed.py
--rw-rw-rw-  2.0 fat     4426 b- defN 23-May-05 16:43 azureml/rag/tasks/embed_prs.py
--rw-rw-rw-  2.0 fat     1172 b- defN 23-May-05 16:43 azureml/rag/tasks/git_clone.py
--rw-rw-rw-  2.0 fat     2157 b- defN 23-May-05 16:43 azureml/rag/tasks/register_mlindex.py
--rw-rw-rw-  2.0 fat    14572 b- defN 23-May-05 16:43 azureml/rag/tasks/update_acs.py
--rw-rw-rw-  2.0 fat      208 b- defN 23-May-05 16:43 azureml/rag/utils/__init__.py
--rw-rw-rw-  2.0 fat     1588 b- defN 23-May-05 16:43 azureml/rag/utils/azureml.py
--rw-rw-rw-  2.0 fat     2139 b- defN 23-May-05 16:43 azureml/rag/utils/git.py
--rw-rw-rw-  2.0 fat     2330 b- defN 23-May-05 16:43 azureml/rag/utils/logging.py
--rw-rw-rw-  2.0 fat     1021 b- defN 23-May-05 16:50 azureml_rag-0.1.2.dist-info/LICENSE.txt
--rw-rw-rw-  2.0 fat     3217 b- defN 23-May-05 16:50 azureml_rag-0.1.2.dist-info/METADATA
--rw-rw-rw-  2.0 fat       97 b- defN 23-May-05 16:50 azureml_rag-0.1.2.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        8 b- defN 23-May-05 16:50 azureml_rag-0.1.2.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     5973 b- defN 23-May-05 16:50 azureml_rag-0.1.2.dist-info/RECORD
-56 files, 1137471 bytes uncompressed, 153862 bytes compressed:  86.5%
+Zip file size: 166101 bytes, number of entries: 57
+-rw-rw-rw-  2.0 fat      246 b- defN 23-May-16 21:28 azureml/rag/__init__.py
+-rw-rw-rw-  2.0 fat    39995 b- defN 23-May-16 21:28 azureml/rag/documents.py
+-rw-rw-rw-  2.0 fat    27598 b- defN 23-May-16 21:28 azureml/rag/embeddings.py
+-rw-rw-rw-  2.0 fat     5058 b- defN 23-May-16 21:28 azureml/rag/mlindex.py
+-rw-rw-rw-  2.0 fat     4149 b- defN 23-May-16 21:28 azureml/rag/models.py
+-rw-rw-rw-  2.0 fat     4646 b- defN 23-May-16 21:29 azureml/rag/_asset_client/client.py
+-rw-rw-rw-  2.0 fat      893 b- defN 23-May-16 21:30 azureml/rag/_asset_client/_restclient/__init__.py
+-rw-rw-rw-  2.0 fat     4381 b- defN 23-May-16 21:30 azureml/rag/_asset_client/_restclient/_azure_machine_learning_workspaces.py
+-rw-rw-rw-  2.0 fat     3538 b- defN 23-May-16 21:30 azureml/rag/_asset_client/_restclient/_configuration.py
+-rw-rw-rw-  2.0 fat     1561 b- defN 23-May-16 21:30 azureml/rag/_asset_client/_restclient/_patch.py
+-rw-rw-rw-  2.0 fat      495 b- defN 23-May-16 21:30 azureml/rag/_asset_client/_restclient/_version.py
+-rw-rw-rw-  2.0 fat      399 b- defN 23-May-16 21:30 azureml/rag/_asset_client/_restclient/models.py
+-rw-rw-rw-  2.0 fat      957 b- defN 23-May-16 21:31 azureml/rag/_asset_client/_restclient/dataset/__init__.py
+-rw-rw-rw-  2.0 fat     3802 b- defN 23-May-16 21:31 azureml/rag/_asset_client/_restclient/dataset/_azure_machine_learning_workspaces.py
+-rw-rw-rw-  2.0 fat     3144 b- defN 23-May-16 21:31 azureml/rag/_asset_client/_restclient/dataset/_configuration.py
+-rw-rw-rw-  2.0 fat      694 b- defN 23-May-16 21:31 azureml/rag/_asset_client/_restclient/dataset/_patch.py
+-rw-rw-rw-  2.0 fat    81019 b- defN 23-May-16 21:31 azureml/rag/_asset_client/_restclient/dataset/_serialization.py
+-rw-rw-rw-  2.0 fat     1833 b- defN 23-May-16 21:31 azureml/rag/_asset_client/_restclient/dataset/_vendor.py
+-rw-rw-rw-  2.0 fat      495 b- defN 23-May-16 21:31 azureml/rag/_asset_client/_restclient/dataset/_version.py
+-rw-rw-rw-  2.0 fat     6787 b- defN 23-May-16 21:31 azureml/rag/_asset_client/_restclient/dataset/models/__init__.py
+-rw-rw-rw-  2.0 fat     4635 b- defN 23-May-16 21:31 azureml/rag/_asset_client/_restclient/dataset/models/_azure_machine_learning_workspaces_enums.py
+-rw-rw-rw-  2.0 fat   131448 b- defN 23-May-16 21:31 azureml/rag/_asset_client/_restclient/dataset/models/_models_py3.py
+-rw-rw-rw-  2.0 fat      694 b- defN 23-May-16 21:31 azureml/rag/_asset_client/_restclient/dataset/models/_patch.py
+-rw-rw-rw-  2.0 fat      585 b- defN 23-May-16 21:31 azureml/rag/_asset_client/_restclient/dataset/operations/__init__.py
+-rw-rw-rw-  2.0 fat   104636 b- defN 23-May-16 21:31 azureml/rag/_asset_client/_restclient/dataset/operations/_data_version_operations.py
+-rw-rw-rw-  2.0 fat    80785 b- defN 23-May-16 21:31 azureml/rag/_asset_client/_restclient/dataset/operations/_mlindex_operations.py
+-rw-rw-rw-  2.0 fat      893 b- defN 23-May-16 21:31 azureml/rag/_asset_client/_restclient/runhistory/__init__.py
+-rw-rw-rw-  2.0 fat     3902 b- defN 23-May-16 21:31 azureml/rag/_asset_client/_restclient/runhistory/_azure_machine_learning_workspaces.py
+-rw-rw-rw-  2.0 fat     3226 b- defN 23-May-16 21:31 azureml/rag/_asset_client/_restclient/runhistory/_configuration.py
+-rw-rw-rw-  2.0 fat     1561 b- defN 23-May-16 21:31 azureml/rag/_asset_client/_restclient/runhistory/_patch.py
+-rw-rw-rw-  2.0 fat     1255 b- defN 23-May-16 21:31 azureml/rag/_asset_client/_restclient/runhistory/_vendor.py
+-rw-rw-rw-  2.0 fat      495 b- defN 23-May-16 21:31 azureml/rag/_asset_client/_restclient/runhistory/_version.py
+-rw-rw-rw-  2.0 fat    11705 b- defN 23-May-16 21:31 azureml/rag/_asset_client/_restclient/runhistory/models/__init__.py
+-rw-rw-rw-  2.0 fat     2566 b- defN 23-May-16 21:31 azureml/rag/_asset_client/_restclient/runhistory/models/_azure_machine_learning_workspaces_enums.py
+-rw-rw-rw-  2.0 fat   175170 b- defN 23-May-16 21:31 azureml/rag/_asset_client/_restclient/runhistory/models/_models.py
+-rw-rw-rw-  2.0 fat   188776 b- defN 23-May-16 21:31 azureml/rag/_asset_client/_restclient/runhistory/models/_models_py3.py
+-rw-rw-rw-  2.0 fat      563 b- defN 23-May-16 21:31 azureml/rag/_asset_client/_restclient/runhistory/operations/__init__.py
+-rw-rw-rw-  2.0 fat   162796 b- defN 23-May-16 21:31 azureml/rag/_asset_client/_restclient/runhistory/operations/_runs_operations.py
+-rw-rw-rw-  2.0 fat    10785 b- defN 23-May-16 21:29 azureml/rag/langchain/acs.py
+-rw-rw-rw-  2.0 fat      183 b- defN 23-May-16 21:29 azureml/rag/tasks/__init__.py
+-rw-rw-rw-  2.0 fat     2094 b- defN 23-May-16 21:29 azureml/rag/tasks/build_faiss.py
+-rw-rw-rw-  2.0 fat     7191 b- defN 23-May-16 21:29 azureml/rag/tasks/crack_and_chunk.py
+-rw-rw-rw-  2.0 fat     7330 b- defN 23-May-16 21:29 azureml/rag/tasks/embed.py
+-rw-rw-rw-  2.0 fat     5410 b- defN 23-May-16 21:29 azureml/rag/tasks/embed_prs.py
+-rw-rw-rw-  2.0 fat     2194 b- defN 23-May-16 21:29 azureml/rag/tasks/git_clone.py
+-rw-rw-rw-  2.0 fat     2791 b- defN 23-May-16 21:29 azureml/rag/tasks/register_mlindex.py
+-rw-rw-rw-  2.0 fat    17073 b- defN 23-May-16 21:29 azureml/rag/tasks/update_acs.py
+-rw-rw-rw-  2.0 fat      208 b- defN 23-May-16 21:29 azureml/rag/utils/__init__.py
+-rw-rw-rw-  2.0 fat     1588 b- defN 23-May-16 21:29 azureml/rag/utils/azureml.py
+-rw-rw-rw-  2.0 fat     7435 b- defN 23-May-16 21:29 azureml/rag/utils/connections.py
+-rw-rw-rw-  2.0 fat     2350 b- defN 23-May-16 21:29 azureml/rag/utils/git.py
+-rw-rw-rw-  2.0 fat     2330 b- defN 23-May-16 21:29 azureml/rag/utils/logging.py
+-rw-rw-rw-  2.0 fat     1021 b- defN 23-May-16 21:36 azureml_rag-0.1.3.dist-info/LICENSE.txt
+-rw-rw-rw-  2.0 fat     3482 b- defN 23-May-16 21:36 azureml_rag-0.1.3.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       97 b- defN 23-May-16 21:36 azureml_rag-0.1.3.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        8 b- defN 23-May-16 21:36 azureml_rag-0.1.3.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     6063 b- defN 23-May-16 21:36 azureml_rag-0.1.3.dist-info/RECORD
+57 files, 1147014 bytes uncompressed, 156007 bytes compressed:  86.4%
```

## zipnote {}

```diff
@@ -141,29 +141,32 @@
 
 Filename: azureml/rag/utils/__init__.py
 Comment: 
 
 Filename: azureml/rag/utils/azureml.py
 Comment: 
 
+Filename: azureml/rag/utils/connections.py
+Comment: 
+
 Filename: azureml/rag/utils/git.py
 Comment: 
 
 Filename: azureml/rag/utils/logging.py
 Comment: 
 
-Filename: azureml_rag-0.1.2.dist-info/LICENSE.txt
+Filename: azureml_rag-0.1.3.dist-info/LICENSE.txt
 Comment: 
 
-Filename: azureml_rag-0.1.2.dist-info/METADATA
+Filename: azureml_rag-0.1.3.dist-info/METADATA
 Comment: 
 
-Filename: azureml_rag-0.1.2.dist-info/WHEEL
+Filename: azureml_rag-0.1.3.dist-info/WHEEL
 Comment: 
 
-Filename: azureml_rag-0.1.2.dist-info/top_level.txt
+Filename: azureml_rag-0.1.3.dist-info/top_level.txt
 Comment: 
 
-Filename: azureml_rag-0.1.2.dist-info/RECORD
+Filename: azureml_rag-0.1.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## azureml/rag/documents.py

```diff
@@ -279,14 +279,18 @@
 
 def get_langchain_splitter(file_extension: str, arguments: dict) -> TextSplitter:
     """Get a text splitter for a given file extension."""
     use_nltk = False
     if "use_nltk" in arguments:
         use_nltk = arguments['use_nltk'] is True
         del arguments["use_nltk"]
+    use_rcts = False
+    if "use_rcts" in arguments:
+        use_nltk = arguments['use_rcts'] is True
+        del arguments['use_rcts']
 
     # Handle non-natural language splitters
     if file_extension == ".py":
         from langchain.text_splitter import PythonCodeTextSplitter
         return PythonCodeTextSplitter.from_tiktoken_encoder(**arguments)
 
     # If configured to use NLTK for splitting on sentence boundaries use that for non-code text formats
@@ -307,15 +311,21 @@
     if file_extension == ".txt" or file_extension in formats_to_treat_as_txt_once_loaded:
         from langchain.text_splitter import TokenTextSplitter
         return TokenTextSplitter(**arguments)
     elif file_extension == ".html" or file_extension == ".htm":
         from langchain.text_splitter import TokenTextSplitter
         return TokenTextSplitter(**arguments)
     elif file_extension == ".md":
-        return MarkdownHeaderSplitter.from_tiktoken_encoder(remove_hyperlinks=True, remove_images=True, **arguments)
+        if use_rcts:
+            logger.info('Using MarkdownTextSplitter')
+            from langchain.text_splitter import MarkdownTextSplitter
+            return MarkdownTextSplitter.from_tiktoken_encoder(**arguments)
+        else:
+            logger.info('Using MarkdownHeaderSplitter')
+            return MarkdownHeaderSplitter.from_tiktoken_encoder(remove_hyperlinks=True, remove_images=True, **arguments)
     else:
         raise ValueError(f"Invalid file_extension: {file_extension}")
 
 
 file_extension_splitters = {
     ".txt": lambda **kwargs: get_langchain_splitter(".txt", kwargs),
     ".md": lambda **kwargs: get_langchain_splitter(".md", kwargs),
@@ -686,16 +696,17 @@
         document_metadata = document.get_metadata()
         chunk_prefix = document_metadata.get('chunk_prefix', '')
         if len(chunk_prefix) > 0:
             if 'chunk_size' in local_splitter_args:
                 prefix_token_length = _tiktoken_len(chunk_prefix)
                 if prefix_token_length > local_splitter_args['chunk_size'] // 2:
                     chunk_prefix = chunk_prefix[:local_splitter_args['chunk_size'] // 2]
+                    # should we update local_splitter_args['chunk_size'] here?
                 else:
-                    local_splitter_args['chunk_size'] = local_splitter_args['chunk_size'] - _tiktoken_len(chunk_prefix)
+                    local_splitter_args['chunk_size'] = local_splitter_args['chunk_size'] - prefix_token_length
 
         if 'chunk_prefix' in document_metadata:
             del document_metadata['chunk_prefix']
 
         # TODO: Move out as own filter
         chunk_overlap = 0
         if 'chunk_overlap' in local_splitter_args:
```

## azureml/rag/embeddings.py

```diff
@@ -23,16 +23,17 @@
 import pyarrow as pa
 import pyarrow.parquet as pq
 import time
 import yaml
 
 from azureml.rag.documents import LazyDocument
 from azureml.rag.models import parse_model_uri
+from azureml.rag.utils.azureml import get_workspace_from_environment
+from azureml.rag.utils.connections import get_connection_credential, get_connection_by_id_v2, workspace_connection_to_credential
 from azureml.rag.utils.logging import get_logger
-from azureml.rag.utils.azureml import get_secrets_from_workspace, get_workspace_from_environment
 
 
 logger = get_logger(__name__)
 
 
 def _parse_open_ai_args(arguments: dict):
     from langchain.embeddings.openai import OpenAIEmbeddings
@@ -49,40 +50,37 @@
 
     if "endpoint" in arguments:
         openai.api_base = arguments["endpoint"]
 
     if "api_base" in arguments:
         openai.api_base = arguments["api_base"]
 
-    if arguments.get("connection_type", "workspace_keyvault") == "workspace_keyvault":
-        workspace = get_workspace_from_environment()
-        if workspace is None and "connection" in arguments:
-            from azureml.core import Workspace
-            workspace = Workspace(
-                subscription_id=arguments["connection"]["subscription"],
-                resource_group=arguments["connection"]["resource_group"],
-                workspace_name=arguments["connection"]["workspace"]
-            )
-
-        secrets = get_secrets_from_workspace(["BAKER-OPENAI-API-BASE", "BAKER-OPENAI-API-KEY", "OPENAI-API-KEY", "OPENAI-API-BASE"], workspace=workspace)
-
-        # hacky way to override OPENAI-API-KEY if Baker key existed
-        if secrets["BAKER-OPENAI-API-BASE"] is not None:
-            secrets["OPENAI-API-BASE"] = secrets["BAKER-OPENAI-API-BASE"]
-        if secrets["BAKER-OPENAI-API-KEY"] is not None:
-            secrets["OPENAI-API-KEY"] = secrets["BAKER-OPENAI-API-KEY"]
-    elif "connection_type" not in arguments:
-        secrets = {
-            "OPENAI-API-KEY": os.environ["OPENAI_API_KEY"],
-            "OPENAI-API-BASE": os.environ.get("OPENAI_API_BASE"),
-        }
+    if "connection_type" not in arguments:
+        openai.api_key = os.environ["OPENAI_API_KEY"]
     else:
-        raise ValueError("Only workspace_keyvault connection_type is supported for OpenAI.")
-
-    openai.api_key = secrets["OPENAI-API-KEY"]
+        if arguments["connection_type"] == "workspace_connection":
+            connection_id = arguments.get('connection', {}).get('id', '')
+            connection = get_connection_by_id_v2(connection_id)
+            openai.api_base = connection.get('properties', {}).get('target')
+            openai.api_version = connection.get('properties', {}).get('metadata', {}).get('apiVersion', '2023-03-15-preview')
+            credential = workspace_connection_to_credential(connection)
+        else:
+            credential = get_connection_credential(arguments)
+        if hasattr(credential, 'key'):
+            openai.api_key = credential.key
+        else:
+            # Add hack to check for "BAKER-OPENAI-API-KEY"
+            if arguments.get("connection_type", "workspace_keyvault") == "workspace_keyvault":
+                new_args = copy.deepcopy(arguments)
+                new_args["connection"]["key"] = "BAKER-OPENAI-API-KEY"
+                credential = get_connection_credential(new_args)
+                if hasattr(credential, 'key'):
+                    openai.api_key = credential.key
+                else:
+                    openai.api_key = credential.get_token('https://cognitiveservices.azure.com/.default').token
 
     embedder = OpenAIEmbeddings(
         openai_api_key=openai.api_key,
         max_retries=100,
     )
 
     if "model_name" in arguments:
@@ -310,44 +308,33 @@
         self._document_embeddings = OrderedDict()
         self.dimension = kwargs.get('dimension', None)
 
     @staticmethod
     def from_uri(uri: str, **kwargs) -> 'Embeddings':
         """Create an embeddings object from a URI."""
         config = parse_model_uri(uri, **kwargs)
-        return Embeddings(**config)
+        return Embeddings(**{**config, **kwargs})
 
     def get_metadata(self):
         """Get the metadata of the embeddings."""
         if self.kind == "custom":
             arguments = copy.deepcopy(self.arguments)
             arguments["pickled_embedding_fn"] = gzip.compress(
                 cloudpickle.dumps(arguments["embedding_fn"]))
             del arguments["embedding_fn"]
         else:
             arguments = self.arguments
 
-        ws = get_workspace_from_environment()
-
         metadata = {
             "schema_version": "2",
             "kind": self.kind,
             "dimension": self.get_embedding_dimensions(),
             **self.arguments
         }
 
-        if "open_ai" in self.kind:
-            metadata["connection_type"] = "workspace_keyvault"
-            metadata["connection"] = {
-                "subscription": ws.subscription_id if ws is not None else "",
-                "resource_group": ws.resource_group if ws is not None else "",
-                "workspace": ws.name if ws is not None else "",
-                "key": "OPENAI-API-KEY"
-            }
-
         return metadata
 
     @staticmethod
     def from_metadata(metadata: dict) -> 'Embeddings':
         """Create an embeddings object from metadata."""
         schema_version = metadata.get('schema_version', "1")
         if schema_version == "1":
@@ -599,16 +586,16 @@
         for ((doc_id, mtime, document_data, document_hash, document_metadata), embeddings) in zip(documents_to_embed, embeddings):
             documents_embedded[doc_id] = \
                 DataEmbeddedDocument(doc_id, mtime, document_hash,
                                      document_data, embeddings, document_metadata)
 
         return documents_embedded
 
-    def write_as_faiss_mlindex(self, output_path: Path):
-        """Writes the embeddings to a FAISS MLIndex file."""
+    def as_faiss_index(self):
+        """Returns a FAISS index that can be used to query the embeddings."""""
         from langchain.docstore.in_memory import InMemoryDocstore
         from langchain.vectorstores import FAISS
         from langchain.vectorstores.faiss import dependable_faiss_import
         import numpy as np
 
         logger.info("Building index", extra={'print': True})
         t1 = time.time()
@@ -638,24 +625,28 @@
             {index_to_id[i]: doc for i, doc in enumerate(documents)}
         )
 
         faiss = dependable_faiss_import()
         index = faiss.IndexFlatL2(len(embeddings[0]))
         index.add(np.array(embeddings, dtype=np.float32))
 
-        faiss_index = FAISS(self.get_query_embed_fn(), index, docstore, index_to_id)
-
         logger.info(f"Built index from {num_source_docs} documents and {len(embeddings)} chunks, took {time.time()-t1:.4f} seconds", extra={'print': True})
 
+        return FAISS(self.get_query_embed_fn(), index, docstore, index_to_id)
+
+    def write_as_faiss_mlindex(self, output_path: Path):
+        """Writes the embeddings to a FAISS MLIndex file."""
+        faiss_index = self.as_faiss_index()
+
         logger.info("Saving index", extra={'print': True})
         faiss_index.save_local(str(output_path))
 
         mlindex_config = {
             "embeddings": self.get_metadata()
         }
         mlindex_config["index"] = {
             "kind": "faiss",
-            "method": "FlatL2",
             "engine": "langchain.vectorstores.FAISS",
+            "method": "FlatL2"
         }
         with open(output_path / "MLIndex", "w") as f:
             yaml.dump(mlindex_config, f)
```

## azureml/rag/mlindex.py

```diff
@@ -1,12 +1,13 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 """MLIndex class for interacting with MLIndex assets."""
 from azureml.rag.embeddings import Embeddings
+from azureml.rag.utils.connections import get_connection_credential
 from azureml.rag.utils.logging import get_logger
 import tempfile
 from typing import Union
 
 
 logger = get_logger('mlindex')
 
@@ -60,68 +61,21 @@
 
     def get_langchain_embeddings(self):
         """Get the LangChainEmbeddings from the MLIndex"""
         embeddings = Embeddings.from_metadata(self.embeddings_config)
 
         return embeddings.as_langchain_embeddings()
 
-    # TODO: Keep this all off in a corner of azureml-rag
-    @staticmethod
-    def _get_connection_credential(config):
-        if config.get('connection_type', None) == 'workspace_keyvault':
-            try:
-                from azure.core.credentials import AzureKeyCredential  # noqa:F401
-            except ImportError:
-                raise ValueError(
-                    "Could not import azure-core python package. "
-                    "Please install it with `pip install azure-core`."
-                )
-
-            from azureml.core import Run, Workspace
-            run = Run.get_context()
-            if hasattr(run, 'experiment'):
-                ws = run.experiment.workspace
-            else:
-                try:
-                    ws = Workspace(
-                        subscription_id=config.get('connection', {}).get('subscription'),
-                        resource_group=config.get('connection', {}).get('resource_group'),
-                        workspace_name=config.get('connection', {}).get('workspace')
-                    )
-                except Exception as e:
-                    logger.warning(f"Could not get workspace '{config.get('connection', {}).get('workspace')}': {e}")
-                    # Fall back to looking for key in environment.
-                    import os
-                    key = os.environ.get(config.get('connection', {}).get('key'))
-                    if key is None:
-                        raise ValueError(f"Could not get workspace '{config.get('connection', {}).get('workspace')}' and no key named '{config.get('connection', {}).get('key')}' in environment")
-                    return AzureKeyCredential(key)
-
-            keyvault = ws.get_default_keyvault()
-            credential = AzureKeyCredential(keyvault.get_secret(config.get('connection', {}).get('key')))
-        elif config.get('connection_type', None) == 'workspace_connection':
-            raise NotImplementedError("workspace_connection not implemented yet")
-        else:
-            try:
-                from azure.identity import DefaultAzureCredential  # noqa:F401
-            except ImportError as e:
-                raise ValueError(
-                    "Could not import azure-identity python package. "
-                    "Please install it with `pip install azure-identity`."
-                ) from e
-            credential = DefaultAzureCredential()
-        return credential
-
     def as_langchain_vectorstore(self):
         """Converts MLIndex to a retriever object that can be used with langchain, may download files."""
         index_kind = self.index_config.get('kind', None)
         if index_kind == 'acs':
             from azureml.rag.langchain.acs import AzureCognitiveSearchVectorStore
 
-            credential = MLIndex._get_connection_credential(self.index_config)
+            credential = get_connection_credential(self.index_config)
 
             return AzureCognitiveSearchVectorStore(
                 index_name=self.index_config.get('index'),
                 endpoint=self.index_config.get('endpoint'),
                 embeddings=self.get_langchain_embeddings(),
                 field_mapping=self.index_config.get('field_mapping', {}),
                 credential=credential,
@@ -146,15 +100,15 @@
     def as_langchain_retriever(self, **kwargs):
         """Converts MLIndex to a retriever object that can be used with langchain, may download files."""
         index_kind = self.index_config.get('kind', None)
         if index_kind == 'acs':
             return self.as_langchain_vectorstore().as_retriever(**kwargs)
             # from azureml.rag.langchain.acs import AzureCognitiveSearchRetriever
 
-            # credential = MLIndex._get_connection_credential(self.index_config)
+            # credential = get_connection_credential(self.index_config)
 
             # return AzureCognitiveSearchRetriever(
             #     index_name=self.index_config.get('index'),
             #     endpoint=self.index_config.get('endpoint'),
             #     credential=credential,
             #     top_k=self.index_config.get('top_k', 4),
             # )
```

## azureml/rag/models.py

```diff
@@ -28,15 +28,18 @@
             dets[details[i]] = details[i + 1]
         return dets
 
     config = {**kwargs}
     if scheme == 'azure_open_ai':
         config = {**split_details(details), **config}
         config['kind'] = 'open_ai'
-        config['api_base'] = f"https://{config['endpoint']}.openai.azure.com"
+        if 'endpoint' in config:
+            config['api_base'] = f"https://{config['endpoint']}.openai.azure.com"
+        elif 'endpoint' in kwargs:
+            config['api_base'] = f"https://{kwargs['endpoint']}.openai.azure.com"
         config['api_type'] = 'azure'
         config['api_version'] = kwargs.get('api_version') if kwargs.get('api_version') is not None else '2023-03-15-preview'
         # Azure OpenAI has a batch_size limit of 1
         config['batch_size'] = '1'
     elif scheme == 'open_ai':
         config['kind'] = 'open_ai'
         config = {**split_details(details), **config}
```

## azureml/rag/langchain/acs.py

```diff
@@ -1,11 +1,12 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 """Azure Cognitive Search vector store."""
+import base64
 import json
 from langchain.embeddings.base import Embeddings
 from langchain.schema import BaseRetriever, Document
 from langchain.vectorstores.base import VectorStore
 from pydantic import BaseModel, Extra, root_validator
 import requests
 import tenacity
@@ -13,15 +14,17 @@
 
 from azureml.rag.utils.logging import get_logger
 
 
 logger = get_logger('langchain.acs')
 
 
-# Retry decorator with specific conditions
+# TODO: FieldMappings dataclass
+
+
 @tenacity.retry(
     wait=tenacity.wait_fixed(5),  # wait 5 seconds between retries
     stop=tenacity.stop_after_attempt(3),  # stop after 3 attempts
     reraise=True,  # re-raise the exception after the last retry attempt
 )
 def send_post_request(url, headers, payload):
     """Send a POST request to the specified URL with the specified headers and payload."""
@@ -81,37 +84,50 @@
     def similarity_search(self, query: str, k: int = 4, **kwargs: Any) -> List[Document]:
         """Search for similar documents by query."""
         return [item[0] for item in self._similarity_search_with_relevance_scores(query, k, **kwargs)]
 
     def _similarity_search_with_relevance_scores(self, query: str, k: int = 4, **kwargs: Any) -> List[Tuple[Document, float]]:
         """Search for similar documents by query."""
         embedded_query = self.embeddings.embed_query(query)
+        return self._similarity_search_by_vector_with_relevance_scores(query, embedded_query, k, **kwargs)
 
+    def _similarity_search_by_vector_with_relevance_scores(self, query: Optional[str], embedded_query: List[float], k: int = 4, **kwargs) -> List[Tuple[Document, float]]:
         post_url = f"{self.endpoint}/indexes/{self.index_name}/docs/search?api-version=2023-07-01-Preview"
         headers = get_acs_headers(self.credential)
-        post_payload = {
-            "search": query,
-            "top": str(k),
-            "vector": {
+        post_payload = {}
+
+        if query is not None:
+            logger.info(f"Query: {query}")
+            post_payload["search"] = query
+
+        post_payload["top"] = str(k)
+
+        if self.field_mapping.get('embedding', None) is not None:
+            logger.info(f"Using embedding field: {self.field_mapping['embedding']}")
+            post_payload["vector"] = {
                 "value": embedded_query,
                 "fields": self.field_mapping['embedding'],
                 "k": k
             }
-        }
 
         response = send_post_request(post_url, headers, post_payload)
 
         if response.content:
             response_json = response.json()
+            logger.info(response_json)
             if 'value' in response_json:
                 return [
                     (
                         Document(
                             page_content=item[self.field_mapping['content']],
-                            metadata=json.loads(item[self.field_mapping['metadata']]) if self.field_mapping['metadata'].endswith('json_string') else item[self.field_mapping['metadata']],
+                            metadata={
+                                "id": item["id"],
+                                "doc_id": base64.b64decode(item["id"]).decode('utf8'),
+                                "content_vector": item[self.field_mapping['embedding']] if self.field_mapping.get('embedding', None) is not None else None,
+                                **(json.loads(item[self.field_mapping['metadata']]) if self.field_mapping['metadata'].endswith('json_string') else item[self.field_mapping['metadata']])},
                         ),
                         item['@search.score']
                     )
                     for item in response_json['value']
                 ]
             else:
                 logger.info('no value in response from ACS')
@@ -120,28 +136,36 @@
 
         return []
 
     def add_texts(self, texts: Iterable[str], metadatas: Optional[List[dict]] = None, **kwargs: Any) -> List[str]:
         """Add texts to the vector store."""
         raise NotImplementedError
 
+    def similarity_search_by_vector_with_relevance_scores(self, vector: List[float], k: int = 4, **kwargs: Any) -> List[Tuple[Document, float]]:
+        """Search for similar documents by vector with relevance scores."""
+        if self.field_mapping.get('embedding', None) is None:
+            raise ValueError("No embedding field specified in field_mapping")
+        return self._similarity_search_by_vector_with_relevance_scores(None, vector, k, **kwargs)
+
     def similarity_search_by_vector(self, vector: List[float], k: int = 4, **kwargs: Any) -> List[Document]:
         """Search for similar documents by vector."""
-        return super().similarity_search_by_vector(vector, k, **kwargs)
+        if self.field_mapping.get('embedding', None) is None:
+            raise ValueError("No embedding field specified in field_mapping")
+        return [doc for (doc, _) in self._similarity_search_by_vector_with_relevance_scores(None, vector, k, **kwargs)]
 
     @classmethod
     def from_texts(cls, texts: Iterable[str], metadatas: Optional[List[dict]] = None, **kwargs: Any) -> VectorStore:
         """Create a vector store from a list of texts."""
         raise NotImplementedError
 
 
 # TODO: Expose semantic search options
 class AzureCognitiveSearchRetriever(BaseRetriever, BaseModel):
     """Retriever class for Azure Cognitive Search."""
-    top_k: int = 8
+    top_k: int = 16
     index_name: str
     endpoint: str
     credential: Optional[object]
 
     class Config:
         """Configuration for this pydantic object."""
```

## azureml/rag/tasks/build_faiss.py

```diff
@@ -1,103 +1,26 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
-import numpy as np
+"""File for building FAISS Indexes."""
 import os
 from pathlib import Path
-import time
-import yaml
-
-from langchain.docstore.document import Document
-from langchain.docstore.in_memory import InMemoryDocstore
-from langchain.vectorstores import FAISS
-from langchain.vectorstores.faiss import dependable_faiss_import
 
 from azureml.rag.embeddings import Embeddings
 from azureml.rag.utils.logging import get_logger, enable_stdout_logging
 
-
 logger = get_logger('build_faiss')
 
 
-def create_index_from_raw_embeddings(embeddings_container: str, embeddings_directory: str, output_path):
-    logger.info("Loading Embeddings", extra={'print': True})
-    emb = Embeddings.load(embeddings_directory, embeddings_container)
-
-    logger.info("Building index", extra={'print': True})
-    t1 = time.time()
-    num_source_docs = 0
-    documents = []
-    embeddings = []
-    for doc_id, emb_doc in emb._document_embeddings.items():
-        logger.info(f'Adding document: {doc_id}', extra={'print': True})
-        logger.debug(f'{doc_id},{emb_doc.document_hash},{emb_doc.get_embeddings()[0:20]}', extra={'print': True})
-        embeddings.append(emb_doc.get_embeddings())
-        # TODO: LazyDocument/RefDocument gets uri to page_content
-        documents.append(Document(page_content=emb_doc.get_data(),
-                                    metadata={"source_doc_id": doc_id,
-                                            "chunk_hash": emb_doc.document_hash,
-                                            "mtime": emb_doc.mtime,
-                                            **emb_doc.metadata}))
-        num_source_docs += 1
-
-    index_to_id = {i: doc.metadata["source_doc_id"] for i, doc in enumerate(documents)}
-    docstore = InMemoryDocstore(
-        {index_to_id[i]: doc for i, doc in enumerate(documents)}
-    )
-
-    faiss = dependable_faiss_import()
-    index = faiss.IndexFlatL2(len(embeddings[0]))
-    index.add(np.array(embeddings, dtype=np.float32))
-
-    faiss_index = FAISS(emb.get_query_embed_fn(), index, docstore, index_to_id)
-
-    logger.info(f"Built index from {num_source_docs} documents and {len(embeddings)} chunks, took {time.time()-t1:.4f} seconds", extra={'print': True})
-
-    logger.info("Saving index", extra={'print': True})
-    faiss_index.save_local(output_path)
-
-    mlindex_config = {
-        "embeddings": emb.get_metadata()
-    }
-    mlindex_config["index"] = {
-        "kind": "faiss",
-        "method": "FlatL2",
-        "engine": "langchain.vectorstores.FAISS",
-    }
-    with open(Path(output_path) / "MLIndex", "w") as f:
-        yaml.dump(mlindex_config, f)
-
-
-def register_run_output(output_data, asset_name):
-    from azureml.core.run import Run
-    from azureml.core import Dataset
-    from azureml.data.datapath import DataPath
-
-    current_run = Run.get_context()
-    if hasattr(current_run, 'experiment'):
-        ws = current_run.experiment.workspace
-        datastore = ws.get_default_datastore()
-        embeddings_ds = Dataset.File.upload_directory(src_dir=output_data,
-           target=DataPath(datastore,  asset_name),
-           show_progress=True)
-        embeddings_ds.register(workspace=ws,
-                               name=asset_name,
-                               description=f"Embeddings for dataset_name: {asset_name}",
-                               create_new_version=True)
-
-
 if __name__ == '__main__':
     from argparse import ArgumentParser
 
     parser = ArgumentParser()
     parser.add_argument("--embeddings", type=str)
     parser.add_argument("--output", type=str)
-    parser.add_argument("--asset_name", type=str)
-    parser.add_argument("--register_output", type=str)
     args = parser.parse_args()
 
     print('\n'.join(f'{k}={v}' for k, v in vars(args).items()))
 
     enable_stdout_logging()
 
     raw_embeddings_uri = args.embeddings
@@ -112,15 +35,13 @@
     os.environ['OPENAI_API_KEY'] = 'nope'
 
     from azureml.dataprep.fuse.dprepfuse import (MountOptions, rslex_uri_volume_mount)
     mnt_options = MountOptions(
         default_permission=0o555, allow_other=False, read_only=True)
     logger.info(f'mounting embeddings container from: \n{parent} \n   to: \n{os.getcwd()}/raw_embeddings', extra={'print': True})
     with rslex_uri_volume_mount(parent, f'{os.getcwd()}/raw_embeddings', options=mnt_options) as mount_context:
-        create_index_from_raw_embeddings(mount_context.mount_point, embeddings_dir_name, args.output)
+        logger.info("Loading Embeddings", extra={'print': True})
+        emb = Embeddings.load(embeddings_dir_name, mount_context.mount_point)
 
-    logger.info('Generated FAISS index', extra={'print': True})
+        emb.write_as_faiss_mlindex(Path(args.output))
 
-    # TODO: Replace with output data def that registers new version
-    register_output = args.register_output == "True" or args.register_output == "true"
-    if register_output:
-        register_run_output(args.output, args.asset_name)
+    logger.info('Generated FAISS index', extra={'print': True})
```

## azureml/rag/tasks/crack_and_chunk.py

```diff
@@ -45,22 +45,23 @@
     parser = argparse.ArgumentParser()
     parser.add_argument("--input_data", type=str)
     parser.add_argument("--input_glob", type=str, default="**/*")
     parser.add_argument("--allowed_extensions", required=False, type=str, default=",".join(SUPPORTED_EXTENSIONS))
     parser.add_argument("--chunk_size", type=int)
     parser.add_argument("--chunk_overlap", type=int)
     parser.add_argument("--output_title_chunk", type=str)
-    parser.add_argument("--output_summary_chunk", type=str)
-    parser.add_argument("--data_source_url", type=str)
+    parser.add_argument("--output_summary_chunk", type=str, default=None)
+    parser.add_argument("--data_source_url", type=str, required=False)
     parser.add_argument("--document_path_replacement_regex", type=str, required=False)
     parser.add_argument("--max_sample_files", type=int, default=-1)
     parser.add_argument("--include_summary", type=str, default="False")
     parser.add_argument("--summary_model_config", type=str, default='{"type": "azure_open_ai", "model_name": "gpt-35-turbo", "deployment_name": "gpt-35-turbo"}')
     parser.add_argument("--openai_api_version", type=str, default='2023-03-15-preview')
-    parser.add_argument("--openai_api_type", type=str)
+    parser.add_argument("--openai_api_type", type=str, default=None)
+    parser.add_argument("--use_rcts", type=bool, default=False)
 
     args = parser.parse_args()
     print('\n'.join(f'{k}={v}' for k, v in vars(args).items()))
 
     enable_stdout_logging()
 
     # Get all files in directory, recursively
@@ -98,15 +99,15 @@
             openai.api_key = summary_model_config["key"]
 
     chunked_documents = DocumentChunksIterator(
         files_source=args.input_data,
         glob=args.input_glob,
         base_url=args.data_source_url,
         document_path_replacement_regex=args.document_path_replacement_regex,
-        chunked_document_processors = [lambda docs: split_documents(docs, splitter_args={'chunk_size': args.chunk_size, 'chunk_overlap': args.chunk_overlap})]
+        chunked_document_processors = [lambda docs: split_documents(docs, splitter_args={'chunk_size': args.chunk_size, 'chunk_overlap': args.chunk_overlap, 'use_rcts': args.use_rcts})],
     )
     file_count = 0
     for document in chunked_documents:
         file_count += 1
         logger.info(f'Processing file: {document.source.filename}', extra={'print': True})
         # TODO: Ideally make it easy to limit number of files with a `- take: n` operation on input URI in MLTable
         if (args.max_sample_files != -1 and file_count >= args.max_sample_files):
@@ -119,15 +120,15 @@
     if include_summary:
         chunked_documents = DocumentChunksIterator(
             files_source=args.input_data,
             glob=args.input_glob,
             base_url=args.data_source_url,
             document_path_replacement_regex=args.document_path_replacement_regex,
             source_loader=lambda sources: crack_documents(sources, summary_model_config=summary_model_config),
-            chunked_document_processors = [lambda docs: split_documents(docs, splitter_args={'chunk_size': args.chunk_size, 'chunk_overlap': args.chunk_overlap})]
+            chunked_document_processors = [lambda docs: split_documents(docs, splitter_args={'chunk_size': args.chunk_size, 'chunk_overlap': args.chunk_overlap, 'use_rcts': args.use_rcts})]
         )
         total_time = 0
         for document in chunked_documents:
             file_start_time = time.time()
             if (args.max_sample_files != -1 and file_count >= args.max_sample_files):
                 logger.info(f"file count: {file_count} - reached max sample file count: {args.max_sample_files}", extra={'print': True})
                 break
```

## azureml/rag/tasks/embed_prs.py

```diff
@@ -1,75 +1,93 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
+"""ParallelRunStep entrypoint for embedding data."""
 import argparse
 import os
 import pandas as pd
 import pathlib
 import time
 
 from azureml.rag.embeddings import Embeddings
 from azureml.rag.tasks.embed import read_chunks_into_documents
+from azureml.rag.utils.azureml import get_workspace_from_environment
 from azureml.rag.utils.logging import get_logger, enable_stdout_logging
 
-
 logger = get_logger('embed')
 
 
 def init():
+    """Load previous embeddings if provided."""
     global output_data
-    global previous_embeddings
+    global embeddings_container
     parser = argparse.ArgumentParser(allow_abbrev=False, description="ParallelRunStep Agent")
     parser.add_argument("--output_data", type=str)
     parser.add_argument("--embeddings_model", type=str)
-    parser.add_argument("--previous_embeddings", required=False, type=str, default=None)
+    parser.add_argument("--embeddings_container", required=False, type=str, default=None)
     args, _ = parser.parse_known_args()
 
     print('\n'.join(f'{k}={v}' for k, v in vars(args).items()))
 
     enable_stdout_logging()
 
     output_data = args.output_data
 
-    previous_embeddings = None
-    if args.previous_embeddings is not None:
+    embeddings_container = None
+    if args.embeddings_container is not None:
         from azureml.dataprep.fuse.dprepfuse import MountOptions, rslex_uri_volume_mount
         mnt_options = MountOptions(
             default_permission=0o555, allow_other=False, read_only=True)
         try:
-            with rslex_uri_volume_mount(args.previous_embeddings, f'{os.getcwd()}/previous_embeddings', options=mnt_options) as mount_context:
-                previous_embeddings_dir_name = None
-                # list all folders in previous_embeddings_container_path and find the latest one
+            with rslex_uri_volume_mount(args.embeddings_container, f'{os.getcwd()}/embeddings_container', options=mnt_options) as mount_context:
+                embeddings_container_dir_name = None
+                # list all folders in embeddings_container and find the latest one
                 try:
-                    previous_embeddings_dir_name = str(max([dir for dir in pathlib.Path(
+                    embeddings_container_dir_name = str(max([dir for dir in pathlib.Path(
                         mount_context.mount_point).glob('*') if dir.is_dir() and dir.name != os.environ['AZUREML_RUN_ID']], key=os.path.getmtime).name)
                 except Exception as e:
                     logger.warn(
                         f'failed to get latest folder from {mount_context.mount_point} with {e}.', extra={'print': True})
                     pass
 
-                if previous_embeddings_dir_name is not None:
+                if embeddings_container_dir_name is not None:
                     logger.info(
-                        f'loading from previous embeddings from {previous_embeddings_dir_name} in {mount_context.mount_point}', extra={'print': True})
+                        f'loading from previous embeddings from {embeddings_container_dir_name} in {mount_context.mount_point}', extra={'print': True})
                     try:
-                        previous_embeddings = Embeddings.load(
-                            previous_embeddings_dir_name, mount_context.mount_point)
+                        embeddings_container = Embeddings.load(
+                            embeddings_container_dir_name, mount_context.mount_point)
                     except Exception as e:
                         logger.warn(
                             f'Failed to load from previous embeddings with {e}.\nCreating new Embeddings.', extra={'print': True})
         except Exception as e:
             logger.warn(f'Failed to load previous embeddings from mount with {e}, proceeding to create new embeddings.', extra={'print': True})
 
-    previous_embeddings = previous_embeddings if previous_embeddings is not None else Embeddings.from_uri(args.embeddings_model)
+    connection_args = {}
+    connection_id = os.environ.get('AZUREML_WORKSPACE_CONNECTION_ID_AOAI')
+    if connection_id is not None:
+        connection_args['connection_type'] = 'workspace_connection'
+        connection_args['connection'] = {'id': connection_id}
+    else:
+        if "open_ai" in args.embeddings_model:
+            ws = get_workspace_from_environment()
+            connection_args["connection_type"] = "workspace_keyvault"
+            connection_args["connection"] = {
+                "subscription": ws.subscription_id if ws is not None else "",
+                "resource_group": ws.resource_group if ws is not None else "",
+                "workspace": ws.name if ws is not None else "",
+                "key": "OPENAI-API-KEY"
+            }
+
+    embeddings_container = embeddings_container if embeddings_container is not None else Embeddings.from_uri(args.embeddings_model, **connection_args)
 
 
 # TODO: Not handling throttling from openai api, need to back off more
 def _run_internal(mini_batch, output_data, embeddings):
     """
-    Internal run method, primarily used for unit tests.
+    Embed minibatch of chunks.
 
     :param mini_batch: The list of files to be processed.
     :param output_data: The output folder to save data to.
     :param embeddings: The Embeddings object that should be used to embed new data.
     """
     logger.info(f'run method start: {__file__}, run({mini_batch})', extra={'print': True})
     logger.info(f'Task id: {mini_batch.task_id}', extra={'print': True})
@@ -85,9 +103,10 @@
         logger.info('Metadata will be saved', extra={'print': True})
     else:
         logger.info('Only data will be saved', extra={'print': True})
     embeddings.save(output_data, with_metadata=save_metadata, suffix=mini_batch.task_id)
 
 
 def run(mini_batch):
-    _run_internal(mini_batch, output_data, previous_embeddings)
+    """Embed minibatch of chunks."""
+    _run_internal(mini_batch, output_data, embeddings_container)
     return pd.DataFrame({"Files": [os.path.split(file)[-1] for file in mini_batch]})
```

## azureml/rag/tasks/git_clone.py

```diff
@@ -1,13 +1,14 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 import argparse
+import os
 
-from azureml.rag.utils.git import clone_repo
+from azureml.rag.utils.git import clone_repo, get_keyvault_authentication
 from azureml.rag.utils.logging import get_logger, enable_stdout_logging
 
 logger = get_logger('git_clone')
 
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser()
@@ -15,10 +16,25 @@
     parser.add_argument("--branch-name", type=str, required=False, default=None)
     parser.add_argument("--authentication-key-prefix", type=str, required=False, default=None, help="<PREFIX>-USER and <PREFIX>-PASS are the expected names of two Secrets in the Workspace Key Vault which will be used for authenticated when pulling the given git repo.")
     parser.add_argument("--output-data", type=str, required=True, dest='output_data')
     args = parser.parse_args()
 
     enable_stdout_logging()
 
-    clone_repo(args.git_repository, args.output_data, args.branch_name, args.authentication_key_prefix)
+    connection_id = os.environ.get('AZUREML_WORKSPACE_CONNECTION_ID_GIT')
+    if connection_id is not None and connection_id != '':
+        from azureml.rag.utils.connections import get_connection_by_id_v2
+
+        connection = get_connection_by_id_v2(connection_id)
+        if args.git_repository != connection['properties']['target']:
+            logger.warning(f"Given git repository '{args.git_repository}' does not match the git repository '{connection['properties']['target']}' specified in the Workspace Connection '{connection_id}'. Using the Workspace Connection git repository.")
+        args.git_repository = connection['properties']['target']
+        authentication = {'username': connection['properties']['metadata']['username'], 'password': connection['properties']['credentials']['pat']}
+    elif args.authentication_key_prefix is not None:
+        authentication = get_keyvault_authentication(args.authentication_key_prefix)
+    else:
+        authentication = None
+
+
+    clone_repo(args.git_repository, args.output_data, args.branch_name, authentication)
 
     logger.info('Finished cloning.')
```

## azureml/rag/tasks/register_mlindex.py

```diff
@@ -1,26 +1,28 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
+"""File for registering ML Indexes."""
 import argparse
 from azureml.core import Run
 import fsspec
+import re
 import yaml
 
-from azureml.rag._asset_client.client import get_rest_client, register_new_data_asset_version
 from azureml.rag.utils.logging import get_logger, enable_stdout_logging
-
+from azureml.rag._asset_client.client import get_rest_client, register_new_data_asset_version
 
 logger = get_logger('register_mlindex')
 
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser()
     parser.add_argument("--storage-uri", type=str, required=True, dest='storage_uri')
     parser.add_argument("--asset-name", type=str, required=False, dest='asset_name', default='MLIndexAsset')
+    parser.add_argument("--output-asset-id", type=str, dest='output_asset_id')
     args = parser.parse_args()
 
     print('\n'.join(f'{k}={v}' for k, v in vars(args).items()))
 
     enable_stdout_logging()
 
     run: Run = Run.get_context()
@@ -49,10 +51,20 @@
         run,
         args.asset_name,
         args.storage_uri,
         properties={
             'azureml.mlIndexAssetKind': index_kind,
             'azureml.mlIndexAsset': 'true',
             'azureml.mlIndexAssetSource': run.properties.get('azureml.mlIndexAssetSource', 'Unknown'),
+            'azureml.mlIndexAssetPipelineRunId': run.properties.get('azureml.pipelinerunid', 'Unknown')
         })
 
+    print(data_version.asset_id)
+
+    asset_id = re.sub('azureml://locations/(.*)/workspaces/(.*)/data', f'azureml://subscriptions/{ws._subscription_id}/resourcegroups/{ws._resource_group}/providers/Microsoft.MachineLearningServices/workspaces/{ws._workspace_name}/data', data_version.asset_id)
+
+    print(asset_id)
+
+    with open(args.output_asset_id, 'w') as f:
+        f.write(asset_id)
+
     logger.info(f"Finished Registering MLIndex Asset '{args.asset_name}', version = {data_version.version_id}")
```

## azureml/rag/tasks/update_acs.py

```diff
@@ -15,35 +15,35 @@
 from azure.identity import DefaultAzureCredential
 from azure.core.credentials import AzureKeyCredential
 from azure.search.documents.indexes import SearchIndexClient
 from azure.search.documents.indexes.models import SearchIndex, SimpleField, SearchableField, SemanticSettings, SemanticConfiguration, PrioritizedFields, SemanticField, SearchField, ComplexField
 from azure.search.documents import SearchClient
 
 from azureml.rag.embeddings import Embeddings
-from azureml.rag.utils.azureml import get_secret_from_workspace
+from azureml.rag.utils.connections import get_connection_credential
 from azureml.rag.utils.logging import get_logger, enable_stdout_logging
 
 
 logger = get_logger('update_acs')
 
 _azure_logger = logging.getLogger('azure.core.pipeline')
 _azure_logger.setLevel(logging.WARNING)
 
 
-def search_client_from_config(acs_config: dict):
+def search_client_from_config(acs_config: dict, credential):
     return SearchClient(endpoint=acs_config['endpoint'],
                         index_name=acs_config['index_name'],
-                        credential=acs_config['credential'],
+                        credential=credential,
                         api_version=acs_config['api_version'])
 
 
-def create_search_index_sdk(acs_config: dict):
+def create_search_index_sdk(acs_config: dict, credential):
     logger.info(f"Ensuring search index {acs_config['index_name']} exists", extra={'print': True})
     index_client = SearchIndexClient(endpoint=acs_config['endpoint'],
-                                    credential=acs_config['credential'])
+                                    credential=credential)
     if acs_config['index_name'] not in index_client.list_index_names():
         index = SearchIndex(
             name=acs_config['index_name'],
             fields=[
                 SimpleField(name="id", type="Edm.String", key=True),
                 SearchableField(name="content", type="Edm.String", analyzer_name="en.microsoft"),
                 SimpleField(name="category", type="Edm.String", filterable=True, facetable=True),
@@ -78,30 +78,31 @@
     response = requests.put(url, data=json.dumps(payload), headers=headers)
 
     # Raise an exception if the response contains an HTTP error status code
     response.raise_for_status()
     return response
 
 
-def create_search_index_rest(acs_config: dict, embeddings: Optional[Embeddings] = None):
+def create_search_index_rest(acs_config: dict, credential, embeddings: Optional[Embeddings] = None):
     # TODO: Ask users in private preview to provide the new api_version? 2023-07-01-Preview
     logger.info(f"Ensuring search index {acs_config['index_name']} exists", extra={'print': True})
+    if 'api_version' not in acs_config:
+        acs_config['api_version'] = "2023-07-01-preview"
     index_client = SearchIndexClient(endpoint=acs_config['endpoint'],
-                                    credential=acs_config['credential'])
+                                    credential=credential,
+                                    api_version=acs_config['api_version'])
     if acs_config['index_name'] not in index_client.list_index_names():
-        if 'api_version' not in acs_config:
-            acs_config['api_version'] = "2023-07-01-preview"
         base_url = f"{acs_config['endpoint']}/indexes/{acs_config['index_name']}?api-version={acs_config['api_version']}"
         headers = {
             "Content-Type": "application/json"
         }
-        if isinstance(acs_config['credential'], DefaultAzureCredential):
-            headers["Authorization"] = f"Bearer {acs_config['credential'].get_token('https://search.azure.com/.default').token}"
+        if isinstance(credential, DefaultAzureCredential):
+            headers["Authorization"] = f"Bearer {credential.get_token('https://search.azure.com/.default').token}"
         else:
-            headers["api-key"] = acs_config['credential'].key
+            headers["api-key"] = credential.key
 
         payload = {
             "name": acs_config['index_name'],
             "fields": [
                 {"name": "id", "type": "Edm.String", "key": True},
                 {"name": "content", "type": "Edm.String", "searchable": True},
                 {"name": "category", "type": "Edm.String", "filterable": True, "facetable": True},
@@ -139,92 +140,147 @@
                 "algorithmConfigurations": [
                     {
                         "name": f"{field_name}_config",
                         "kind": "hnsw",
                         "hnswParameters": {
                             "m": 4,
                             "efConstruction": 400,
-                            "metric": "cosine"
+                            "metric": "cosine",
+                            "efSearch": 500
                         }
                     }
                 ]
             }
 
         try:
             response = send_put_request(base_url, headers, payload)
-            logger.info(response)
+            logger.info(response.text)
         except requests.exceptions.RequestException as e:
             logger.error(f"Request failed: {e}\nResponse: {e.response.text}")
             raise e
     else:
         logger.info(f"Search index {acs_config['index_name']} already exists", extra={'print': True})
 
 
-def upsert_documents_sdk():
-    pass
+@tenacity.retry(
+    wait=tenacity.wait_fixed(5),  # wait 5 seconds between retries
+    stop=tenacity.stop_after_attempt(3),  # stop after 3 attempts
+    reraise=True,  # re-raise the exception after the last retry attempt
+)
+def send_post_request(url, headers, payload):
+    response = requests.post(url, data=json.dumps(payload), headers=headers)
+
+    # Raise an exception if the response contains an HTTP error status code
+    response.raise_for_status()
+    return response
 
-def upsert_documents_rest():
-    pass
+
+def upload_documents_rest(acs_config: dict, credential, documents):
+    if 'api_version' not in acs_config:
+        acs_config['api_version'] = "2023-07-01-preview"
+    base_url = f"{acs_config['endpoint']}/indexes/{acs_config['index_name']}/docs/index?api-version={acs_config['api_version']}"
+    headers = {
+        "Content-Type": "application/json"
+    }
+    if isinstance(credential, DefaultAzureCredential):
+        headers["Authorization"] = f"Bearer {credential.get_token('https://search.azure.com/.default').token}"
+    else:
+        headers["api-key"] = credential.key
+
+    payload = {
+        "value": documents
+    }
+
+    try:
+        response = send_post_request(base_url, headers, payload)
+        logger.info(response.text)
+    except requests.exceptions.RequestException as e:
+        logger.error(f"Request failed: {e}\nResponse: {e.response.text}")
+        raise e
+
+    return response.json()['value']
 
 
-def create_index_from_raw_embeddings(emb: Embeddings, acs_config={}, output_path: Optional[str] = None):
+def create_index_from_raw_embeddings(emb: Embeddings, acs_config={}, connection={}, output_path: Optional[str] = None):
     logger.info("Updating ACS index", extra={'print': True})
 
-    if 'push_embeddings' in acs_config:
-        create_search_index_rest(acs_config, emb)
+    credential = get_connection_credential(connection)
+
+    if str(acs_config.get('push_embeddings')).lower() == "true":
+        create_search_index_rest(acs_config, credential, emb)
+
+        def upload_docs_batch(batch):
+            return upload_documents_rest(acs_config, credential, batch)
     else:
-        create_search_index_sdk(acs_config)
+        create_search_index_sdk(acs_config, credential)
+        search_client = search_client_from_config(acs_config, credential)
 
-    search_client = search_client_from_config(acs_config)
+        def upload_docs_batch(batch):
+            return search_client.upload_documents(documents=batch)
 
     batch_size = acs_config['batch_size'] if 'batch_size' in acs_config else 100
 
+    def process_upload_results(results):
+        succeeded = []
+        failed = []
+        for r in results:
+            if isinstance(r, dict):
+                if r['status'] == False:
+                    failed.append(r)
+                else:
+                    succeeded.append(r)
+            else:
+                if r.succeeded:
+                    succeeded.append(r)
+                else:
+                    failed.append(r)
+        logger.info(f"Uploaded {len(succeeded)} documents to ACS in {time.time() - start_time:.4f} seconds, {len(failed)} failed", extra={'print': True})
+        if len(failed) > 0:
+            for r in results:
+                if isinstance(r, dict):
+                    error = r['error_message']
+                else:
+                    error = r.error_message
+                logger.error(f"Failed document reason: {error}", extra={'print': True})
+            raise RuntimeError(f"Failed to upload {len(failed)} documents.")
+
     t1 = time.time()
     num_source_docs = 0
     batch = []
     for doc_id, emb_doc in emb._document_embeddings.items():
         logger.info(f'Adding document: {doc_id}', extra={'print': True})
         acs_doc = {
+            "@search.action": "mergeOrUpload",
             "id": base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8'),
             "content": emb_doc.get_data(),
             "category": "document",
             "sourcepage": emb_doc.metadata.get("source", {}).get("url"),
             "sourcefile": emb_doc.metadata.get("source", {}).get("filename"),
             "title": emb_doc.metadata.get("source", {}).get("title"),
             "content_hash": emb_doc.document_hash,
             "meta_json_string": json.dumps(emb_doc.metadata),
         }
 
-        if 'push_embeddings' in acs_config:
+        if str(acs_config.get('push_embeddings')).lower() == "true":
             acs_doc[f"content_vector_{emb.kind}"] = emb_doc.get_embeddings()
 
         batch.append(acs_doc)
         if len(batch) % batch_size == 0:
             logger.info(f"Sending {len(batch)} documents to ACS", extra={'print': True})
             start_time = time.time()
-            results = search_client.upload_documents(documents=batch)
-            succeeded = []
-            failed = []
-            for r in results:
-                if r.succeeded:
-                    succeeded.append(r)
-                else:
-                    failed.append(r)
-            logger.info(f"Uploaded {len(succeeded)} documents to ACS in {time.time() - start_time:.4f} seconds, {len(failed)} failed", extra={'print': True})
-            if len(failed) > 0:
-                logger.error(f"Failed documents: {failed}", extra={'print': True})
+            results = upload_docs_batch(batch)
+            process_upload_results(results)
             batch = []
             num_source_docs += batch_size
 
     if len(batch) > 0:
         logger.info(f"Sending {len(batch)} documents to ACS", extra={'print': True})
         start_time = time.time()
-        results = search_client.upload_documents(documents=batch)
-        succeeded = sum(1 for r in results if r.succeeded)
-        logger.info(f"Uploaded {succeeded} documents to ACS in {time.time() - start_time:.4f} seconds, {len(batch) - succeeded} failed", extra={'print': True})
+        results = upload_docs_batch(batch)
+        process_upload_results(results)
 
         num_source_docs += len(batch)
 
     logger.info(f"Built index from {num_source_docs} documents and {len(emb._document_embeddings)} chunks, took {time.time()-t1:.4f} seconds", extra={'print': True})
 
     if output_path is not None:
         logger.info('Writing MLIndex yaml', extra={'print': True})
@@ -240,28 +296,20 @@
                 "content": "content",
                 "url": "sourcepage",
                 "filename": "sourcefile",
                 "title": "title",
                 "metadata": "meta_json_string",
             }
         }
-        if 'push_embeddings' in acs_config:
+        if str(acs_config.get('push_embeddings')).lower() == "true":
             mlindex_config["index"]["field_mapping"]["embedding"] = f"content_vector_{emb.kind}"
 
-        if isinstance(acs_config['credential'], AzureKeyCredential):
-            from azureml.core import Run
-            run = Run.get_context()
-            ws = run.experiment.workspace
-            mlindex_config["index"]["connection_type"] = "workspace_keyvault"
-            mlindex_config["index"]["connection"] = {
-                "subscription": ws.subscription_id,
-                "resource_group": ws.resource_group,
-                "workspace": ws.name,
-                "key": acs_config['endpoint_key_name']
-            }
+        if not isinstance(connection, DefaultAzureCredential):
+            mlindex_config["index"] = {**mlindex_config["index"], **connection}
+
         # Keyvault auth and Default ambient auth need the endpoint, Workspace Connection auth could get endpoint.
         mlindex_config["index"]["endpoint"] = acs_config['endpoint']
         output = Path(output_path)
         output.mkdir(parents=True, exist_ok=True)
         with open(output / "MLIndex", "w") as f:
             yaml.dump(mlindex_config, f)
 
@@ -285,25 +333,38 @@
     embeddings_dir_name = splits.pop(len(splits)-2)
     logger.info(f'extracted embeddings directory name: {embeddings_dir_name}', extra={'print': True})
     parent = '/'.join(splits)
     logger.info(f'extracted embeddings container path: {parent}', extra={'print': True})
 
     acs_config = json.loads(args.acs_config)
 
-    if 'endpoint_key_name' in acs_config:
-        from azure.core.credentials import AzureKeyCredential
-        name = acs_config['endpoint_key_name']
-        key = get_secret_from_workspace(name)
-        acs_config['credential'] = AzureKeyCredential(key)
-    else:
-        from azure.identity import DefaultAzureCredential
-        acs_config['credential'] = DefaultAzureCredential()
+    connection_args = {}
+    connection_id = os.environ.get('AZUREML_WORKSPACE_CONNECTION_ID_ACS')
+    if connection_id is not None:
+        connection_args['connection_type'] = 'workspace_connection'
+        connection_args['connection'] = {'id': connection_id}
+        from azureml.rag.utils.connections import get_connection_by_id_v2
+
+        connection = get_connection_by_id_v2(connection_id)
+        acs_config['endpoint'] = connection['properties']['target']
+        acs_config['api_version'] = connection['properties'].get('metadata', {}).get('apiVersion', "2023-07-01-preview")
+    elif 'endpoint_key_name' in acs_config:
+        connection_args['connection_type'] = 'workspace_keyvault'
+        from azureml.core import Run
+        run = Run.get_context()
+        ws = run.experiment.workspace
+        connection_args['connection'] = {
+            'key': acs_config['endpoint_key_name'],
+            "subscription": ws.subscription_id,
+            "resource_group": ws.resource_group,
+            "workspace": ws.name,
+        }
 
     from azureml.dataprep.fuse.dprepfuse import (MountOptions, rslex_uri_volume_mount)
     mnt_options = MountOptions(
         default_permission=0o555, allow_other=False, read_only=True)
     logger.info(f'mounting embeddings container from: \n{parent} \n   to: \n{os.getcwd()}/embeddings_mount', extra={'print': True})
     with rslex_uri_volume_mount(parent, f'{os.getcwd()}/embeddings_mount', options=mnt_options) as mount_context:
         emb = Embeddings.load(embeddings_dir_name, mount_context.mount_point)
-        create_index_from_raw_embeddings(emb, acs_config=acs_config, output_path=args.output)
+        create_index_from_raw_embeddings(emb, acs_config=acs_config, connection=connection_args, output_path=args.output)
 
     logger.info('Updated ACS index', extra={'print': True})
```

## azureml/rag/utils/git.py

```diff
@@ -25,22 +25,27 @@
             git_logger.info("Begin clone")
         elif op_code & git.remote.RemoteProgress.END:
             git_logger.info("Clone complete")
         elif op_code & git.remote.RemoteProgress.COUNTING:
             git_logger.info("Received %d/%d objects" % (cur_count, max_count))
 
 
-def clone_repo(git_url: str, local_path: Path, branch: Optional[str] = None, authentication_key_prefix: Optional[str] = None):
+def get_keyvault_authentication(authentication_key_prefix: str):
+    """Get the username and password for a keyvault authentication key"""""
+    username = get_secret_from_workspace(f'{authentication_key_prefix}-USER')
+    password = get_secret_from_workspace(f'{authentication_key_prefix}-PASS')
+    return {'username': username, 'password': password}
+
+
+def clone_repo(git_url: str, local_path: Path, branch: Optional[str] = None, authentication: Optional[dict] = None):
     """Clone a git repository to a local path, optionally checking out a branch"""
     logger.info(f'Cloning {git_url} to {local_path}')
 
-    if authentication_key_prefix is not None:
-        username = get_secret_from_workspace(f'{authentication_key_prefix}-USER')
-        password = get_secret_from_workspace(f'{authentication_key_prefix}-PASS')
-        git_url = git_url.replace('https://', f'https://{username}:{password}@')
+    if authentication is not None:
+        git_url = git_url.replace('https://', f'https://{authentication["username"]}:{authentication["password"]}@')
 
     logger.info(f'Cloning with depth={1 if branch is None else None}')
     repo = git.Repo.clone_from(git_url, local_path, progress=GitCloneProgress(), depth=1 if branch is None else None)
     if branch is not None:
         logger.info('fetch --all')
         repo.git.fetch("--all")
         logger.info(f'checkout {branch}')
```

## Comparing `azureml_rag-0.1.2.dist-info/LICENSE.txt` & `azureml_rag-0.1.3.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `azureml_rag-0.1.2.dist-info/METADATA` & `azureml_rag-0.1.3.dist-info/METADATA`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: azureml-rag
-Version: 0.1.2
+Version: 0.1.3
 Summary: Contains Retrieval Augmented Generation related utilities for Azure Machine Learning and OSS interoperability.
 Home-page: https://docs.microsoft.com/python/api/overview/azure/ml/?view=azure-ml-py
 Author: Microsoft Corporation
 License: Proprietary https://aka.ms/azureml-preview-sdk-license 
 Platform: UNKNOWN
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Intended Audience :: Developers
@@ -17,15 +17,15 @@
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Classifier: Operating System :: MacOS
 Classifier: Operating System :: Microsoft :: Windows
 Classifier: Operating System :: POSIX :: Linux
 Classifier: Topic :: Scientific/Engineering
 Requires-Python: >=3.7,<4.0
-Description-Content-Type: text/x-rst
+Description-Content-Type: text/markdown
 Requires-Dist: azureml-dataprep[parquet] (<4.11.0a,>=4.10.0a)
 Requires-Dist: azureml-mlflow (~=1.50.0)
 Requires-Dist: azureml-fsspec
 Requires-Dist: fsspec (~=2023.3)
 Requires-Dist: openai (~=0.27.4)
 Requires-Dist: tiktoken (~=0.3.0)
 Requires-Dist: langchain (>=0.0.149)
@@ -36,19 +36,22 @@
 Provides-Extra: document_parsing
 Requires-Dist: pandas (>=1) ; extra == 'document_parsing'
 Requires-Dist: nltk (~=3.8.1) ; extra == 'document_parsing'
 Requires-Dist: markdown ; extra == 'document_parsing'
 Requires-Dist: beautifulsoup4 (~=4.11.2) ; extra == 'document_parsing'
 Requires-Dist: tika (~=2.6.0) ; extra == 'document_parsing'
 Requires-Dist: pypdf (~=3.7.0) ; extra == 'document_parsing'
+Requires-Dist: unstructured ; extra == 'document_parsing'
 Provides-Extra: faiss
 Requires-Dist: faiss-cpu (~=1.7.3) ; extra == 'faiss'
 Provides-Extra: hugging_face
 Requires-Dist: scikit-learn (~=0.24.2) ; extra == 'hugging_face'
 Requires-Dist: sentence-transformers ; extra == 'hugging_face'
+Provides-Extra: remote_tests
+Requires-Dist: azure-ai-ml ; extra == 'remote_tests'
 
 # AzureML Retrieval Augmented Generation Utilities
 
 This package is in alpha stage at the moment, use at risk of breaking changes and unstable behavior.
 
 It contains utilities for:
 
@@ -69,14 +72,18 @@
 ## Documentation
 
 TBA
 
 
 # Changelog
 
+## 0.2.0 (2023-05-12)
+
+- Support Workspace Connection based auth for Git, Azure OpenAI and Azure Cognitive Search usage.
+
 ## 0.1.2 (2023-05-05)
 
 - Refactored document chunking to allow insertion of custom processing logic
 
 ## 0.0.1 (2023-04-25)
 
 ### Features Added
```

## Comparing `azureml_rag-0.1.2.dist-info/RECORD` & `azureml_rag-0.1.3.dist-info/RECORD`

 * *Files 4% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 azureml/rag/__init__.py,sha256=zgRl-az58He0no_ZUWab0AuVQSOTmgCLlfwbEcnwKS0,246
-azureml/rag/documents.py,sha256=4MJunpYwPQ8nrulPlO1Rig1ruvHPVDb6b596FKWnIzU,39484
-azureml/rag/embeddings.py,sha256=rqRQpb4BtZMLPO-qPMWhYu6h9BwaybpSZOLlRwIWlVE,27816
-azureml/rag/mlindex.py,sha256=mqVExoxEI1XIBq2c-5R_xevGV4FxA9i7Dug4og6X9a4,7469
-azureml/rag/models.py,sha256=q7-w3W9xoavxKUkmwkYoZFvBjlh7Umtr6vDeK6YzxV0,3992
+azureml/rag/documents.py,sha256=5lhL6lxXhTa1wlmyLhaz7Ob-ePqzC8lYpaYF50Kqi-M,39995
+azureml/rag/embeddings.py,sha256=XAFRu19tq_F8bixWFGXzwS5nnzAJx6XLP_Zr9hS33mk,27598
+azureml/rag/mlindex.py,sha256=DNb-Z15nTzjmGZH5h0RGjqxQRvnIDg756W8gJDbObhA,5058
+azureml/rag/models.py,sha256=Ga9tvV0FsVhXprEeWpPVakNJ9q96byc0OjDESqkMuCw,4149
 azureml/rag/_asset_client/client.py,sha256=LpsDsedlQRxytezdvnxx1zfEQUb9DcYDY6RfkRSlXT0,4646
 azureml/rag/_asset_client/_restclient/__init__.py,sha256=38lKUIqL59KqhES7ZGBUGcrELWICWet0VFLxwY4W0fo,893
 azureml/rag/_asset_client/_restclient/_azure_machine_learning_workspaces.py,sha256=7XqTcSuvXHcKXu4FE71E4rBQAQ4Fmpdq8SXrhU7_AAg,4381
 azureml/rag/_asset_client/_restclient/_configuration.py,sha256=E52K3BmA4ZqAE6oP5VjRK32HsBrl6W9Y1oOsWCJu8Ig,3538
 azureml/rag/_asset_client/_restclient/_patch.py,sha256=wuqrJGWK488sJvWwSq6iwPTqil7TPaRadoxE7BMK0tA,1561
 azureml/rag/_asset_client/_restclient/_version.py,sha256=72yoX3gRVc4OFrnlCjEq_1cS0FLSIvofYIXtMN1ElvI,495
 azureml/rag/_asset_client/_restclient/models.py,sha256=rethYtGgAYZ6YZ51EbJFIS62FN1sJLUSGNjDvMdueCg,399
@@ -32,25 +32,26 @@
 azureml/rag/_asset_client/_restclient/runhistory/_version.py,sha256=72yoX3gRVc4OFrnlCjEq_1cS0FLSIvofYIXtMN1ElvI,495
 azureml/rag/_asset_client/_restclient/runhistory/models/__init__.py,sha256=RSjYy-jYXXp31fA5tEprD1uPTdaoCJsE4IKfDBlg8a0,11705
 azureml/rag/_asset_client/_restclient/runhistory/models/_azure_machine_learning_workspaces_enums.py,sha256=cyvWolLt_kCOalE8BvtnI_msfmAAlTYzdob0QDp1KZ0,2566
 azureml/rag/_asset_client/_restclient/runhistory/models/_models.py,sha256=CqQccTVrcm5_9uBZOJ6AMubF9K8GphuIzmF1j46G84Q,175170
 azureml/rag/_asset_client/_restclient/runhistory/models/_models_py3.py,sha256=G5x7IlIzWN98TsXpY80M5nyLjXIwBWWeL9Fk6Piwpzw,188776
 azureml/rag/_asset_client/_restclient/runhistory/operations/__init__.py,sha256=FFrjEyJdO53e7YDI8o_8W9BgTTEVt1ZzS8FwwnNp7-g,563
 azureml/rag/_asset_client/_restclient/runhistory/operations/_runs_operations.py,sha256=1nl-ETYrSQSZH3B-a2QXTKIUCx-Hx3XeQPmgAwKSGoA,162796
-azureml/rag/langchain/acs.py,sha256=-3r30nnXFBey79nCiHW09IlMeKQjW0M6p-P7GXhCGj0,9201
+azureml/rag/langchain/acs.py,sha256=zC6qLmyvQKvM48bMKC9CBtaiEvGx_TalMUyHE8I-nbc,10785
 azureml/rag/tasks/__init__.py,sha256=JlCCObuOLZyNhIZlsoL51KtFxiY4xKIIzIRDBcdeu6Y,183
-azureml/rag/tasks/build_faiss.py,sha256=bVXfYqR3AlzoEJXO3etHI1BYt3ONJ1-Rphabzn6dO4c,5480
-azureml/rag/tasks/crack_and_chunk.py,sha256=EfZQGNvnCJp69cVNx51Q9GR2g3l4D-v5tsX84lxyA1c,7027
+azureml/rag/tasks/build_faiss.py,sha256=dXETVJJT77PGrav6OiUBzve5EjqOutNcD5cpP1vIHxU,2094
+azureml/rag/tasks/crack_and_chunk.py,sha256=V-aqVaGvJx9Gp80A6K0sQZvdOij9yiwwPv04VZNajD4,7191
 azureml/rag/tasks/embed.py,sha256=yr_PnSq1a-BLh7EX7TvIVeusk2iD4Udw-xXlvpiCCI0,7330
-azureml/rag/tasks/embed_prs.py,sha256=Ks3sf-iAb8kt-PjHRPFsDWPZ2Ho61yUnbgsBcVdRgiY,4426
-azureml/rag/tasks/git_clone.py,sha256=tMuihmOVFPFv04pV9jyAAaRd_IJy-Y93iXBTRmZPpeI,1172
-azureml/rag/tasks/register_mlindex.py,sha256=kmLhxOhwRbU-7NuNSYt4P8ek2Sapda2QBVZtLYBHI7o,2157
-azureml/rag/tasks/update_acs.py,sha256=HOCZsNUrUwz6VxmDCNHuJ3faYLx14t0BX5ytVNSxJNo,14572
+azureml/rag/tasks/embed_prs.py,sha256=PmtPLaopfPhW2RC1EIQRuHzEAj-jJFIGZie3psQIcR8,5410
+azureml/rag/tasks/git_clone.py,sha256=Y_f4EiPLG000asZWsYMvhheLPHsKJHsXyvmHyGiEoPA,2194
+azureml/rag/tasks/register_mlindex.py,sha256=4T7UEav88naHA-bzKWija8pdAwOlRV2kAWmVXyJXi6A,2791
+azureml/rag/tasks/update_acs.py,sha256=Rf15hg17COV3BwFlOPvCmzhMigXUB_QZQ6Wo2NQMpw4,17073
 azureml/rag/utils/__init__.py,sha256=FTY5BaTKeythd7R1SXsf3midWHdshZj-bdFZLmZ7J50,208
 azureml/rag/utils/azureml.py,sha256=dOfvDTgqjTWHg-I9Ho7_o6e7I8kNcVJdl53O0Ts7xQs,1588
-azureml/rag/utils/git.py,sha256=VsErY3sT_jbt5Ozlhk8E_-Xz5MVTW3ME6T53_-splMk,2139
+azureml/rag/utils/connections.py,sha256=zInLr6twjfHW4fF2zgChtNSI7YqnxY3eOVnu74xpq-Y,7435
+azureml/rag/utils/git.py,sha256=C_79CuCdBNLrid1eUf2nruPynpIgghbf9d9ArnpI_Wc,2350
 azureml/rag/utils/logging.py,sha256=fusY7czHGANkXnZdtdKc9TQjaeH-dSsC0Vts5xWQbfQ,2330
-azureml_rag-0.1.2.dist-info/LICENSE.txt,sha256=FOfkEEz4uS7g278F9Rq12rqKF3lLyBUZhVpLetuZrTg,1021
-azureml_rag-0.1.2.dist-info/METADATA,sha256=bQ-Eo3uV9Q_QzO4mE5qpY2C9qMZMOP2yMIyPp8lKiQ4,3217
-azureml_rag-0.1.2.dist-info/WHEEL,sha256=YUYzQ6UQdoqxXjimOitTqynltBCkwY6qlTfTh2IzqQU,97
-azureml_rag-0.1.2.dist-info/top_level.txt,sha256=ZOeEa0TAXo6i5wOjwBoqfIGEuxOcKuscGgNSpizqREY,8
-azureml_rag-0.1.2.dist-info/RECORD,,
+azureml_rag-0.1.3.dist-info/LICENSE.txt,sha256=FOfkEEz4uS7g278F9Rq12rqKF3lLyBUZhVpLetuZrTg,1021
+azureml_rag-0.1.3.dist-info/METADATA,sha256=Ie90rTL7DikTHl7kJW_gkKatFnh5zxz_JV0_k9GrakM,3482
+azureml_rag-0.1.3.dist-info/WHEEL,sha256=YUYzQ6UQdoqxXjimOitTqynltBCkwY6qlTfTh2IzqQU,97
+azureml_rag-0.1.3.dist-info/top_level.txt,sha256=ZOeEa0TAXo6i5wOjwBoqfIGEuxOcKuscGgNSpizqREY,8
+azureml_rag-0.1.3.dist-info/RECORD,,
```

