# Comparing `tmp/k1lib-1.3.7.9-py3-none-any.whl.zip` & `tmp/k1lib-1.3.8-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,16 +1,16 @@
-Zip file size: 2534659 bytes, number of entries: 84
+Zip file size: 2538244 bytes, number of entries: 84
 -rw-rw-r--  2.0 unx     1435 b- defN 23-Feb-08 00:13 k1lib/__init__.py
 -rw-rw-r--  2.0 unx    53793 b- defN 23-Apr-02 08:25 k1lib/_baseClasses.py
 -rw-rw-r--  2.0 unx    13494 b- defN 23-May-05 13:41 k1lib/_basics.py
 -rw-rw-r--  2.0 unx     4908 b- defN 23-May-08 18:53 k1lib/_context.py
 -rw-rw-r--  2.0 unx     2866 b- defN 22-Jul-20 04:30 k1lib/_higher.py
 -rw-rw-r--  2.0 unx      948 b- defN 22-Sep-21 23:54 k1lib/_k1a.py
 -rw-rw-r--  2.0 unx    11646 b- defN 23-Jan-14 15:37 k1lib/_learner.py
--rw-rw-r--  2.0 unx    19722 b- defN 23-May-08 18:56 k1lib/_monkey.py
+-rw-rw-r--  2.0 unx    21467 b- defN 23-May-15 18:54 k1lib/_monkey.py
 -rw-rw-r--  2.0 unx     3571 b- defN 21-Nov-04 18:35 k1lib/_perlin.py
 -rw-rw-r--  2.0 unx    13377 b- defN 21-Dec-29 02:58 k1lib/eqn.py
 -rw-rw-r--  2.0 unx     6317 b- defN 23-May-07 04:31 k1lib/fmt.py
 -rw-rw-r--  2.0 unx     6398 b- defN 22-Sep-29 08:23 k1lib/graphEqn.py
 -rw-rw-r--  2.0 unx     2750 b- defN 23-May-11 19:08 k1lib/imports.py
 -rw-rw-r--  2.0 unx     3107 b- defN 22-Oct-26 20:43 k1lib/knn.py
 -rw-rw-r--  2.0 unx     3564 b- defN 23-Jan-25 02:48 k1lib/p5.py
@@ -43,27 +43,27 @@
 -rw-rw-r--  2.0 unx     3465 b- defN 22-Nov-27 08:16 k1lib/callbacks/lossFunctions/shorts.py
 -rw-rw-r--  2.0 unx       45 b- defN 21-Aug-11 18:19 k1lib/callbacks/profilers/__init__.py
 -rw-rw-r--  2.0 unx     5054 b- defN 22-May-15 09:01 k1lib/callbacks/profilers/computation.py
 -rw-rw-r--  2.0 unx     2319 b- defN 22-May-15 08:59 k1lib/callbacks/profilers/io.py
 -rw-rw-r--  2.0 unx     4419 b- defN 22-May-15 09:00 k1lib/callbacks/profilers/memory.py
 -rw-rw-r--  2.0 unx     4215 b- defN 22-May-15 09:03 k1lib/callbacks/profilers/time.py
 -rw-rw-r--  2.0 unx      925 b- defN 22-Nov-16 09:22 k1lib/cli/__init__.py
--rw-rw-r--  2.0 unx     6056 b- defN 23-May-11 19:50 k1lib/cli/_applyCl.py
+-rw-rw-r--  2.0 unx    11353 b- defN 23-May-15 18:50 k1lib/cli/_applyCl.py
 -rw-rw-r--  2.0 unx     8308 b- defN 22-Nov-27 07:16 k1lib/cli/bio.py
 -rw-rw-r--  2.0 unx     4033 b- defN 23-Jan-25 02:02 k1lib/cli/cif.py
--rw-rw-r--  2.0 unx    18911 b- defN 23-May-05 13:24 k1lib/cli/conv.py
+-rw-rw-r--  2.0 unx    19291 b- defN 23-May-12 02:55 k1lib/cli/conv.py
 -rw-rw-r--  2.0 unx    24092 b- defN 23-May-10 23:07 k1lib/cli/filt.py
 -rw-rw-r--  2.0 unx     6672 b- defN 23-Jan-25 02:02 k1lib/cli/gb.py
 -rw-rw-r--  2.0 unx     6348 b- defN 23-Apr-05 15:10 k1lib/cli/grep.py
--rw-rw-r--  2.0 unx    18312 b- defN 23-May-02 14:44 k1lib/cli/init.py
--rw-rw-r--  2.0 unx    21754 b- defN 23-May-09 22:55 k1lib/cli/inp.py
+-rw-rw-r--  2.0 unx    18351 b- defN 23-May-12 23:10 k1lib/cli/init.py
+-rw-rw-r--  2.0 unx    24942 b- defN 23-May-15 20:29 k1lib/cli/inp.py
 -rw-rw-r--  2.0 unx      623 b- defN 22-Jun-22 10:43 k1lib/cli/kcsv.py
 -rw-rw-r--  2.0 unx     4819 b- defN 22-Aug-11 20:44 k1lib/cli/kxml.py
 -rw-rw-r--  2.0 unx     1915 b- defN 21-Nov-12 16:48 k1lib/cli/mgi.py
--rw-rw-r--  2.0 unx    53907 b- defN 23-May-11 19:14 k1lib/cli/modifier.py
+-rw-rw-r--  2.0 unx    55103 b- defN 23-May-15 19:32 k1lib/cli/modifier.py
 -rw-rw-r--  2.0 unx      694 b- defN 22-Nov-27 07:17 k1lib/cli/mol.py
 -rw-rw-r--  2.0 unx     4038 b- defN 23-May-09 15:53 k1lib/cli/nb.py
 -rw-rw-r--  2.0 unx     3530 b- defN 22-Aug-16 15:08 k1lib/cli/optimizations.py
 -rw-rw-r--  2.0 unx    11559 b- defN 23-Apr-25 00:01 k1lib/cli/output.py
 -rw-rw-r--  2.0 unx     2394 b- defN 23-Jan-25 02:03 k1lib/cli/sam.py
 -rw-rw-r--  2.0 unx    49930 b- defN 23-May-11 15:29 k1lib/cli/structural.py
 -rw-rw-r--  2.0 unx    10399 b- defN 22-Aug-05 01:15 k1lib/cli/trace.py
@@ -71,16 +71,16 @@
 -rw-rw-r--  2.0 unx    20345 b- defN 23-Apr-26 17:09 k1lib/cli/utils.py
 -rw-rw-r--  2.0 unx       20 b- defN 23-Jan-19 22:00 k1lib/k1ui/__init__.py
 -rw-rw-r--  2.0 unx    61803 b- defN 23-Feb-10 12:10 k1lib/k1ui/main.py
 -rw-rw-r--  2.0 unx       20 b- defN 22-Sep-16 01:12 k1lib/serve/__init__.py
 -rw-rw-r--  2.0 unx    10361 b- defN 23-May-05 16:49 k1lib/serve/main.py
 -rw-rw-r--  2.0 unx      153 b- defN 23-May-05 16:00 k1lib/serve/suffix-dash.py
 -rw-rw-r--  2.0 unx      642 b- defN 23-Feb-13 19:00 k1lib/serve/suffix.py
--rw-rw-r--  2.0 unx  2453826 b- defN 23-Jan-19 22:50 k1lib-1.3.7.9.data/data/k1lib/k1ui/256.model.state_dict.pth
--rw-rw-r--  2.0 unx   304735 b- defN 23-Jan-17 19:16 k1lib-1.3.7.9.data/data/k1lib/k1ui/mouseKey.pth
--rw-rw-r--  2.0 unx    20544 b- defN 23-Mar-19 11:14 k1lib-1.3.7.9.data/data/k1lib/serve/main.html
--rw-rw-r--  2.0 unx     1049 b- defN 23-May-11 19:51 k1lib-1.3.7.9.dist-info/LICENSE
--rw-rw-r--  2.0 unx     3864 b- defN 23-May-11 19:51 k1lib-1.3.7.9.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-May-11 19:51 k1lib-1.3.7.9.dist-info/WHEEL
--rw-rw-r--  2.0 unx        6 b- defN 23-May-11 19:51 k1lib-1.3.7.9.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     6703 b- defN 23-May-11 19:51 k1lib-1.3.7.9.dist-info/RECORD
-84 files, 3488212 bytes uncompressed, 2524319 bytes compressed:  27.6%
+-rw-rw-r--  2.0 unx  2453826 b- defN 23-Jan-19 22:50 k1lib-1.3.8.data/data/k1lib/k1ui/256.model.state_dict.pth
+-rw-rw-r--  2.0 unx   304735 b- defN 23-Jan-17 19:16 k1lib-1.3.8.data/data/k1lib/k1ui/mouseKey.pth
+-rw-rw-r--  2.0 unx    20544 b- defN 23-Mar-19 11:14 k1lib-1.3.8.data/data/k1lib/serve/main.html
+-rw-rw-r--  2.0 unx     1049 b- defN 23-May-15 20:29 k1lib-1.3.8.dist-info/LICENSE
+-rw-rw-r--  2.0 unx     3862 b- defN 23-May-15 20:29 k1lib-1.3.8.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-May-15 20:29 k1lib-1.3.8.dist-info/WHEEL
+-rw-rw-r--  2.0 unx        6 b- defN 23-May-15 20:29 k1lib-1.3.8.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     6688 b- defN 23-May-15 20:29 k1lib-1.3.8.dist-info/RECORD
+84 files, 3500040 bytes uncompressed, 2527936 bytes compressed:  27.8%
```

## zipnote {}

```diff
@@ -222,32 +222,32 @@
 
 Filename: k1lib/serve/suffix-dash.py
 Comment: 
 
 Filename: k1lib/serve/suffix.py
 Comment: 
 
-Filename: k1lib-1.3.7.9.data/data/k1lib/k1ui/256.model.state_dict.pth
+Filename: k1lib-1.3.8.data/data/k1lib/k1ui/256.model.state_dict.pth
 Comment: 
 
-Filename: k1lib-1.3.7.9.data/data/k1lib/k1ui/mouseKey.pth
+Filename: k1lib-1.3.8.data/data/k1lib/k1ui/mouseKey.pth
 Comment: 
 
-Filename: k1lib-1.3.7.9.data/data/k1lib/serve/main.html
+Filename: k1lib-1.3.8.data/data/k1lib/serve/main.html
 Comment: 
 
-Filename: k1lib-1.3.7.9.dist-info/LICENSE
+Filename: k1lib-1.3.8.dist-info/LICENSE
 Comment: 
 
-Filename: k1lib-1.3.7.9.dist-info/METADATA
+Filename: k1lib-1.3.8.dist-info/METADATA
 Comment: 
 
-Filename: k1lib-1.3.7.9.dist-info/WHEEL
+Filename: k1lib-1.3.8.dist-info/WHEEL
 Comment: 
 
-Filename: k1lib-1.3.7.9.dist-info/top_level.txt
+Filename: k1lib-1.3.8.dist-info/top_level.txt
 Comment: 
 
-Filename: k1lib-1.3.7.9.dist-info/RECORD
+Filename: k1lib-1.3.8.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## k1lib/_monkey.py

```diff
@@ -1,9 +1,9 @@
 # AUTOGENERATED FILE! PLEASE DON'T EDIT
-import k1lib, math, numpy as np
+import k1lib, math, numpy as np, time
 from k1lib import cli
 from typing import List, Tuple, ContextManager
 from contextlib import contextmanager
 try: import torch; from torch import nn; hasTorch = True
 except: hasTorch = False
 try: import matplotlib.animation
 except: pass
@@ -489,8 +489,49 @@
         words = [[s[0]]]
         for c in s[1:]:
             if words[-1][-1].islower() and c.isupper():
                 words.append(list(c))
             else: words[-1].append(c)
         return [''.join(word) for word in words]
     forbiddenfruit.curse(str, "splitCamel", splitCamel)
+except: pass
+try:
+    import ray
+    @ray.remote
+    class RayProgress:
+        def __init__(self, n): self.values = [0]*n; self.thStop = False
+        def update(self, idx:int, val:float): self.values[idx] = val; return self.values[idx]
+        def stop(self): self.thStop = True
+        def content(self): return self.thStop, self.values | cli.apply(lambda x: f"{round(x*100)}%") | cli.join(" | ")
+    def startRayProgressThread(rp, title:str="Progress"):
+        def inner(x):
+            if x == 0: return
+            print("Starting...\r", end="")
+            beginTime = time.time()
+            while True:
+                stop, content = ray.get(rp.content.remote())
+                print(f"{title}: {content}, {round(time.time()-beginTime)}s elapsed         \r", end="")
+                if stop: break
+                time.sleep(0.01)
+        [0, 1] | cli.applyTh(inner, timeout=1e9, prefetch=10) | cli.item()
+    @k1lib.patch(ray)
+    @contextmanager
+    def progress(n:int, title:str="Progress"):
+        """Manages multiple progress bars distributedly.
+Example::
+
+    with ray.progress(5) as rp:
+        def process(idx:int):
+            for i in range(100):
+                time.sleep(0.05) # do some processing
+                rp.update.remote(idx, (i+1)/100) # update progress. Expect number between 0 and 1
+        range(5) | applyCl(process) | deref() # execute function in multiple nodes
+
+This will print out a progress bar that looks like this::
+
+    Progress: 100% | 100% | 100% | 100% | 100%
+
+:param n: number of progresses to keep track of
+:param title: title of the progress to show"""
+        rp = RayProgress.remote(n); startRayProgressThread(rp, title); yield rp
+        ray.get(rp.stop.remote()); time.sleep(0.1)
 except: pass
```

## k1lib/cli/_applyCl.py

```diff
@@ -90,8 +90,70 @@
     cpus = None | applyCl.aS(lambda: applyCl.cpu()) | deref(); n = cpus | cut(1) | toSum()
     tasks = [cpus | ~apply(lambda x,y: [x]*y) | joinStreams(), range(size) | splitW(*[1]*n)] | transpose() | insertIdColumn(True, False) | ~apply(lambda x,y,z: [x,[y,f"{folder}/{z}.bin"]]) | deref(1)
     tasks | ~applyCl(lambda r, fn: getChunks(url, r.start, r.stop-1, None, chunkTimeout) | file(fn), pre=True, timeout=timeout) | deref()
     if merge:
         None | cmd(f"rm -rf {destFile}") | deref()
         None | cmd(f"mkdir -p {dirname}") | deref()
         None | applyCl.aS(lambda: ls(folder)) | ungroup(True, True) | deref() | sortF(op().split(".bin")[0].split("/")[-1].ab_int(), 1) | applyCl(cat(text=False), pre=True) | cut(1) | file(destFile)
-        None | cmd(f"rm -rf {folder}") | deref()
+        None | cmd(f"rm -rf {folder}") | deref()
+def a_transfer(fn, nse, nodeB, rpF:callable=iden()):
+    """Transfers a lot of blocks from a bunch of nodes to nodeB. Does not delete from those node though
+
+nse = List[nodeAId, [sB, eB]]
+
+Runs on driver process, blocks, so better use applyTh outside of this
+
+:param rpF: ray progress function"""
+    blockSize = settings.cli.cat.chunkSize
+    def inner():
+        totalBytes = nse | cut(1) | ~apply(lambda x,y:y-x) | toSum(); currentByte = 0
+        for chunk in nse | ~apply(lambda x, y: range(x, y) | batched(blockSize, True) | apply("[x.start, x.stop]"), 1) | ungroup(True, True) | deref()\
+            | ~applyCl(lambda sB, eB: cat(fn, False, sB=sB, eB=eB), pre=True, timeout=None, prefetch=20) | cut(1):
+            chunk >> file(fn); currentByte += len(chunk); rpF(currentByte/totalBytes)
+    [nodeB] | applyCl.aS(inner, timeout=None) | item()
+def decommission(fn:str, nodeAs:List[str], nodeBs:List[str], rS=iden()):
+    """Spreads out a particular file in nodeAs to all nodeBs, to prepare
+to decomission nodeAs. The 2 sets should be mutually exclusive
+
+:param rS: instance of refineSeek"""
+    nodeAs, nodeBs = [nodeAs, nodeBs] | deref()
+    if len(nodeAs) == 0: return
+    if len(nodeBs) == 0: raise Exception("Unsupported configuration! Trying to move data from A+B to C+D. Has to have some shared nodes, like moving data from A+B+C to B+C+D. This is not a fundamental limitation, but just can't be done with the current architecture. Might be fixed in the future.")
+    # some initial metadata
+    nodeIds = applyCl.nodeIds(); nodeId_cpu = nodeIds | applyCl.aS(lambda: applyCl.cpu()) | deref(); nodeId2Cpu = nodeId_cpu | toDict()
+    ws = nodeId_cpu | inSet(nodeBs, 0) | cut(1) | deref() # weights to split files on nodeAs into
+    # splitting file on nodeAs into chunks first, to plan things out
+    a = nodeAs | applyCl.aS(lambda: fn | splitSeek(ws=ws) | rS | window(2) | deref() | insertColumn(nodeBs) | insert(applyCl.nodeId()).all() | deref()) | cut(1) | joinStreams() | deref()
+    # actually transferring chunks
+    with ray.progress(a | groupBy(1) | shape(0), "Decommissioning") as rp:
+        c = b = a | groupBy(1, True) | apply(iden() + apply(lambda arr: [arr[0], arr[1:]]) | reverse() | insert(fn)) | deref()
+        enumerate(c) | applyTh(~aS(lambda idx, e: a_transfer(*e, rpF=aS(lambda p: ray.get(rp.update.remote(idx, p))))), timeout=None) | deref()
+    # deleting files from nodeAs
+    nodeAs | applyCl.aS(lambda: None | cmd(f"rm -rf {fn}") | ignore()) | deref()
+def spreadOut(fn:str, nAs:List[str], nBs:List[str], rS=iden()):
+    """Spreads out a file from nodes A to B, where B fully contains A (no decomissioning).
+A and B should be mutually exclusive. Initial nodes are A, final nodes are A + B"""
+    nAs, nBs = [nAs, nBs] | deref(); rS.fn = fn
+    if len(nBs) == 0: return # no need to spread out
+    nBs | applyCl.aS(lambda: None | cmd(f"mkdir -p {os.path.dirname(fn)}") | deref(), timeout=None) | deref()
+    nBs | applyCl.aS(lambda: None | cmd(f"rm -rf {fn}") | deref(), timeout=None) | deref()
+    # some initial metadata
+    nodeIds = applyCl.nodeIds(); nodeId_cpu = nodeIds | applyCl.aS(lambda: applyCl.cpu()) | deref(); nodeId2Cpu = nodeId_cpu | toDict()
+    sizes = nAs | applyCl.aS(lambda: os.path.getsize(fn) if os.path.exists(fn) else 0) | deref(); totalSize = sizes | cut(1) | toSum()
+    ns = [*nAs, *nBs]; totalCpu = ns | lookup(nodeId2Cpu) | toSum(); bytePerCpu = totalSize/totalCpu; wsB = nBs | lookup(nodeId2Cpu) | deref()
+    # prepares segments and metadata, List[nodeId, [sB, eB]], where sB and eB are the ranges of nAs that they're willing to share
+    sizePost = sizes | ~apply(lambda idx, size: [idx, nodeId2Cpu[idx]/totalCpu*totalSize/size]) | deref()
+    invalidNodes = sizePost | ~filt(lambda x: 0 <= x <= 1, 1) | cut(0) | deref()
+    if len(invalidNodes) > 0: raise Exception(f"Unsupported configuration! These nodes have too little data to share: {invalidNodes}. This couldn't have happen using applyCl alone. Data is not corrupted, but you'll have to combine data from all files into 1 and spread them back out again.")
+    inter = sizePost | ~apply(lambda idx, x: [idx, [x, 1-x]]) | applyCl(lambda ws: fn | splitSeek(ws=ws) | rS | ~head(1), pre=True) | deref() | filt(~aS(lambda x,y: y-x>0), 1) | deref()
+    # actually transferring data to new nodes
+    meta = inter | apply(~aS(range) | splitW(*wsB) | rS | apply(wrapList()) | insertColumn(nBs) | deref(1), 1) | ungroup(begin=True) | apply("[x.start, x.stop]", 2) | groupBy(1, True) | deref()
+    with ray.progress(len(meta), "Transferring data to new nodes") as rp:
+        meta | insertIdColumn(True) | applyTh(~aS(lambda idx, nB, nse: a_transfer(fn, nse, nB, rpF=aS(lambda p: ray.get(rp.update.remote(idx, p))))), timeout=None) | deref()
+    # truncates the files in nAs nodes
+    inter | ~apply(lambda idx,se: [idx,se[0]]) | applyCl(lambda sB: open(fn, 'a').truncate(sB), pre=True, timeout=None) | deref()
+def balanceFile(fn:str, nAs:List[str]=None, nBs:List[str]=None, rS=iden()):
+    fn = os.path.expanduser(fn)
+    if nAs is None: nAs = None | applyCl.aS(lambda: os.path.exists(fn)) | filt(op(), 1) | cut(0) | deref()
+    if nBs is None: nBs = applyCl.nodeIds()
+    decommission(fn, *nAs | inSet(nBs).split() | reverse(), rS)
+    spreadOut(fn, *nBs | inSet(nAs).split(), rS)
```

## k1lib/cli/conv.py

```diff
@@ -11,16 +11,16 @@
 
 If it sounds complicated (convert to PIL image, tensor, ...) then most likely it will
 convert object to object. Lastly, there are some that just feels right to input
 an iterator and output a single object (like getting max, min, std, mean values)."""
 __all__ = ["toTensor", "toRange", "toList",
            "toSum", "toProd", "toAvg", "toMean", "toMax", "toMin", "toPIL", "toImg",
            "toRgb", "toRgba", "toGray", "toDict",
-           "toFloat", "toInt", "toBytes", "toHtml"]
-import re, k1lib, math, os, numpy as np, io, base64
+           "toFloat", "toInt", "toBytes", "toHtml", "toAscii"]
+import re, k1lib, math, os, numpy as np, io, base64, unicodedata
 from k1lib.cli.init import BaseCli, Table, Row, T, yieldT; import k1lib.cli as cli
 from k1lib.cli.typehint import *; import matplotlib as mpl; import matplotlib.pyplot as plt
 from collections import deque; from typing import Iterator, Any, List, Set, Tuple, Dict, Callable, Union
 settings = k1lib.settings.cli
 try: import PIL; hasPIL = True
 except: hasPIL = False
 try: import torch; hasTorch = True
@@ -452,8 +452,17 @@
         return cli.aS(Chem.MolFromSmiles)
     def toSmiles():
         """Molecule to smiles.
 Example::
 
     "c1ccc(C)cc1" | toMol() | toSmiles()"""
         return cli.aS(Chem.MolToSmiles)
-except: pass
+except: pass
+import unicodedata
+def toAscii():
+    """Converts complex unicode text to its base ascii form.
+Example::
+
+    "hà nội" | toAscii() # returns "ha noi"
+
+Taken from https://stackoverflow.com/questions/2365411/convert-unicode-to-ascii-without-errors-in-python"""
+    return cli.aS(lambda word: unicodedata.normalize('NFKD', word).encode('ascii', 'ignore'))
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## k1lib/cli/init.py

```diff
@@ -247,15 +247,15 @@
     def _typehint(self, inp):
         ts = []
         for f in self.clis:
             try: ts.append(f._typehint(inp))
             except: ts.append(cli.typehint.tAny())
         return cli.typehint.tCollection(*ts).reduce()
     def __ror__(self, it:Iterator[Any]) -> Iterator[Iterator[Any]]:
-        if isinstance(it, atomic.baseAnd) or not _iterable(it):
+        if isinstance(it, atomic.baseAnd) or isinstance(it, k1lib.cli.splitSeek) or not _iterable(it):
             for cli in self._cliCs: yield cli(it)
         else:
             its = itertools.tee(it, len(self.clis))
             for cli, it in zip(self._cliCs, its): yield cli(it)
     def _cache(self): self._cliCs = [fastF(c) for c in self.clis]; return self
     def _before(self, c): self.clis = [c] + self.clis; return self._cache()
     def _after(self, c): self.clis = self.clis + [c]; return self._cache()
```

## k1lib/cli/inp.py

```diff
@@ -2,37 +2,36 @@
 """This module for tools that will likely start the processing stream."""
 from typing import Iterator, Union, Any
 import k1lib, urllib, subprocess, warnings, os, k1lib, threading, time, warnings, math, io, dill
 from k1lib.cli import BaseCli; import k1lib.cli as cli
 from k1lib.cli.typehint import *
 try: import minio; hasMinio = True
 except: hasMinio = False
-__all__ = ["cat", "splitSeek", "curl", "wget", "ls", "cmd", "walk", "requireCli"]
+__all__ = ["cat", "splitSeek", "refineSeek", "curl", "wget", "ls", "cmd", "walk", "requireCli"]
 settings = k1lib.settings.cli
 catSettings = k1lib.Settings().add("chunkSize", 100000, "file reading chunk size for binary+chunk mode. Decrease it to avoid wasting memory and increase it to avoid disk latency")
 catSettings.add("every", k1lib.Settings().add("text", 1000, "for text mode, will print every n lines").add("binary", 10, "for binary mode, will print every n 100000-byte blocks"), "profiler print frequency")
 settings.add("cat", catSettings, "inp.cat() settings")
 def _catGenText(fn, sB, eB): # fn for "file name"
     try:
         if sB == 0 and eB == -1: # fast path without bounds (90-160 MB/s expected)
             with open(fn) as f:
                 while True:
                     line = f.readline()
                     if line == "": return
                     yield line[:-1] if line[-1] == "\n" else line
-        else: # slow path with bounds (15 MB/s expected)
+        else: # slow path with bounds (15 MB/s expected). Update: much faster now, expect only 40% slower than the path above
             sB = wrap(fn, sB); eB = wrap(fn, eB)
             with open(fn) as f:
                 f.seek(sB); b = sB # current byte
                 while True:
-                    line = f.readline()
+                    line = f.readline(); b += len(line)
                     if line == "": return
-                    if f.tell() > eB: yield line[:len(line)-(f.tell()-eB)]; return
-                    if line[-1] == "\n": yield line[:-1]
-                    else: yield line
+                    if b > eB: yield line[:len(line)-(b-eB)]; return
+                    yield line[:-1] if line[-1] == "\n" else line
     except FileNotFoundError: pass
 def _catGenBin(fn, sB, eB):
     chunkSize = settings.cat.chunkSize; sB = wrap(fn, sB); eB = wrap(fn, eB); nB = eB - sB # number of bytes to read total
     with open(fn, "rb") as f:
         f.seek(sB); nChunks = math.ceil(nB / chunkSize); lastChunkSize = nB - chunkSize*(nChunks-1)
         yield from range(nChunks) | cli.applyTh(lambda i: f.read(chunkSize) if i < nChunks-1 else f.read(chunkSize)[:lastChunkSize], prefetch=10)
 def fileLength(fn):
@@ -109,15 +108,15 @@
     that accepts a file name and outputs Iterator[str]
 :param text: if True, read text file, else read binary file
 :param chunks: if True then reads the file chunk by chunk, else reads the
     entire file. Defaults to True in text mode and False in binary mode
 :param profile: whether to profile the file reading rate or not. Can adjust
     printing frequency using `settings.cli.cat.every`
 :param sB: "start byte". Specify this if you want to start reading from this byte
-:param eB: "end byte", inclusive. Default -1 means end of file"""
+:param eB: "end byte", exclusive. Default -1 means end of file"""
     if chunks is None: chunks = True if text else False
     if profile and not chunks: warnings.warn(f"Can't profile reading rate when you're trying to read everything at once"); profile = False
     f = _cat(text, chunks, sB, eB)
     if profile: f = f | Profile(text)
     return f if fileName is None else fileName | f
 def _catPickle(fileName=None, pickleModule=dill):
     """Reads a file as a series of pickled objects.
@@ -137,37 +136,39 @@
         with open(os.path.expanduser(fn), "rb") as f:
             try:
                 while True: yield dill.load(f)
             except: pass
     return cli.aS(gen) if fileName is None else fileName | cli.aS(gen)
 cat.pickle = _catPickle
 class splitSeek(BaseCli):
-    def __init__(self, n, c=b'\n', weights=None):
+    def __init__(self, n=None, c=b'\n', ws=None):
         """Splits a file up into n fragments aligned to the closest character and return the seek points.
 Example::
 
     # preparing the large file
     range(120) | apply(lambda x: f"{x:3}_56789") | file("test/largeFile.txt")
     # returns [0, 30, 70, 110, 150, 190, 230, 270, 300, 340]
     "test/largeFile.txt" | splitSeek(31) | head()
     # returns 32
     "test/largeFile.txt" | splitSeek(31) | shape(0)
     # returns 32, demonstrating you can also pipe file objects in, if that's what you want
     open("test/largeFile.txt") | splitSeek(31) | shape(0)
     # returns [0, 0, 10, 10, 20, 30, 30, 40, 40, 50], notice some segments have zero length
     "test/largeFile.txt" | splitSeek(200) | head()
+    # returns [0, 400, 1200], demonstrating that you can split a file up unevenly by weights
+    "test/largeFile.txt" | splitSeek(ws=[1, 2]) | deref()
 
 So, the generated file has 120 lines in total. Each line is 10 bytes (9 for
 the string, and 1 for the new line character). Splitting the file into 31
 fragments will result in 32 seek points (:math:`p_i\quad i\in[1, n+1]`). You
 can then use these seek points to read the file in multiple threads/processes
 using :meth:`cat`, like this::
 
     # returns [['  0_56789', '  1_56789', '  2_56789'], ['  3_56789', '  4_56789', '  5_56789', '  6_56789']]
-    "test/largeFile.txt" | splitSeek(31) | window(2) | ~apply(lambda sB, eB: cat("test/largeFile.txt", sB=sB, eB=eB-1)) | head(2) | deref()
+    "test/largeFile.txt" | splitSeek(31) | splitSeek.window() | ~apply(lambda sB, eB: cat("test/largeFile.txt", sB=sB, eB=eB)) | head(2) | deref()
 
 Because :math:`120/31\\approx4`, most of cat's reads contain 4 lines, but some
 has 3 lines. Also notice that the lines smoothly transitions between cat's
 reads (``2_56789`` to ``3_56789``), so that's pretty nice.
 
 .. warning::
 
@@ -195,16 +196,18 @@
 
 Here, each 4 lines are (title, read, blank, quality). Because by default, this will
 only split neatly along new lines, you will have to write extra functions to detect
 if a particular seek point is desirable, and if not, either jump forward or backward
 using :meth:`splitSeek.forward` and :meth:`splitSeek.backward`.
 
 :param n: how many splits do you want?
-:param c: block-boundary character, usually just the new line character"""
-        self.n = n; self.c = c; self.weights = weights
+:param c: block-boundary character, usually just the new line character
+:param ws: weights. If given, the splits length ratios will roughly correspond to this"""
+        self.n = n; self.c = c; self.ws = ws; self.fn = None; self.res = None # file name, result
+        if ws is None and n is None: raise Exception("Specify at least n or ws for splitSeek to work")
     @staticmethod
     def forward(f, i:int, c=b'\n') -> int:
         """Returns char location after the search char, going forward.
 Example::
 
     f = io.BytesIO(b"123\\n456\\n789\\nabc")
     f | splitSeek(2) # returns [0, 4, 15]
@@ -245,38 +248,90 @@
                 f.seek(begin); b = f.tell()
                 s = f.read(end-begin); di = s.rfind(c)
                 if di > -1: return b + di + 1
                 if b == 0: return 0
         if isinstance(f, str):
             with open(os.path.expanduser(f), "rb") as _f: return inner(_f)
         else: return inner(f)
-    def __ror__(self, fn):
-        n = self.n; c = self.c; weights = self.weights
+    def __ror__(self, fn): # why return self instead of the seek positions directly? Because we want to pass along dependencies like file name to downstream processes like refineSeek
+        if isinstance(fn, str): fn = os.path.expanduser(fn)
+        n = self.n; c = self.c; ws = self.ws; self.fn = fn
         def func(f):
             f.seek(0, os.SEEK_END); end = f.tell()
-            if weights is None: begins = range(n) | cli.apply(lambda x: int(x*end/n))
-            else: begins = range(end) | cli.splitW(*weights) | cli.apply(lambda x: x.start)
+            if ws is None: begins = range(n) | cli.apply(lambda x: int(x*end/n))
+            else: begins = range(end) | cli.splitW(*ws) | cli.apply(lambda x: x.start)
             return [*begins | cli.apply(lambda x: splitSeek.backward(f, x, c)), end]
         if isinstance(fn, str):
-            with open(os.path.expanduser(fn), 'rb') as f: return func(f)
-        else: return func(fn)
+            with open(os.path.expanduser(fn), 'rb') as f: self.res = func(f); return self
+        else: self.res = func(fn); return self
+    def __or__(self, aft):
+        if self.res is None: return super().__or__(aft)
+        return aft.__ror__(self)
+    def __getitem__(self, idx): return self.res[idx]
+    def __len__(self): return len(self.res)
+    def __iter__(self): return iter(self.res)
     @staticmethod
     def window():
         """Converts input boundaries into segment windows.
 Example::
 
     # returns [[0, 4], [5, 9], [10, 20]]
     [0, 5, 10, 20] | splitSeek.window()
     # example use case
     "abc.txt" | splitSeek(10) | splitSeek.window()
 """
         def inner(it):
             it = it | cli.aS(list); ranges = it | cli.window(2) | cli.apply(lambda x: x-1, 1) | cli.deref()
             ranges[-1][1] += 1; return ranges
         return cli.aS(inner)
+    def __repr__(self): return self.res.__repr__()
+class refineSeek(BaseCli):
+    def __init__(self, f, window=1):
+        """Refines seek positions.
+Example::
+
+    # returns list of integers for seek positions
+    "abc.txt" | splitSeek(30)
+    # returns refined seek positions, such that the line starting at the seek positions starts with "@"
+    "abc.txt" | splitSeek(30) | refineSeek(lambda x: x.startswith(b"@"))
+    # same thing as above
+    "abc.txt" | splitSeek(30) | refineSeek(lambda x: x[0] == b"@"[0])
+    # returns refined seek positions, such that 0th line starts with "@" and 2nd line starts with "+". This demonstrates `window` parameter
+    "abc.txt" | splitSeek(30) | refineSeek(lambda x: x[0][0] == b"@"[0] and x[2][0] == b"+"[0], 3)
+    # same thing as above, demonstrating some builtin refine seek functions
+    "abc.txt" | splitSeek(30) | refineSeek.fastq()
+
+:param f: function that returns True if the current line/lines is a valid block boundary
+:param window: by default (1), will fetch 1 line and check boundary using ``f(line)``.
+    If a value greater than 1 is passed (for example, 3), will fetch 3 lines and check
+    boundary using ``f([line1, line2, line3])``
+"""
+        self.f = cli.fastF(f); self.window = window; self.fn = None
+    def __ror__(self, seeks):
+        f = self.f; window = self.window
+        def read(fio, sB:int):
+            fio.seek(sB)
+            if window == 1: return fio.readline()
+            return list(cli.repeatF(lambda: fio.readline(), window))
+        if len(seeks) <= 2: return seeks
+        fn = self.fn or seeks.fn; newSeeks = [seeks[0]]
+        def process(fio):
+            with open(fn, "rb") as fio:
+                for seek in seeks[1:-1]:
+                    line = read(fio, seek)
+                    while not f(line): seek = splitSeek.forward(fio, seek); line = read(fio, seek)
+                    newSeeks.append(seek)
+            newSeeks.append(seeks[-1]); return newSeeks
+        if isinstance(fn, str):
+            with open(fn, "rb") as fio: return process(fio)
+        else: return process(fn)
+    @classmethod
+    def fastq(cls):
+        """Refine fastq file's seek points"""
+        return cls(lambda x: x[0][0] == b"@"[0] and x[2][0] == b"+"[0], 3)
 def curl(url:str) -> Iterator[str]:
     """Gets file from url. File can't be a binary blob.
 Example::
 
     # prints out first 10 lines of the website
     curl("https://k1lib.github.io/") | headOut()"""
     for line in urllib.request.urlopen(url):
```

## k1lib/cli/modifier.py

```diff
@@ -33,33 +33,35 @@
         return x+2
     # returns 5
     3 | f
 
 This also decorates the returned object so that it has same qualname, docstring
 and whatnot.
 
-(Advanced) Writing out "lambda x:" all the time is annoying, and there are ways
-to quickly say ``lambda x: x+2`` like so::
+.. admonition:: Shorthands
 
-    3 | op()+2 # returns 5
-    3 | aS("x+2") # returns 5. Behind the scenes, it compiles and execute `lambda x: x+2`
+    Writing out "lambda x:" all the time is annoying, and there are ways
+    to quickly say ``lambda x: x+2`` like so::
 
-The first way is to use :class:`op`, that will absorb all operations done on it,
-like "+", and returns a function that essentially replays all the operations.
+        3 | op()+2 # returns 5
+        3 | aS("x+2") # returns 5. Behind the scenes, it compiles and execute `lambda x: x+2`
 
-In the second way, you only have to pass in the string containing code that you want
-done on the variable "x". Then internally, it will compile to regular Python code.
+    The first way is to use :class:`op`, that will absorb all operations done on it,
+    like "+", and returns a function that essentially replays all the operations.
 
-In fact, you can pass in ``op()`` or just a string to any cli that accepts any kind
-of function, like :class:`~k1lib.cli.filt.filt` or :class:`apply`::
+    In the second way, you only have to pass in the string containing code that you want
+    done on the variable "x". Then internally, it will compile to regular Python code.
 
-    range(4) | apply("x-2") | deref()
-    range(4) | apply(op()-2) | deref()
-    range(4) | filt("x%2") | deref()
-    range(4) | filt(op()%2) | deref()
+    In fact, you can pass in ``op()`` or just a string to any cli that accepts any kind
+    of function, like :class:`~k1lib.cli.filt.filt` or :class:`apply`::
+
+        range(4) | apply("x-2") | deref()
+        range(4) | apply(op()-2) | deref()
+        range(4) | filt("x%2") | deref()
+        range(4) | filt(op()%2) | deref()
 
 :param f: the function to be executed
 :param kwargs: other keyword arguments to pass to the function, together with ``args``"""
         super().__init__(fs=[f]); self.args = args; self.kwargs = kwargs
         self.f = f; self._fC = fastF(f); update_wrapper(self, f, updated=())
     def _typehint(self, inp):
         if self.hasHint: return self._hint
@@ -549,47 +551,95 @@
         fn = os.path.expanduser(fn); dirname = os.path.dirname(fn)
         if nodeIds is None: nodeIds = applyCl.nodeIds(False)
         nodeIds = nodeIds | cli.wrapList().all() | cli.deref()
         nodeIds | cli.insert(None, False).all() | applyCl(lambda _: None | cli.cmd(f"mkdir -p {dirname}; rm {fn}") | cli.deref(), pre=True) | cli.deref()
         for chunk in cli.cat(fn, text=False, chunks=True):
             nodeIds | cli.insert(chunk, False).all() | applyCl(lambda chunk: chunk >> cli.file(fn) | cli.deref(), pre=True) | cli.deref()
     @staticmethod
-    def splitFile(fn:str, nodeIds=None):
-        """Splits a specified file in the current node and dumps other parts
-to other nodes. Example::
+    def balanceFile(fn:str, nAs:List[str]=None, nBs:List[str]=None, rS=None):
+        """Splits a specified file in node nAs and dumps other parts
+to nodes nBs. Example::
 
-    applyCl.splitFile("~/cron.log")
+    applyCl.balanceFile("~/cron.log")
 
-This will split the big file up into multiple segments (1 for each node). Then
+This will split the big files up into multiple segments (1 for each node). Then
 for each segment, it will read through it chunk by chunk into memory, and then
-deposits it into the respective nodes. Finally, it truncates the original file
+deposits it into the respective nodes. Finally, it truncates the original files
 down to its segment boundary.
 
-The exact split rule depends on the number of CPUs of each node. Say there're
-2 nodes: A and B. A has 16 cpus and B has 8 cpus. Then, a 48MB file will be split
-into 2 parts. A will have the first 32MB, and B will have the last 16MB. This is
-to optimize for distributed reading and processing using :meth:`applyCl.cat`.
-This makes sense right? If you don't have as many cores, you should process less
-data, so you should have less data to work with from the beginning.
+The exact split rule depends on the number of CPUs of each node. Best to see an
+example::
+
+    Command:         applyCl.balanceFile("~/cron.log")
+    Verbose command: applyCl.balanceFile("~/cron.log", ["1"], ["1", "2", "3", "4", "5"])
+    ----------- Before -----------
+    Node:      1  2  3  4 5
+    Cpu:       8  12 16 8 8
+    Size (GB): 52 0  0  0 0
+    ----------- After  -----------
+    Node:      1  2  3  4 5
+    Cpu:       8  12 16 8 8
+    Size (GB): 8  12 16 8 8
+
+This also works if you have files on existing nodes already, and are upgrading the
+cluster::
+
+    Command:         applyCl.balanceFile("~/cron.log")
+    Verbose command: applyCl.balanceFile("~/cron.log", ["1", "5"], ["1", "2", "3", "4", "5"])
+    ----------- Before -----------
+    Node:      1  2  3  4  5
+    Cpu:       8  12 16 8  8
+    Size (GB): 26 0  0  26 0
+    ----------- After  -----------
+    Node:      1  2  3  4  5
+    Cpu:       8  12 16 8  8
+    Size (GB): 8  12 16 8  8
+
+If you want to move files out of a node when decommissioning them, you can do
+something like this::
+
+    Command:         applyCl.decommission("~/cron.log", ["3", "4"])
+    Verbose command: applyCl.balanceFile("~/cron.log", ["1", "2", "3", "4", "5"], ["1", "2", "5"])
+    ----------- Before -----------
+    Node:      1  2  3  4 5
+    Cpu:       8  12 16 8 8
+    Size (GB): 8  12 16 8 8
+    ----------- After  -----------
+    Node:      1  2  3  4 5
+    Cpu:       8  12 16 8 8
+    Size (GB): 15 22 0  0 15
+
+Remember that the node ids "1", etc. is for illustrative purposes only. You should get
+real node ids from :meth:`nodeIds`.
+
+Why is the file size proportional to the number of cores on each node? Well, if
+you have more cores, you should be able to process more, so as to make everything
+balanced, right?
 
 Again, this means that you can split arbitrarily large files as long as you have
 the disk space for it, ram size is not a concern. How does this perform? Not
 the best in the world if you don't have a lot of nodes. With sata 3 ssds, 750MB/s
 ethernet, I got transfer speeds of roughly 100MB/s. This should increase as you
 have more nodes based on the code structure, but I haven't tested it yet. Can
-it be faster? Definitely. Am I willing to spend time optimizing it? No."""
-        fn = os.path.expanduser(fn); dirname = os.path.dirname(fn)
-        if nodeIds is None: nodeIds = applyCl.nodeIds(False)
-        nodeIds | cli.wrapList().all() | cli.insert(None, False).all() | applyCl(lambda _: None | cli.cmd(f"mkdir -p {dirname}; rm {fn}") | cli.deref(), pre=True) | cli.deref()
-        nodeId2Cpu = ray.nodes() | cli.filt(lambda x: x["Alive"]) | cli.apply(lambda x: [x["NodeID"], int(x["Resources"]["CPU"])]) | cli.toDict()
-        n = len(nodeIds)+1; ranges = fn | cli.splitSeek(n, weights=[nodeId2Cpu[applyCl.nodeId()], *nodeIds | cli.lookup(nodeId2Cpu)]) | cli.splitSeek.window()
-        for chunks in ranges[1:] | ~cli.apply(lambda a,b: [cli.cat(fn, False, True, sB=a, eB=b), b'' | cli.repeat()] | cli.joinStreams()) | cli.transpose():
-            if chunks | cli.apply(len) | cli.toSum() == 0: break
-            [nodeIds, chunks] | cli.transpose() | applyCl(lambda chunk: chunk >> cli.file(fn) | cli.deref(), pre=True) | cli.deref()
-        with open(fn, 'a') as f: f.truncate(ranges[0][1])
+it be faster? Definitely. Am I willing to spend time optimizing it? No.
+
+:param fn: file name
+:param nAs: node ids that currently stores the file. If not specified, try to detect
+    what nodes the file exists in
+:param nBs: node ids that will store the file after balancing everything out. If not
+    specified, will take all available nodes
+:param rS: :class:`~k1lib.cli.inp.refineSeek` instance, if you need more fine-grained
+    control over section boundaries so as to not make everything corrupted
+"""
+        from k1lib.cli._applyCl import balanceFile
+        balanceFile(fn, nAs, nBs, rS or cli.iden())
+    def decommission(self, fn, nAs:List[str], rS=None):
+        """Convenience function for :meth:`balanceFile`. See docs over there."""
+        from k1lib.cli._applyCl import balanceFile
+        balanceFile(fn, None, applyCl.nodeIds() | ~cli.inSet(nAs) | deref(), rS or cli.iden())
     @staticmethod
     def cat(fn:str, f:Callable, timeout:float=30, keepNodeIds:bool=False, multiplier:int=1, includeId:bool=False):
         """Reads a file distributedly, does some operation on them, collects and
 returns all of the data together. Example::
 
     fn = "~/repos/labs/k1lib/k1lib/cli/test/applyCl.cat.data"
     ("0123456789"*5 + "\\n") * 1000 | file(fn)
@@ -629,15 +679,15 @@
 :param timeout: kills the processes if it takes longer than this amount of seconds
 :param keepNodeIds: whether to keep the node id column or not
 :param multiplier: by default, each node will spawn as many process as there
     are cpus. Sometimes you want to spawn more process, change this to a higher number
 :param includeId: includes a unique id for this process
 """
         postprocess = cli.insertIdColumn(True, False) | ~apply(lambda x,y,z: [x,[*y,z]])
-        checkpoints = None | applyCl.aS(lambda: fn | cli.splitSeek(int(applyCl.meta()["Resources"]["CPU"]*multiplier)) | cli.splitSeek.window() | cli.deref()) | cli.ungroup(single=True, begin=True) | postprocess | cli.deref()
+        checkpoints = None | applyCl.aS(lambda: fn | cli.splitSeek(int(applyCl.meta()["Resources"]["CPU"]*multiplier)) | cli.window(2) | cli.deref()) | cli.ungroup(single=True, begin=True) | postprocess | cli.deref()
         postprocess = cli.iden() if keepNodeIds else cli.cut(1)
         return checkpoints | applyCl(~aS(lambda x,y,idx: cli.cat(fn, sB=x, eB=y) | ((cli.wrapList() | cli.insert(idx)) if includeId else cli.iden()) | f), pre=True, timeout=timeout, num_cpus=1) | postprocess
     @staticmethod
     def balanceFolder(folder:str, audit:bool=False):
         """Balances all files within a folder across all nodes.
 Example::
```

## Comparing `k1lib-1.3.7.9.data/data/k1lib/k1ui/256.model.state_dict.pth` & `k1lib-1.3.8.data/data/k1lib/k1ui/256.model.state_dict.pth`

 * *Files identical despite different names*

## Comparing `k1lib-1.3.7.9.data/data/k1lib/k1ui/mouseKey.pth` & `k1lib-1.3.8.data/data/k1lib/k1ui/mouseKey.pth`

 * *Files identical despite different names*

## Comparing `k1lib-1.3.7.9.data/data/k1lib/serve/main.html` & `k1lib-1.3.8.data/data/k1lib/serve/main.html`

 * *Files identical despite different names*

## Comparing `k1lib-1.3.7.9.dist-info/LICENSE` & `k1lib-1.3.8.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `k1lib-1.3.7.9.dist-info/METADATA` & `k1lib-1.3.8.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: k1lib
-Version: 1.3.7.9
+Version: 1.3.8
 Summary: Some nice ML overhaul
 Home-page: https://k1lib.com
 Author: Quang Ho
 Author-email: 157239q@gmail.com
 License: MIT
 Requires-Python: >=3.7
 Description-Content-Type: text/markdown
```

## Comparing `k1lib-1.3.7.9.dist-info/RECORD` & `k1lib-1.3.8.dist-info/RECORD`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 k1lib/__init__.py,sha256=zdjKwxgRuNNLKhHen5zaW-zmo6DmOlGBR9frAQI0jh0,1435
 k1lib/_baseClasses.py,sha256=0uR1wDKqLHP2FJmMqb8ao3M-gGMHzyJ85UzUxtpUqGI,53793
 k1lib/_basics.py,sha256=f9Hn4UTRfhbzKotZ9eRMmb4Ic8A4_1ORP0qWnInkLu8,13494
 k1lib/_context.py,sha256=wZB8OhBZiJNgfk-CA9tVWmhyC38J-6vVXSn1vEIkCtE,4908
 k1lib/_higher.py,sha256=BZ54rlMNygGrKBZjLpakicQ_FDSt936X5sgfJnvns2Y,2866
 k1lib/_k1a.py,sha256=i-_CPMBdiE-CCI375eaguwcC01dGHar4SlaaQDFsRwg,948
 k1lib/_learner.py,sha256=63aPrhGE5jO1YApvsIcn_HXA1E4ttLfyZVkIUE-meqA,11646
-k1lib/_monkey.py,sha256=UnI0vUg0gWk7zzsCQ6FJEutBpfBX9mqH5_aYgDqQtK4,19722
+k1lib/_monkey.py,sha256=ojty9WZTOcW0NVajMLuL_5Dn_1523i5ONvok3m8hbGU,21467
 k1lib/_perlin.py,sha256=sL4B8caVEz4xgWj65Oy3FYxELGDM8wPAtsn0Ro8R3dc,3571
 k1lib/eqn.py,sha256=1wchJwyCIOxbEzf9keQsnvhQfqbHe_cgPzI2crtuHRw,13377
 k1lib/fmt.py,sha256=0dGvEl_oewAXIKXLzaye4bDeX5AJ4yykNS7DGILQbmc,6317
 k1lib/graphEqn.py,sha256=DkdJlDcgC8THRuOFXGsbNuuA94_pUYy3jaU7XcggDLw,6398
 k1lib/imports.py,sha256=kW-HdhcJFE_OXO8Oh5PePyYCSHwhc6fhfdu5ZQgFua4,2750
 k1lib/knn.py,sha256=LASgb0z_9x1u4eZTsat_FspX1g5aIyxjkEWHhKRTn1g,3107
 k1lib/p5.py,sha256=nB0XmMXSz6CuDDZveGgsMCMhoKBXOG_rF0tjkCxJCec,3564
@@ -42,27 +42,27 @@
 k1lib/callbacks/lossFunctions/shorts.py,sha256=wXeUSgGDIdu_nsiAvn4pKs3fQrcO2FwpNXKcBCFvGzY,3465
 k1lib/callbacks/profilers/__init__.py,sha256=Gp5IvLRABYAg1J0ilTT2v72gfDbyTvUUHUEpvlSo1Lc,45
 k1lib/callbacks/profilers/computation.py,sha256=gPNsoioghh4PI5w8s2p8qYOrR7XObslqCLH5EkNOPSI,5054
 k1lib/callbacks/profilers/io.py,sha256=H8E0YzmLWRD9T2_RyDG2eXvbQQnJgzEnEyx8PqfQ4wY,2319
 k1lib/callbacks/profilers/memory.py,sha256=L0F5pc5LB0dtSnRot8ReR-amZn1uIXv0py0XmGS166U,4419
 k1lib/callbacks/profilers/time.py,sha256=R2-2ZooDwLQIeyonLp2Zz5E_uXzdy6mWUgw6uavQbpE,4215
 k1lib/cli/__init__.py,sha256=hF0ODhL20OSM9o1j68VhcIVflibgSpPuqeyYlsh8oow,925
-k1lib/cli/_applyCl.py,sha256=szbkJRbSd5eqEhdjgyUMRqK2iZSXZheTXNjDQ0CCfQY,6056
+k1lib/cli/_applyCl.py,sha256=y3olIGpaTxnExD14JEOvh01LdPcAbTShamQfqHYz16w,11353
 k1lib/cli/bio.py,sha256=PhGvy-fDA-wrUzzEDpuRe4x-Kbylx0sNmoXCEZfE_FA,8308
 k1lib/cli/cif.py,sha256=77FX83m1FRYEeZkdXJ8MiVapqCSzZ-1xOQ8ZLeHfhf8,4033
-k1lib/cli/conv.py,sha256=EIaZPZ4oCG2mr7Y-Esa1TjV2PnmrUCRWNjJSS7ZEYn4,18911
+k1lib/cli/conv.py,sha256=-P7YTTEi3lks1Lz6dQRkN4NiL37wySVDAbcc3cReJgw,19291
 k1lib/cli/filt.py,sha256=VJ5Jfep6KFvVeYQm6n8czsy7TEsNIoE3zeUtKAYNIRs,24092
 k1lib/cli/gb.py,sha256=xxjuNYgWrrElRckon3gP0sj-dShYnKs3jmHAb1U0kVI,6672
 k1lib/cli/grep.py,sha256=Lu4PFOe2pkaqd-UfJe_HhHCUFTUifE6Bh96_k80sQDA,6348
-k1lib/cli/init.py,sha256=jtOLSw3PR_0VIepOzK5cDmeAea4WmDNa3jy6WIM6qV4,18312
-k1lib/cli/inp.py,sha256=ZllbDCi-tmwbiPVLPYrg55kN3mpc6IXPcvHe4B-0SvI,21754
+k1lib/cli/init.py,sha256=2FaWeRqZnb--XWV4Xpo0z4ANDdyWpNQlHKkALtOGHKQ,18351
+k1lib/cli/inp.py,sha256=pWKen2tRyQR2ZjrzyQFmMuvgmBnCRftOA7hPjaLw49Y,24942
 k1lib/cli/kcsv.py,sha256=YGUVVLTZGGujokhxtj5MfjU9t1jRGqp23d58JK8lhq0,623
 k1lib/cli/kxml.py,sha256=YQGutvKNm0_xAi_NhCNtuGey7fx3zZSmSo33kS--54c,4819
 k1lib/cli/mgi.py,sha256=aLke90nG89tgWLPwyKmTj3kM8yJnIBCJSrPS1jT8mUk,1915
-k1lib/cli/modifier.py,sha256=FUAvAOjiVXAmyOZlDZ6ikKZt8eoxVI7iOpb61tSfujc,53907
+k1lib/cli/modifier.py,sha256=JVK52iP8GmQjhBP9zae_q8CLHjrB7sV5FWjFRm_k25k,55103
 k1lib/cli/mol.py,sha256=wNFuCPXtdEcH4DRBbmYaLAWxtDzjN2MOKFX7ynJhaJs,694
 k1lib/cli/nb.py,sha256=LsNN7OFJ6KzAYKvZpm4fj9WRpsX6Srx6D_xpSTCV328,4038
 k1lib/cli/optimizations.py,sha256=iZ73DwLqZCxRm0sECVZ7A2nDxf5D4rsoSGzrKTgzGaI,3530
 k1lib/cli/output.py,sha256=HQD61-Epy6WfteMHXW6SV14SiprXF55ELnfer2CWXEA,11559
 k1lib/cli/sam.py,sha256=_ersEPP2ue0Oa3AyftNjQu2PABpH4L7iFBbRJDOkeug,2394
 k1lib/cli/structural.py,sha256=4fa04Qe5Uoi7ZtKsQB1QGzkfkzkL6IsDsccOGOn-o3Q,49930
 k1lib/cli/trace.py,sha256=nzZgOyXqFJYkQfbpR0lpX0Nnp0bQHXPjk8sDUBIe2hk,10399
@@ -70,15 +70,15 @@
 k1lib/cli/utils.py,sha256=gaHYEw2_gjqNjOQFLUfa3JAXXwi6vZPCQqKwDuhrUDg,20345
 k1lib/k1ui/__init__.py,sha256=8D5a8oKgqd6WA1RUkiKCn4l_PVemtyuckxQut0vDHXM,20
 k1lib/k1ui/main.py,sha256=PnmdOhkjYgRSZnDyGYNMYtQ5Nvcb1NhQ9yjfP_3QORI,61803
 k1lib/serve/__init__.py,sha256=8D5a8oKgqd6WA1RUkiKCn4l_PVemtyuckxQut0vDHXM,20
 k1lib/serve/main.py,sha256=Xh2SzgABfsBp2dRLUJRMftsG_We8ReVHYqXLi3ntMVA,10361
 k1lib/serve/suffix-dash.py,sha256=HMNJvB4d-PTHXDRDQTdYUKtzgirJ0LVnqqAkXxO0B4w,153
 k1lib/serve/suffix.py,sha256=UH3ITN6O2vzoha2f6v4bcQG3_Boav7VA7EC8wf8r9f8,642
-k1lib-1.3.7.9.data/data/k1lib/k1ui/256.model.state_dict.pth,sha256=Ga-UXlAJfUPNOZsvP_c1-1cfB2Hp_-oQ4ghdouX1d7g,2453826
-k1lib-1.3.7.9.data/data/k1lib/k1ui/mouseKey.pth,sha256=KULhK_gdK2Ppju9gQnv1zV2kf_A0K-vX2W7trY6DIg8,304735
-k1lib-1.3.7.9.data/data/k1lib/serve/main.html,sha256=gHFNqzE9JKb4eCwCJN-Du45jj75lDRED4Ico91T-b4g,20544
-k1lib-1.3.7.9.dist-info/LICENSE,sha256=psuy2wnTg9zacuCQ0dXfxS44iaa89aTgsNzHDzx4UGM,1049
-k1lib-1.3.7.9.dist-info/METADATA,sha256=BKJ-MS9tlGBrC6iPR2y-iunkbEUwKkrBrN2GyeFh9uE,3864
-k1lib-1.3.7.9.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-k1lib-1.3.7.9.dist-info/top_level.txt,sha256=xxWmqZzuThnLZn49Mse6A6j41-IVduPVnQW54imcOTA,6
-k1lib-1.3.7.9.dist-info/RECORD,,
+k1lib-1.3.8.data/data/k1lib/k1ui/256.model.state_dict.pth,sha256=Ga-UXlAJfUPNOZsvP_c1-1cfB2Hp_-oQ4ghdouX1d7g,2453826
+k1lib-1.3.8.data/data/k1lib/k1ui/mouseKey.pth,sha256=KULhK_gdK2Ppju9gQnv1zV2kf_A0K-vX2W7trY6DIg8,304735
+k1lib-1.3.8.data/data/k1lib/serve/main.html,sha256=gHFNqzE9JKb4eCwCJN-Du45jj75lDRED4Ico91T-b4g,20544
+k1lib-1.3.8.dist-info/LICENSE,sha256=psuy2wnTg9zacuCQ0dXfxS44iaa89aTgsNzHDzx4UGM,1049
+k1lib-1.3.8.dist-info/METADATA,sha256=hByOaFpdoBBBwayxIYgBlCp5JH-o2scLVBWTEZfzbq4,3862
+k1lib-1.3.8.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+k1lib-1.3.8.dist-info/top_level.txt,sha256=xxWmqZzuThnLZn49Mse6A6j41-IVduPVnQW54imcOTA,6
+k1lib-1.3.8.dist-info/RECORD,,
```

